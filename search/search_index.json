{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Cloud Pak Deployer","text":"<p>The intention of the Cloud Pak Deployer is to simplify the initial installation and also continuous management of OpenShift, watsonx and the IBM Cloud Paks on top of that, driven by automation. It will help you deploy watsonx, Cloud Pak for Data, Cloud Pak for Integration, Cloud Pak for Business Automation and Cloud Pak for Watson AIOps on various OpenShift and infrastructures:</p> <ul> <li>IBM Cloud ROKS</li> <li>IBM Fusion HCI</li> <li>Azure Red Hat OpenShift (ARO)</li> <li>Azure self-managed Red Hat OpenShift</li> <li>Red Hat OpenShift on AWS (ROSA)</li> <li>AWS self-managed Red Hat OpenShift</li> <li>vSphere</li> <li>Existing OpenShift on x86</li> <li>Existing OpenShift on IBM Power</li> </ul> <p>Cloud Pak Deployer was created for a joint project with one of our key partners who need to fully automate the deployment of IBM containerized software based on a configuration that is kept in a Git repository. As additional needs for the deployed environment surface, the configuration is changed, committed, approved and then changes are deployed without destroying the current environment.</p> <p>\"If we have seen a screen during deployment, it means something has failed\"</p> <p>Not all software implementations require governance using the previously described GitOps approach. We also wanted to accelerate containerized software deployment for POCs, MVPs and services engagements using the same tool. Simple by default, flexible when needed</p> <p>Cloud Pak Deployer has been designed with the following key principles in mind:</p> <p></p> <p>Every deployment starts with a set of configuration files which define the infrastructure, OpenShift cluster and Cloud Pak or watsonx to be installed. The Cloud Pak Deployer reads the configuration from the specified directory, and secrets which are kept in a vault, and does whatever it needs to do to reach the desired end state. During the deployment, new secrets may be created and these are also stored in the vault. In its simplest form, the vault is a flat file in the specified status directory, but you can also choose to keep the secrets in HashiCorp Vault or the Vault service on IBM Cloud.</p> <p>.</p> <p>As long as you keep the configuration directory and the vault available, you can make changes to the config and re-run the deployer to reach the new desired end state. For example, if you choose to add another cartridge (service) to your Cloud Pak deployment, just change the <code>state</code> of that cartridge and re-run the deployer; this applies to other Cloud Paks too.</p>"},{"location":"#opinionated","title":"Opinionated","text":"<p>Red Hat OpenShift, watsonx and IBM Cloud Paks offer a wide variety of deployment and configuration options. It is the intention of the Cloud Pak Deployer to simplify the deployment by focusing on proven deployment patterns. As an example: for a non-highly available deployment of the Cloud Pak, we use an NFS storage class; for a production deployment, we use OpenShift Container Storage (aka OpenShift Data Foundation).</p> <p>Choosing from proven deployment patterns improves the probability for a straightforward installation without surprises.</p>"},{"location":"#declarative-and-desired-end-state","title":"Declarative and desired end-state","text":"<p>It is our intention to deploy a combination of OpenShift and containerized software based on a (set of) configuration file(s) that describe the desired end-state. Although the deployment pipeline follows a pre-defined flow, as a user you do not necessarily need to know what happens under the hood. Instead, you have entered the destination (end-state) you want the deployment to have and the deployer will take care of getting you there.</p>"},{"location":"#idempotent","title":"Idempotent","text":"<p>Idempotence goes hand in hand with the desired end-state principle of the Cloud Pak Deployer. Basically, we're saying: if we make multiple identical requests, we will still arrive at the same end-state, and (very important): if nothing needs to change, don't change. As an example of what that means: say that there was a timeout in the provisioning process because the OpenShift cluster could not be created within the pre-defined timeframe and other resources were successfully created. When the deployer is re-run, it will leave the successfully created resources alone and will not delete or change them, but rather continue the provisioning pipeline.</p>"},{"location":"01-introduction/current-state-and-support/","title":"Current state of the Cloud Pak Deployer","text":"<p>The below picture indicates the current state of the Cloud Pak Deployer, which infrastructures are supported to provision or use OpenShift, the storage classes which can be controlled and the Cloud Paks with cartridges and components. </p>"},{"location":"01-introduction/current-state-and-support/#support-statement","title":"Support statement","text":"<p>Cloud Pak Deployer has proven to be a useful tool to install OpenShift, the IBM Cloud Paks and watsonx through a unified and unattended installation script. It has been used by many IBM field specialists, IBM business partners, clients and even several development teams are using the tools. A question that often comes up: \"Is Cloud Pak Deployer supported by IBM Support\". The answer is NO. for the tool itself, similar to any bash, Python, Ansible, Chef, Terraform, CI/CD pipelines, or other scripts that would be developed by IBM Expert Labs, business partners or clients. </p> <p>However, the IBM products which are deployed using the tool can continue to be supported, given that the steps executed for the installation correspond with the official product documentation. For example, the installation of Cloud Pak for Data and watsonx and its services are done through the <code>apply-olm</code> and <code>apply-cr</code> subcommands which are part of <code>cpd-cli manage</code>.</p> <p>To create better insights into which steps have been executed for the installation of the products, Cloud Pak Deployer now generates a log file <code>$STATUS_DIR/log/deployer-activities.log</code>, which logs all the steps and which can be used to reproduce an installation using the standard <code>cpd-cli</code> and <code>oc</code> commands found in the documentation. Activity logging has currently been implemented only for Existing OpenShift installations of Cloud Pak for Data and watsonx. Other Cloud Paks will follow.</p> <p>Additionally, you can use Cloud Pak Deployer to generate an activity log only using the <code>--dry-run</code> command-line option, run as an OpenShift user with <code>cluster-reader</code> role. This will be convenient if you do not want to let deployer do the installation but generate the steps you can then execute manually or through a script.</p> <p>For the \"general\" support statement, check the Apache license which is referenced in the GitHub repository.</p>"},{"location":"05-install/install/","title":"Installing the Cloud Pak Deployer","text":""},{"location":"05-install/install/#prerequisites","title":"Prerequisites","text":"<p>To install and run the Cloud Pak Deployer, ensure that either podman or docker is available on your system. These are typically available on various Linux distributions such as Red Hat Enterprise Linux (preferred), Fedora, CentOS, Ubuntu, and MacOS. Note that Docker behaves differently on Windows compared to Linux platforms, potentially causing deployment issues.</p> <p>Info</p> <p>If you plan to run Cloud Pak Deployer from the OpenShift console, you can skip these steps. A Cloud Pak Deployer container image is already available on quay.io, to be used in an OpenShift job. For running Cloud Pak Deployer from the OpenShift console, please refer to Run on OpenShift using console or Run on OpenShift using wizard.</p>"},{"location":"05-install/install/#using-a-windows-workstation","title":"Using a Windows workstation","text":"<p>If you're working on a Windows workstation without access to a Linux server, you can use VirtualBox to create a Linux virtual machine for deployment.</p> <ul> <li>Install VirtualBox: https://www.virtualbox.org</li> <li>Install a Linux guest operating system: https://www.virtualbox.org/wiki/Guest_OSes</li> </ul> <p>Once the guest operating system is set up, log in as root. VirtualBox supports port forwarding for easy access to the Linux command line using tools like PuTTY.</p>"},{"location":"05-install/install/#install-on-linux","title":"Install on Linux","text":"<p>On Red Hat Enterprise Linux of CentOS, run the following commands: <pre><code>yum install -y podman git\nyum clean all\n</code></pre></p> <p>On MacOS, run the following commands: <pre><code>brew install podman git\npodman machine create\npodman machine init\n</code></pre></p> <p>On Ubuntu, debian Based :  <pre><code>apt-get -y install podman\npodman machine create\npodman machine init\n</code></pre></p> <p>Generally, adhere to the instructions provided to install either podman or docker on your Linux system.</p>"},{"location":"05-install/install/#clone-the-current-repository","title":"Clone the current repository","text":""},{"location":"05-install/install/#using-the-command-line","title":"Using the command line","text":"<p>If you clone the repository from the command line, you will need to enter a token when you run the <code>git clone</code> command. You can retrieve your token as follows:</p> <p>Go to a directory where you want to download the Git repo. <pre><code>git clone --depth=1 https://github.com/IBM/cloud-pak-deployer.git\n</code></pre></p>"},{"location":"05-install/install/#build-the-image","title":"Build the image","text":"<p>First go to the directory where you cloned the GitHub repository, for example <code>~/cloud-pak-deployer</code>. <pre><code>cd cloud-pak-deployer\n</code></pre></p>"},{"location":"05-install/install/#set-path-and-alias-for-the-deployer","title":"Set path and alias for the deployer","text":"<pre><code>source ./set-env.sh\n</code></pre> <p>Then run the following command to build the container image. <pre><code>cp-deploy.sh build [--clean-up]\n</code></pre></p> <p>This process will take 5-10 minutes to complete and it will install all the pre-requisites needed to run the automation, including Ansible, Python and required operating system packages. For the installation to work, the system on which the image is built must be connected to the internet.</p> <p>Info</p> <p>If you want to keep your system clean if you're regularly building the Cloud Pak Deployer image, you can add the <code>--clean-up</code> option or set environment variable <code>CPD_CLEANUP</code> to <code>true</code>.</p>"},{"location":"05-install/install/#downloading-the-cloud-pak-deployer-image-from-registry","title":"Downloading the Cloud Pak Deployer Image from Registry","text":"<p>To download the Cloud Pak Deployer image from the Quay.io registry, you can use the Docker command-line interface (CLI) or Podman.</p> <pre><code>podman pull quay.io/cloud-pak-deployer/cloud-pak-deployer\n</code></pre> <p>This command pulls the latest version of the Cloud Pak Deployer image from the Quay.io repository. Once downloaded, you can use this image to deploy Cloud Paks</p>"},{"location":"05-install/install/#tags-and-versions","title":"Tags and Versions","text":"<p>By default, the above command pulls the latest version of the Cloud Pak Deployer image. If you want to specify a particular version or tag, you can append it to the image name. For example:</p> <pre><code>podman pull quay.io/cloud-pak-deployer/cloud-pak-deployer:&lt;tag_or_version&gt;\n</code></pre> <p>Replace <code>&lt;tag_or_version&gt;</code> with the specific tag or version you want to download.</p>"},{"location":"10-use-deployer/1-overview/overview/","title":"Using Cloud Pak Deployer","text":""},{"location":"10-use-deployer/1-overview/overview/#running-cloud-pak-deployer","title":"Running Cloud Pak Deployer","text":"<p>There are 3 main steps you need to perform to provision an OpenShift cluster with the desired Cloud Pak(s):</p> <ol> <li>Install the Cloud Pak Deployer, when not running from OpenShift console</li> <li>Run the Cloud Pak Deployer to create the cluster and install the Cloud Pak</li> </ol>"},{"location":"10-use-deployer/1-overview/overview/#what-will-i-need","title":"What will I need?","text":"<p>To complete the deployment, you will or may need the following. Details will be provided when you need them.</p> <ul> <li>Your Cloud Pak entitlement key to pull images from the IBM Container Registry</li> <li>IBM Cloud VPC: An IBM Cloud API key that allows you to provision infrastructure</li> <li>vSphere: A vSphere user and password which has infrastructure create permissions</li> <li>AWS ROSA: AWS IAM credentials (access key and secret access key), a ROSA login token and optionally a temporary security token</li> <li>AWS Self-managed: AWS IAM credentials (access key and secret access key) and optionally a temporary security token</li> <li>Azure: Azure service principal with the correct permissions</li> <li>Existing OpenShift: Cluster admin login credentials of the OpenShift cluster</li> </ul>"},{"location":"10-use-deployer/1-overview/overview/#executing-commands-on-the-openshift-cluster","title":"Executing commands on the OpenShift cluster","text":"<p>The server on which you run the Cloud Pak Deployer may not have the necessary clients to interact with the cloud infrastructure, OpenShift, or the installed Cloud Pak. You can run commands using the same container image that runs the deployment of OpenShift and the Cloud Paks through the command line: Open a command line</p>"},{"location":"10-use-deployer/1-overview/overview/#destroying-your-openshift-cluster","title":"Destroying your OpenShift cluster","text":"<p>If you want to destroy the provisioned OpenShift cluster, including the installed Cloud Pak(s), you can do this through the Cloud pak Deployer. Steps can be found here: Destroy the assets</p>"},{"location":"10-use-deployer/3-run/aws-rosa/","title":"Running the Cloud Pak Deployer on AWS (ROSA)","text":"<p>On Amazon Web Services (AWS), OpenShift can be set up in various ways, managed by Red Hat (ROSA) or self-managed. The steps below are applicable to the ROSA (Red Hat OpenShift on AWS) installation. More information about ROSA can be found here: https://aws.amazon.com/rosa/</p> <p>There are 5 main steps to run the deployer for AWS:</p> <ol> <li>Configure deployer</li> <li>Prepare the cloud environment</li> <li>Obtain entitlement keys and secrets</li> <li>Set environment variables and secrets</li> <li>Run the deployer</li> </ol> <p>See the deployer in action in this video:</p>"},{"location":"10-use-deployer/3-run/aws-rosa/#topology","title":"Topology","text":"<p>A typical setup of the ROSA cluster is pictured below: </p> <p>When deploying ROSA, an external host name and domain name are automatically generated by Amazon Web Services and both the API and Ingress servers can be resolved by external clients. At this stage, one cannot configure the domain name to be used.</p>"},{"location":"10-use-deployer/3-run/aws-rosa/#1-configure-deployer","title":"1. Configure deployer","text":""},{"location":"10-use-deployer/3-run/aws-rosa/#deployer-configuration-and-status-directories","title":"Deployer configuration and status directories","text":"<p>Deployer reads the configuration from a directory you set in the <code>CONFIG_DIR</code> environment variable. A status directory (<code>STATUS_DIR</code> environment variable) is used to log activities, store temporary files, scripts. If you use a File Vault (default), the secrets are kept in the <code>$STATUS_DIR/vault</code> directory.</p> <p>You can find OpenShift and Cloud Pak sample configuration (yaml) files here: sample configuration. For ROSA installations, copy one of <code>ocp-aws-rosa-*.yaml</code> files into the <code>$CONFIG_DIR/config</code> directory. If you also want to install a Cloud Pak, copy one of the <code>cp4*.yaml</code> files.</p> <p>Example: <pre><code>mkdir -p $HOME/cpd-config/config\ncp sample-configurations/sample-dynamic/config-samples/ocp-aws-rosa-elastic.yaml $HOME/cpd-config/config/\ncp sample-configurations/sample-dynamic/config-samples/cp4d-471.yaml $HOME/cpd-config/config/\n</code></pre></p>"},{"location":"10-use-deployer/3-run/aws-rosa/#set-configuration-and-status-directories-environment-variables","title":"Set configuration and status directories environment variables","text":"<p>Cloud Pak Deployer uses the status directory to log its activities and also to keep track of its running state. For a given environment you're provisioning or destroying, you should always specify the same status directory to avoid contention between different deploy runs. </p> <pre><code>export CONFIG_DIR=$HOME/cpd-config\nexport STATUS_DIR=$HOME/cpd-status\n</code></pre> <ul> <li><code>CONFIG_DIR</code>: Directory that holds the configuration, it must have a <code>config</code> subdirectory which contains the configuration <code>yaml</code> files.</li> <li><code>STATUS_DIR</code>: The directory where the Cloud Pak Deployer keeps all status information and logs files.</li> </ul>"},{"location":"10-use-deployer/3-run/aws-rosa/#optional-advanced-configuration","title":"Optional: advanced configuration","text":"<p>If the deployer configuration is kept on GitHub, follow the instructions in GitHub configuration.</p> <p>For special configuration with defaults and dynamic variables, refer to Advanced configuration.</p>"},{"location":"10-use-deployer/3-run/aws-rosa/#2-prepare-the-cloud-environment","title":"2. Prepare the cloud environment","text":""},{"location":"10-use-deployer/3-run/aws-rosa/#enable-rosa-on-aws","title":"Enable ROSA on AWS","text":"<p>Before you can use ROSA on AWS, you have to enable it if this has not been done already. This can be done as follows:</p> <ul> <li>Go to https://aws.amazon.com/</li> <li>Login to the AWS console</li> <li>Search for ROSA service</li> <li>Click Enable OpenShift</li> </ul>"},{"location":"10-use-deployer/3-run/aws-rosa/#obtain-the-aws-iam-credentials","title":"Obtain the AWS IAM credentials","text":"<p>You will need an Access Key ID and Secret Access Key for the deployer to run <code>rosa</code> commands.</p> <ul> <li>Go to https://aws.amazon.com/</li> <li>Login to the AWS console</li> <li>Click on your user name at the top right of the screen</li> <li>Select Security credentials. You can also reach this screen via https://console.aws.amazon.com/iam/home?region=us-east-2#/security_credentials.</li> <li>If you do not yet have an access key (or you no longer have the associated secret), create an access key</li> <li>Store your Access Key ID and Secret Access Key in safe place</li> </ul>"},{"location":"10-use-deployer/3-run/aws-rosa/#alternative-using-temporary-aws-security-credentials-sts","title":"Alternative: Using temporary AWS security credentials (STS)","text":"<p>If your account uses temporary security credentials for AWS resources, you must use the Access Key ID, Secret Access Key and Session Token associated with your temporary credentials. </p> <p>For more information about using temporary security credentials, see https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html.</p> <p>The temporary credentials must be issued for an IAM role that has sufficient permissions to provision the infrastructure and all other components. More information about required permissions for ROSA cluster can be found here: https://docs.openshift.com/rosa/rosa_planning/rosa-sts-aws-prereqs.html#rosa-sts-aws-prereqs.</p> <p>An example on how to retrieve the temporary credentials for a user-defined role: <pre><code>printf \"\\nexport AWS_ACCESS_KEY_ID=%s\\nexport AWS_SECRET_ACCESS_KEY=%s\\nexport AWS_SESSION_TOKEN=%s\\n\" $(aws sts assume-role \\\n--role-arn arn:aws:iam::678256850452:role/ocp-sts-role \\\n--role-session-name OCPInstall \\\n--query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" \\\n--output text)\n</code></pre></p> <p>This would return something like the below, which you can then paste into the session running the deployer. <pre><code>export AWS_ACCESS_KEY_ID=ASIxxxxxxAW\nexport AWS_SECRET_ACCESS_KEY=jtLxxxxxxxxxxxxxxxGQ\nexport AWS_SESSION_TOKEN=IQxxxxxxxxxxxxxbfQ\n</code></pre></p> <p>You must set the <code>infrastructure.use_sts</code> to <code>True</code> in the <code>openshift</code> configuration if you need to use the temporary security credentials. Cloud Pak Deployer will then run the <code>rosa create cluster</code> command with the appropriate flag.</p>"},{"location":"10-use-deployer/3-run/aws-rosa/#obtain-your-rosa-login-token","title":"Obtain your ROSA login token","text":"<p>To run <code>rosa</code> commands to manage the cluster, the deployer requires the ROSA login token.</p> <ul> <li>Go to https://cloud.redhat.com/openshift/token/rosa</li> <li>Login with your Red Hat user ID and password. If you don't have one yet, you need to create it.</li> <li>Copy the offline access token presented on the screen and store it in a safe place.</li> </ul>"},{"location":"10-use-deployer/3-run/aws-rosa/#if-rosa-is-already-installed","title":"If ROSA is already installed","text":"<p>This scenario is supported. To enable this feature, please ensure that you take the following steps:</p> <ol> <li>Include the environment ID in the infrastrucure definition <code>{{ env_id }}</code> to match existing cluster</li> <li> <p>Create \"cluster-admin \" password token using the following command:</p> <pre><code>$ cp-deploy.sh vault set -vs={{env_id}}-cluster-admin-password=[YOUR PASSWORD]\n</code></pre> </li> </ol> <p>Without these changes, sthe cloud player will fail and you will receive the following error message: \"Failed to get the cluster-admin password from the vault\".</p>"},{"location":"10-use-deployer/3-run/aws-rosa/#3-acquire-entitlement-keys-and-secrets","title":"3. Acquire entitlement keys and secrets","text":"<p>If you want to pull the Cloud Pak images from the entitled registry (i.e. an online install), or if you want to mirror the images to your private registry, you need to download the entitlement key. You can skip this step if you're installing from a private registry and all Cloud Pak images have already been downloaded to the private registry.</p> <ul> <li>Navigate to https://myibm.ibm.com/products-services/containerlibrary and login with your IBMId credentials</li> <li>Select Get Entitlement Key and create a new key (or copy your existing key)</li> <li>Copy the key value</li> </ul> <p>Warning</p> <p>As stated for the API key, you can choose to download the entitlement key to a file. However, when we reference the entitlement key, we mean the 80+ character string that is displayed, not the file.</p>"},{"location":"10-use-deployer/3-run/aws-rosa/#4-set-environment-variables-and-secrets","title":"4. Set environment variables and secrets","text":"<pre><code>export AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_access_key\nexport ROSA_LOGIN_TOKEN=\"your_rosa_login_token\"\nexport CP_ENTITLEMENT_KEY=your_cp_entitlement_key\n</code></pre> <p>Optional: If your user does not have permanent administrator access but using temporary credentials, you can set the <code>AWS_SESSION_TOKEN</code> to be used for the AWS CLI. <pre><code>export AWS_SESSION_TOKEN=your_session_token\n</code></pre></p> <ul> <li><code>AWS_ACCESS_KEY_ID</code>: This is the AWS Access Key you retrieved above, often this is something like <code>AK1A2VLMPQWBJJQGD6GV</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code>: The secret associated with your AWS Access Key, also retrieved above</li> <li><code>AWS_SESSION_TOKEN</code>: The session token that will grant temporary elevated permissions</li> <li><code>ROSA_LOGIN_TOKEN</code>: The offline access token that was retrieved before. This is a very long string (200+ characters). Make sure you enclose the string in single or double quotes as it may hold special characters</li> <li><code>CP_ENTITLEMENT_KEY</code>: This is the entitlement key you acquired as per the instructions above, this is a 80+ character string</li> </ul> <p>Warning</p> <p>If your <code>AWS_SESSION_TOKEN</code> is expires while the deployer is still running, the deployer may end abnormally. In such case, you can just issue new temporary credentials (<code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code> and <code>AWS_SESSION_TOKEN</code>) and restart the deployer. Alternatively, you can update the 3 vault secrets, respectively <code>aws-access-key</code>, <code>aws-secret-access-key</code> and <code>aws-session-token</code> with new values as they are re-retrieved by the deployer on a regular basis.</p>"},{"location":"10-use-deployer/3-run/aws-rosa/#optional-set-the-github-personal-access-token-pat","title":"Optional: Set the GitHub Personal Access Token (PAT)","text":"<p>In some cases, download of the <code>cloudctl</code> and <code>cpd-cli</code> clients from @IBM will fail because GitHub limits the number of API calls from non-authenticated clients. You can remediate this issue by creating a Personal Access Token on github.com and creating a secret in the vault.</p> <pre><code>cp-deploy.sh vault set -vs github-ibm-pat=&lt;your PAT&gt;\n</code></pre> <p>Alternatively, you can set the secret by adding <code>-vs github-ibm-pat=&lt;your PAT&gt;</code> to the <code>cp-deploy.sh env apply</code> command.</p>"},{"location":"10-use-deployer/3-run/aws-rosa/#5-run-the-deployer","title":"5. Run the deployer","text":""},{"location":"10-use-deployer/3-run/aws-rosa/#set-path-and-alias-for-the-deployer","title":"Set path and alias for the deployer","text":"<pre><code>source ./set-env.sh\n</code></pre>"},{"location":"10-use-deployer/3-run/aws-rosa/#optional-validate-the-configuration","title":"Optional: validate the configuration","text":"<p>If you only want to validate the configuration, you can run the dpeloyer with the <code>--check-only</code> argument. This will run the first stage to validate variables and vault secrets and then execute the generators.</p> <pre><code>cp-deploy.sh env apply --check-only --accept-all-licenses\n</code></pre>"},{"location":"10-use-deployer/3-run/aws-rosa/#run-the-cloud-pak-deployer","title":"Run the Cloud Pak Deployer","text":"<p>To run the container using a local configuration input directory and a data directory where temporary and state is kept, use the example below. If you don't specify the status directory, the deployer will automatically create a temporary directory. Please note that the status directory will also hold secrets if you have configured a flat file vault. If you lose the directory, you will not be able to make changes to the configuration and adjust the deployment. It is best to specify a permanent directory that you can reuse later. If you specify an existing directory the current user must be the owner of the directory. Failing to do so may cause the container to fail with insufficient permissions.</p> <pre><code>cp-deploy.sh env apply --accept-all-licenses\n</code></pre> <p>You can also specify extra variables such as <code>env_id</code> to override the names of the objects referenced in the <code>.yaml</code> configuration files as <code>{{ env_id }}-xxxx</code>. For more information about the extra (dynamic) variables, see advanced configuration.</p> <p>The <code>--accept-all-licenses</code> flag is optional and confirms that you accept all licenses of the installed cartridges and instances. Licenses must be either accepted in the configuration files or at the command line.</p> <p>When running the command, the container will start as a daemon and the command will tail-follow the logs. You can press Ctrl-C at any time to interrupt the logging but the container will continue to run in the background.</p> <p>You can return to view the logs as follows:</p> <pre><code>cp-deploy.sh env logs\n</code></pre> <p>Deploying the infrastructure, preparing OpenShift and installing the Cloud Pak will take a long time, typically between 1-5 hours,dependent on which Cloud Pak cartridges you configured. For estimated duration of the steps, refer to Timings.</p> <p>If you need to interrupt the automation, use CTRL-C to stop the logging output and then use:</p> <pre><code>cp-deploy.sh env kill\n</code></pre>"},{"location":"10-use-deployer/3-run/aws-rosa/#on-failure","title":"On failure","text":"<p>If the Cloud Pak Deployer fails, for example because certain infrastructure components are temporarily not available, fix the cause if needed and then just re-run it with the same <code>CONFIG_DIR</code> and <code>STATUS_DIR</code> as well extra variables. The provisioning process has been designed to be idempotent and it will not redo actions that have already completed successfully.</p>"},{"location":"10-use-deployer/3-run/aws-rosa/#finishing-up","title":"Finishing up","text":"<p>Once the process has finished, it will output the URLs by which you can access the deployed Cloud Pak. You can also find this information under the <code>cloud-paks</code> directory in the status directory you specified.</p> <p>To retrieve the Cloud Pak URL(s):</p> <pre><code>cat $STATUS_DIR/cloud-paks/*\n</code></pre> <p>This will show the Cloud Pak URLs:</p> <pre><code>Cloud Pak for Data URL for cluster pluto-01 and project cpd:\nhttps://cpd-cpd.apps.pluto-01.pmxz.p1.openshiftapps.com\n</code></pre> <p>The <code>admin</code> password can be retrieved from the vault as follows:</p> <p>List the secrets in the vault:</p> <pre><code>cp-deploy.sh vault list\n</code></pre> <p>This will show something similar to the following:</p> <pre><code>Secret list for group sample:\n- aws-access-key\n- aws-secret-access-key\n- ibm_cp_entitlement_key\n- rosa-login-token\n- pluto-01-cluster-admin-password\n- cp4d_admin_zen_40_pluto_01\n- all-config\n</code></pre> <p>You can then retrieve the Cloud Pak for Data admin password like this:</p> <pre><code>cp-deploy.sh vault get --vault-secret cp4d_admin_zen_40_pluto_01\n</code></pre> <pre><code>PLAY [Secrets] *****************************************************************\nincluded: /cloud-pak-deployer/automation-roles/99-generic/vault/vault-get-secret/tasks/get-secret-file.yml for localhost\ncp4d_admin_zen_40_pluto_01: gelGKrcgaLatBsnAdMEbmLwGr\n</code></pre>"},{"location":"10-use-deployer/3-run/aws-rosa/#post-install-configuration","title":"Post-install configuration","text":"<p>You can find examples of a couple of typical changes you may want to do here: Post-run changes.</p>"},{"location":"10-use-deployer/3-run/aws-self-managed/","title":"Running the Cloud Pak Deployer on AWS (Self-managed)","text":"<p>On Amazon Web Services (AWS), OpenShift can be set up in various ways, self-managed or managed by Red Hat (ROSA). The steps below are applicable to a self-managed OpenShift installation. The IPI (Installer Provisioned Infrastructure) installer will be used. More information about IPI installation can be found here: https://docs.openshift.com/container-platform/4.12/installing/installing_aws/installing-aws-customizations.html.</p> <p>There are 5 main steps to run the deploye for AWS:</p> <ol> <li>Configure deployer</li> <li>Prepare the cloud environment</li> <li>Obtain entitlement keys and secrets</li> <li>Set environment variables and secrets</li> <li>Run the deployer</li> <li>Post-install configuration (Add GPU nodes)</li> </ol> <p>See the deployer in action in this video:</p>"},{"location":"10-use-deployer/3-run/aws-self-managed/#topology","title":"Topology","text":"<p>A typical setup of the self-managed OpenShift cluster is pictured below: </p>"},{"location":"10-use-deployer/3-run/aws-self-managed/#single-node-openshift-sno-on-aws","title":"Single-node OpenShift (SNO) on AWS","text":"<p>Red Hat OpenShift also supports single-node deployments in which control plane and compute are combined into a single node. Obviously, this type of configuration does not cater for any high availability requirements that are usually part of a production installation, but it does offer a more cost-efficient option for development and testing purposes.</p> <p>Cloud Pak Deployer can deploy a single-node OpenShift with elastic storage and a sample configuration is provided as part of the deployer.</p> <p>Warning</p> <p>When deploying the IBM Cloud Paks on single-node OpenShift, there may be intermittent timeouts as pods are starting up. In those cases, just re-run the deployer with the same configuration and check status of the pods.</p>"},{"location":"10-use-deployer/3-run/aws-self-managed/#1-configure-deployer","title":"1. Configure deployer","text":""},{"location":"10-use-deployer/3-run/aws-self-managed/#deployer-configuration-and-status-directories","title":"Deployer configuration and status directories","text":"<p>Deployer reads the configuration from a directory you set in the <code>CONFIG_DIR</code> environment variable. A status directory (<code>STATUS_DIR</code> environment variable) is used to log activities, store temporary files, scripts. If you use a File Vault (default), the secrets are kept in the <code>$STATUS_DIR/vault</code> directory.</p> <p>You can find OpenShift and Cloud Pak sample configuration (yaml) files here: sample configuration. For self-managed OpenShift installations, copy one of <code>ocp-aws-self-managed-*.yaml</code> files into the <code>$CONFIG_DIR/config</code> directory. If you also want to install a Cloud Pak, copy one of the <code>cp4*.yaml</code> files.</p> <p>Example: <pre><code>mkdir -p $HOME/cpd-config/config\ncp sample-configurations/sample-dynamic/config-samples/ocp-aws-self-managed-elastic.yaml $HOME/cpd-config/config/\ncp sample-configurations/sample-dynamic/config-samples/cp4d-471.yaml $HOME/cpd-config/config/\n</code></pre></p>"},{"location":"10-use-deployer/3-run/aws-self-managed/#set-configuration-and-status-directories-environment-variables","title":"Set configuration and status directories environment variables","text":"<p>Cloud Pak Deployer uses the status directory to log its activities and also to keep track of its running state. For a given environment you're provisioning or destroying, you should always specify the same status directory to avoid contention between different deploy runs. </p> <pre><code>export CONFIG_DIR=$HOME/cpd-config\nexport STATUS_DIR=$HOME/cpd-status\n</code></pre> <ul> <li><code>CONFIG_DIR</code>: Directory that holds the configuration, it must have a <code>config</code> subdirectory which contains the configuration <code>yaml</code> files.</li> <li><code>STATUS_DIR</code>: The directory where the Cloud Pak Deployer keeps all status information and logs files.</li> </ul>"},{"location":"10-use-deployer/3-run/aws-self-managed/#optional-advanced-configuration","title":"Optional: advanced configuration","text":"<p>If the deployer configuration is kept on GitHub, follow the instructions in GitHub configuration.</p> <p>For special configuration with defaults and dynamic variables, refer to Advanced configuration.</p>"},{"location":"10-use-deployer/3-run/aws-self-managed/#2-prepare-the-cloud-environment","title":"2. Prepare the cloud environment","text":""},{"location":"10-use-deployer/3-run/aws-self-managed/#configure-route53-service-on-aws","title":"Configure Route53 service on AWS","text":"<p>When deploying a self-managed OpenShift on Amazon web Services, a public hosted zone must be created in the same account as your OpenShift cluster. The domain name or subdomain name registered in the Route53 service must be specifed in the <code>openshift</code> configuration of the deployer. </p> <p>For more information on acquiring or specifying a domain on AWS, you can refer to https://github.com/openshift/installer/blob/master/docs/user/aws/route53.md.</p>"},{"location":"10-use-deployer/3-run/aws-self-managed/#obtain-the-aws-iam-credentials","title":"Obtain the AWS IAM credentials","text":"<p>If you can use your permanent security credentials for the AWS account, you will need an Access Key ID and Secret Access Key for the deployer to setup an OpenShift cluster on AWS. </p> <ul> <li>Go to https://aws.amazon.com/</li> <li>Login to the AWS console</li> <li>Click on your user name at the top right of the screen</li> <li>Select Security credentials. You can also reach this screen via https://console.aws.amazon.com/iam/home?region=us-east-2#/security_credentials.</li> <li>If you do not yet have an access key (or you no longer have the associated secret), create an access key</li> <li>Store your Access Key ID and Secret Access Key in safe place</li> </ul>"},{"location":"10-use-deployer/3-run/aws-self-managed/#alternative-using-temporary-aws-security-credentials-sts","title":"Alternative: Using temporary AWS security credentials (STS)","text":"<p>If your account uses temporary security credentials for AWS resources, you must use the Access Key ID, Secret Access Key and Session Token associated with your temporary credentials. </p> <p>For more information about using temporary security credentials, see https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_use-resources.html.</p> <p>The temporary credentials must be issued for an IAM role that has sufficient permissions to provision the infrastructure and all other components. More information about required permissions can be found here: https://docs.openshift.com/container-platform/4.10/authentication/managing_cloud_provider_credentials/cco-mode-sts.html#sts-mode-create-aws-resources-ccoctl.</p> <p>An example on how to retrieve the temporary credentials for a user-defined role: <pre><code>printf \"\\nexport AWS_ACCESS_KEY_ID=%s\\nexport AWS_SECRET_ACCESS_KEY=%s\\nexport AWS_SESSION_TOKEN=%s\\n\" $(aws sts assume-role \\\n--role-arn arn:aws:iam::678256850452:role/ocp-sts-role \\\n--role-session-name OCPInstall \\\n--query \"Credentials.[AccessKeyId,SecretAccessKey,SessionToken]\" \\\n--output text)\n</code></pre></p> <p>Thie would return something like the below, which you can then paste into the session running the deployer. <pre><code>export AWS_ACCESS_KEY_ID=ASIxxxxxxAW\nexport AWS_SECRET_ACCESS_KEY=jtLxxxxxxxxxxxxxxxGQ\nexport AWS_SESSION_TOKEN=IQxxxxxxxxxxxxxbfQ\n</code></pre></p> <p>If the <code>openshift</code> configuration has the <code>infrastructure.credentials_mode</code> set to <code>Manual</code>, Cloud Pak Deployer will automatically configure and run the Cloud Credential Operator utility.</p>"},{"location":"10-use-deployer/3-run/aws-self-managed/#3-acquire-entitlement-keys-and-secrets","title":"3. Acquire entitlement keys and secrets","text":""},{"location":"10-use-deployer/3-run/aws-self-managed/#acquire-ibm-cloud-pak-entitlement-key","title":"Acquire IBM Cloud Pak entitlement key","text":"<p>If you want to pull the Cloud Pak images from the entitled registry (i.e. an online install), or if you want to mirror the images to your private registry, you need to download the entitlement key. You can skip this step if you're installing from a private registry and all Cloud Pak images have already been downloaded to the private registry.</p> <ul> <li>Navigate to https://myibm.ibm.com/products-services/containerlibrary and login with your IBMId credentials</li> <li>Select Get Entitlement Key and create a new key (or copy your existing key)</li> <li>Copy the key value</li> </ul> <p>Warning</p> <p>As stated for the API key, you can choose to download the entitlement key to a file. However, when we reference the entitlement key, we mean the 80+ character string that is displayed, not the file.</p>"},{"location":"10-use-deployer/3-run/aws-self-managed/#acquire-an-openshift-pull-secret","title":"Acquire an OpenShift pull secret","text":"<p>To install OpenShift you need an OpenShift pull secret which holds your entitlement.</p> <ul> <li>Navigate to https://console.redhat.com/openshift/install/pull-secret and download the pull secret into file <code>/tmp/ocp_pullsecret.json</code></li> </ul>"},{"location":"10-use-deployer/3-run/aws-self-managed/#optional-locate-or-generate-a-public-ssh-key","title":"Optional: Locate or generate a public SSH Key","text":"<p>To obtain access to the OpenShift nodes post-installation, you will need to specify the public SSH key of your server; typically this is <code>~/.ssh/id_rsa.pub</code>, where <code>~</code> is the home directory of your user. If you don't have an SSH key-pair yet, you can generate one using the steps documented here: https://cloud.ibm.com/docs/ssh-keys?topic=ssh-keys-generating-and-using-ssh-keys-for-remote-host-authentication#generating-ssh-keys-on-linux. Alternatively, deployer can generate SSH key-pair automatically if credential <code>ocp-ssh-pub-key</code> is not in the vault.</p>"},{"location":"10-use-deployer/3-run/aws-self-managed/#4-set-environment-variables-and-secrets","title":"4. Set environment variables and secrets","text":""},{"location":"10-use-deployer/3-run/aws-self-managed/#set-the-cloud-pak-entitlement-key","title":"Set the Cloud Pak entitlement key","text":"<p>If you want the Cloud Pak images to be pulled from the entitled registry, set the Cloud Pak entitlement key.</p> <pre><code>export CP_ENTITLEMENT_KEY=your_cp_entitlement_key\n</code></pre> <ul> <li><code>CP_ENTITLEMENT_KEY</code>: This is the entitlement key you acquired as per the instructions above, this is a 80+ character string. You don't need to set this environment variable when you install the Cloud Pak(s) from a private registry</li> </ul>"},{"location":"10-use-deployer/3-run/aws-self-managed/#set-the-environment-variables-for-aws-self-managed-openshift-deployment","title":"Set the environment variables for AWS self-managed OpenShift deployment","text":"<pre><code>export AWS_ACCESS_KEY_ID=your_access_key\nexport AWS_SECRET_ACCESS_KEY=your_secret_access_key\n</code></pre> <p>Optional: If your user does not have permanent administrator access but using temporary credentials, you can set the <code>AWS_SESSION_TOKEN</code> to be used for the AWS CLI. <pre><code>export AWS_SESSION_TOKEN=your_session_token\n</code></pre></p> <ul> <li><code>AWS_ACCESS_KEY_ID</code>: This is the AWS Access Key you retrieved above, often this is something like <code>AK1A2VLMPQWBJJQGD6GV</code></li> <li><code>AWS_SECRET_ACCESS_KEY</code>: The secret associated with your AWS Access Key, also retrieved above</li> <li><code>AWS_SESSION_TOKEN</code>: The session token that will grant temporary elevated permissions</li> </ul> <p>Warning</p> <p>If your <code>AWS_SESSION_TOKEN</code> is expires while the deployer is still running, the deployer may end abnormally. In such case, you can just issue new temporary credentials (<code>AWS_ACCESS_KEY_ID</code>, <code>AWS_SECRET_ACCESS_KEY</code> and <code>AWS_SESSION_TOKEN</code>) and restart the deployer. Alternatively, you can update the 3 vault secrets, respectively <code>aws-access-key</code>, <code>aws-secret-access-key</code> and <code>aws-session-token</code> with new values as they are re-retrieved by the deployer on a regular basis.</p>"},{"location":"10-use-deployer/3-run/aws-self-managed/#create-the-secrets-needed-for-self-managed-openshift-cluster","title":"Create the secrets needed for self-managed OpenShift cluster","text":"<p>You need to store the below credentials in the vault so that the deployer has access to them when installing self-managed OpenShift cluster on AWS.</p> <pre><code>cp-deploy.sh vault set \\\n    --vault-secret ocp-pullsecret \\\n    --vault-secret-file /tmp/ocp_pullsecret.json\n</code></pre>"},{"location":"10-use-deployer/3-run/aws-self-managed/#optional-create-secret-for-public-ssh-key","title":"Optional: Create secret for public SSH key","text":"<p>If you want to use your SSH key to access nodes in the cluster, set the Vault secret with the public SSH key. <pre><code>cp-deploy.sh vault set \\\n    --vault-secret ocp-ssh-pub-key \\\n    --vault-secret-file ~/.ssh/id_rsa.pub\n</code></pre></p>"},{"location":"10-use-deployer/3-run/aws-self-managed/#optional-set-the-github-personal-access-token-pat","title":"Optional: Set the GitHub Personal Access Token (PAT)","text":"<p>In some cases, download of the <code>cloudctl</code> and <code>cpd-cli</code> clients from @IBM will fail because GitHub limits the number of API calls from non-authenticated clients. You can remediate this issue by creating a Personal Access Token on github.com and creating a secret in the vault.</p> <pre><code>cp-deploy.sh vault set -vs github-ibm-pat=&lt;your PAT&gt;\n</code></pre> <p>Alternatively, you can set the secret by adding <code>-vs github-ibm-pat=&lt;your PAT&gt;</code> to the <code>cp-deploy.sh env apply</code> command.</p>"},{"location":"10-use-deployer/3-run/aws-self-managed/#5-run-the-deployer","title":"5. Run the deployer","text":""},{"location":"10-use-deployer/3-run/aws-self-managed/#set-path-and-alias-for-the-deployer","title":"Set path and alias for the deployer","text":"<pre><code>source ./set-env.sh\n</code></pre>"},{"location":"10-use-deployer/3-run/aws-self-managed/#optional-validate-the-configuration","title":"Optional: validate the configuration","text":"<p>If you only want to validate the configuration, you can run the dpeloyer with the <code>--check-only</code> argument. This will run the first stage to validate variables and vault secrets and then execute the generators.</p> <pre><code>cp-deploy.sh env apply --check-only --accept-all-licenses\n</code></pre>"},{"location":"10-use-deployer/3-run/aws-self-managed/#run-the-cloud-pak-deployer","title":"Run the Cloud Pak Deployer","text":"<p>To run the container using a local configuration input directory and a data directory where temporary and state is kept, use the example below. If you don't specify the status directory, the deployer will automatically create a temporary directory. Please note that the status directory will also hold secrets if you have configured a flat file vault. If you lose the directory, you will not be able to make changes to the configuration and adjust the deployment. It is best to specify a permanent directory that you can reuse later. If you specify an existing directory the current user must be the owner of the directory. Failing to do so may cause the container to fail with insufficient permissions.</p> <pre><code>cp-deploy.sh env apply --accept-all-licenses\n</code></pre> <p>You can also specify extra variables such as <code>env_id</code> to override the names of the objects referenced in the <code>.yaml</code> configuration files as <code>{{ env_id }}-xxxx</code>. For more information about the extra (dynamic) variables, see advanced configuration.</p> <p>The <code>--accept-all-licenses</code> flag is optional and confirms that you accept all licenses of the installed cartridges and instances. Licenses must be either accepted in the configuration files or at the command line.</p> <p>When running the command, the container will start as a daemon and the command will tail-follow the logs. You can press Ctrl-C at any time to interrupt the logging but the container will continue to run in the background.</p> <p>You can return to view the logs as follows:</p> <pre><code>cp-deploy.sh env logs\n</code></pre> <p>Deploying the infrastructure, preparing OpenShift and installing the Cloud Pak will take a long time, typically between 1-5 hours,dependent on which Cloud Pak cartridges you configured. For estimated duration of the steps, refer to Timings.</p> <p>If you need to interrupt the automation, use CTRL-C to stop the logging output and then use:</p> <pre><code>cp-deploy.sh env kill\n</code></pre>"},{"location":"10-use-deployer/3-run/aws-self-managed/#on-failure","title":"On failure","text":"<p>If the Cloud Pak Deployer fails, for example because certain infrastructure components are temporarily not available, fix the cause if needed and then just re-run it with the same <code>CONFIG_DIR</code> and <code>STATUS_DIR</code> as well extra variables. The provisioning process has been designed to be idempotent and it will not redo actions that have already completed successfully.</p>"},{"location":"10-use-deployer/3-run/aws-self-managed/#finishing-up","title":"Finishing up","text":"<p>Once the process has finished, it will output the URLs by which you can access the deployed Cloud Pak. You can also find this information under the <code>cloud-paks</code> directory in the status directory you specified.</p> <p>To retrieve the Cloud Pak URL(s):</p> <pre><code>cat $STATUS_DIR/cloud-paks/*\n</code></pre> <p>This will show the Cloud Pak URLs:</p> <pre><code>Cloud Pak for Data URL for cluster pluto-01 and project cpd (domain name specified was example.com):\nhttps://cpd-cpd.apps.pluto-01.example.com\n</code></pre> <p>The <code>admin</code> password can be retrieved from the vault as follows:</p> <p>List the secrets in the vault:</p> <pre><code>cp-deploy.sh vault list\n</code></pre> <p>This will show something similar to the following:</p> <pre><code>Secret list for group sample:\n- aws-access-key\n- aws-secret-access-key\n- ocp-pullsecret\n- ocp-ssh-pub-key\n- ibm_cp_entitlement_key\n- pluto-01-cluster-admin-password\n- cp4d_admin_zen_40_pluto_01\n- all-config\n</code></pre> <p>You can then retrieve the Cloud Pak for Data admin password like this:</p> <pre><code>cp-deploy.sh vault get --vault-secret cp4d_admin_zen_40_pluto_01\n</code></pre> <pre><code>PLAY [Secrets] *****************************************************************\nincluded: /cloud-pak-deployer/automation-roles/99-generic/vault/vault-get-secret/tasks/get-secret-file.yml for localhost\ncp4d_admin_zen_40_pluto_01: gelGKrcgaLatBsnAdMEbmLwGr\n</code></pre>"},{"location":"10-use-deployer/3-run/aws-self-managed/#6-post-install-configuration","title":"6. Post-install configuration","text":"<p>You can find examples of a couple of typical changes you may want to do here: Post-run changes:</p> <ul> <li>Update the Cloud Pak for Data administrator password</li> <li>Add GPU node(s) to your OpenShift cluster</li> </ul>"},{"location":"10-use-deployer/3-run/azure-aro/","title":"Running the Cloud Pak Deployer on Microsoft Azure - ARO","text":"<p>On Azure, OpenShift can be set up in various ways, managed by Red Hat (ARO) or self-managed. The steps below are applicable to the ARO (Azure Red Hat OpenShift).</p> <p>There are 5 main steps to run the deployer for Azure:</p> <ol> <li>Configure deployer</li> <li>Prepare the cloud environment</li> <li>Obtain entitlement keys and secrets</li> <li>Set environment variables and secrets</li> <li>Run the deployer</li> </ol>"},{"location":"10-use-deployer/3-run/azure-aro/#topology","title":"Topology","text":"<p>A typical setup of the ARO cluster is pictured below: </p> <p>When deploying ARO, you can configure the domain name by setting the <code>openshift.domain_name</code> attribute. The resulting domain name is managed by Azure, and it must be unique across all ARO instances deployed in Azure. Both the API and Ingress urls are set to be public in the template, so they can be resolved by external clients. If you want to use a custom domain and don't have one yet, you buy one from Azure: https://learn.microsoft.com/en-us/azure/app-service/manage-custom-dns-buy-domain.</p>"},{"location":"10-use-deployer/3-run/azure-aro/#1-configure-deployer","title":"1. Configure deployer","text":""},{"location":"10-use-deployer/3-run/azure-aro/#deployer-configuration-and-status-directories","title":"Deployer configuration and status directories","text":"<p>Deployer reads the configuration from a directory you set in the <code>CONFIG_DIR</code> environment variable. A status directory (<code>STATUS_DIR</code> environment variable) is used to log activities, store temporary files, scripts. If you use a File Vault (default), the secrets are kept in the <code>$STATUS_DIR/vault</code> directory.</p> <p>You can find OpenShift and Cloud Pak sample configuration (yaml) files here: sample configuration. For ARO installations, copy one of <code>ocp-azure-aro*.yaml</code> files into the <code>$CONFIG_DIR/config</code> directory. If you also want to install a Cloud Pak, copy one of the <code>cp4*.yaml</code> files.</p> <p>Example: <pre><code>mkdir -p $HOME/cpd-config/config\ncp sample-configurations/sample-dynamic/config-samples/ocp-azure-aro.yaml $HOME/cpd-config/config/\ncp sample-configurations/sample-dynamic/config-samples/cp4d-471.yaml $HOME/cpd-config/config/\n</code></pre></p>"},{"location":"10-use-deployer/3-run/azure-aro/#set-configuration-and-status-directories-environment-variables","title":"Set configuration and status directories environment variables","text":"<p>Cloud Pak Deployer uses the status directory to log its activities and also to keep track of its running state. For a given environment you're provisioning or destroying, you should always specify the same status directory to avoid contention between different deploy runs. </p> <pre><code>export CONFIG_DIR=$HOME/cpd-config\nexport STATUS_DIR=$HOME/cpd-status\n</code></pre> <ul> <li><code>CONFIG_DIR</code>: Directory that holds the configuration, it must have a <code>config</code> subdirectory which contains the configuration <code>yaml</code> files.</li> <li><code>STATUS_DIR</code>: The directory where the Cloud Pak Deployer keeps all status information and logs files.</li> </ul>"},{"location":"10-use-deployer/3-run/azure-aro/#optional-advanced-configuration","title":"Optional: advanced configuration","text":"<p>If the deployer configuration is kept on GitHub, follow the instructions in GitHub configuration.</p> <p>For special configuration with defaults and dynamic variables, refer to Advanced configuration.</p>"},{"location":"10-use-deployer/3-run/azure-aro/#2-prepare-the-cloud-environment","title":"2. Prepare the cloud environment","text":""},{"location":"10-use-deployer/3-run/azure-aro/#install-the-azure-cli-tool","title":"Install the Azure CLI tool","text":"<p>Install Azure CLI tool, and run the commands in your operating system.</p>"},{"location":"10-use-deployer/3-run/azure-aro/#verify-your-quota-and-permissions-in-microsoft-azure","title":"Verify your quota and permissions in Microsoft Azure","text":"<ul> <li>Check Azure resource quota of the subscription - Azure Red Hat OpenShift requires a minimum of 40 cores to create and run an OpenShift cluster.</li> <li>The ARO cluster is provisioned using the <code>az</code> command. Ideally, one has to have <code>Contributor</code> permissions on the subscription (Azure resources) and <code>Application administrator</code> role assigned in the Azure Active Directory. See details here.</li> </ul>"},{"location":"10-use-deployer/3-run/azure-aro/#set-environment-variables-for-azure","title":"Set environment variables for Azure","text":"<pre><code>export AZURE_RESOURCE_GROUP=pluto-01-rg\nexport AZURE_LOCATION=westeurope\nexport AZURE_SP=pluto-01-sp\n</code></pre> <ul> <li><code>AZURE_RESOURCE_GROUP</code>: The Azure resource group that will hold all resources belonging to the cluster: VMs, load balancers, virtual networks, subnets, etc.. Typically you will create a resource group for every OpenShift cluster you provision.</li> <li><code>AZURE_LOCATION</code>: The Azure location of the resource group, for example <code>useast</code> or <code>westeurope</code>.</li> <li><code>AZURE_SP</code>: Azure service principal that is used to create the resources on Azure. You will get the service principal from the Azure administrator.</li> </ul>"},{"location":"10-use-deployer/3-run/azure-aro/#store-service-principal-credentials","title":"Store Service Principal credentials","text":"<p>You must run the OpenShift installation using an Azure Service Principal with sufficient permissions. The Azure account administrator will share the SP credentials as a JSON file. If you have subscription-level access you can also create the Service Principal yourself. See steps in Create Azure service principal.</p> <p>Example output in credentials file: <pre><code>{\n  \"appId\": \"a4c39ae9-f9d1-4038-b4a4-ab011e769111\",\n  \"displayName\": \"pluto-01-sp\",\n  \"password\": \"xyz-xyz\",\n  \"tenant\": \"869930ac-17ee-4dda-bbad-7354c3e7629c8\"\n}\n</code></pre></p> <p>Store this file as <code>/tmp/${AZURE_SP}-credentials.json</code>.</p>"},{"location":"10-use-deployer/3-run/azure-aro/#login-as-service-principal","title":"Login as Service Principal","text":"<p>Login as the service principal: <pre><code>az login --service-principal -u a4c39ae9-f9d1-4038-b4a4-ab011e769111 -p xyz-xyz --tenant 869930ac-17ee-4dda-bbad-7354c3e7629c8\n</code></pre></p>"},{"location":"10-use-deployer/3-run/azure-aro/#register-resource-providers","title":"Register Resource Providers","text":"<p>Make sure the following Resource Providers are registered for your subscription by running:</p> <pre><code>az provider register -n Microsoft.RedHatOpenShift --wait\naz provider register -n Microsoft.Compute --wait\naz provider register -n Microsoft.Storage --wait\naz provider register -n Microsoft.Authorization --wait\n</code></pre>"},{"location":"10-use-deployer/3-run/azure-aro/#create-the-resource-group","title":"Create the resource group","text":"<p>First the resource group must be created; this resource group must match the one configured in your OpenShift yaml config file. <pre><code>az group create \\\n  --name ${AZURE_RESOURCE_GROUP} \\\n  --location ${AZURE_LOCATION}\n</code></pre></p>"},{"location":"10-use-deployer/3-run/azure-aro/#3-acquire-entitlement-keys-and-secrets","title":"3. Acquire entitlement keys and secrets","text":"<p>If you want to pull the Cloud Pak images from the entitled registry (i.e. an online install), or if you want to mirror the images to your private registry, you need to download the entitlement key. You can skip this step if you're installing from a private registry and all Cloud Pak images have already been downloaded to the private registry.</p> <ul> <li>Navigate to https://myibm.ibm.com/products-services/containerlibrary and login with your IBMId credentials</li> <li>Select Get Entitlement Key and create a new key (or copy your existing key)</li> <li>Copy the key value</li> </ul> <p>Warning</p> <p>As stated for the API key, you can choose to download the entitlement key to a file. However, when we reference the entitlement key, we mean the 80+ character string that is displayed, not the file.</p>"},{"location":"10-use-deployer/3-run/azure-aro/#acquire-an-openshift-pull-secret","title":"Acquire an OpenShift pull secret","text":"<p>To install OpenShift you need an OpenShift pull secret which holds your entitlement.</p> <ul> <li>Navigate to https://console.redhat.com/openshift/install/pull-secret and download the pull secret into file <code>/tmp/ocp_pullsecret.json</code></li> </ul>"},{"location":"10-use-deployer/3-run/azure-aro/#4-set-environment-variables-and-secrets","title":"4. Set environment variables and secrets","text":""},{"location":"10-use-deployer/3-run/azure-aro/#create-the-secrets-needed-for-aro-deployment","title":"Create the secrets needed for ARO deployment","text":"<p>You need to store the OpenShift pull secret and service principal credentials in the vault so that the deployer has access to it.</p> <pre><code>cp-deploy.sh vault set \\\n    --vault-secret ocp-pullsecret \\\n    --vault-secret-file /tmp/ocp_pullsecret.json\n\n\ncp-deploy.sh vault set \\\n    --vault-secret ${AZURE_SP}-credentials \\\n    --vault-secret-file /tmp/${AZURE_SP}-credentials.json\n</code></pre>"},{"location":"10-use-deployer/3-run/azure-aro/#optional-set-the-github-personal-access-token-pat","title":"Optional: Set the GitHub Personal Access Token (PAT)","text":"<p>In some cases, download of the <code>cloudctl</code> and <code>cpd-cli</code> clients from @IBM will fail because GitHub limits the number of API calls from non-authenticated clients. You can remediate this issue by creating a Personal Access Token on github.com and creating a secret in the vault.</p> <pre><code>cp-deploy.sh vault set -vs github-ibm-pat=&lt;your PAT&gt;\n</code></pre> <p>Alternatively, you can set the secret by adding <code>-vs github-ibm-pat=&lt;your PAT&gt;</code> to the <code>cp-deploy.sh env apply</code> command.</p>"},{"location":"10-use-deployer/3-run/azure-aro/#5-run-the-deployer","title":"5. Run the deployer","text":""},{"location":"10-use-deployer/3-run/azure-aro/#set-path-and-alias-for-the-deployer","title":"Set path and alias for the deployer","text":"<pre><code>source ./set-env.sh\n</code></pre>"},{"location":"10-use-deployer/3-run/azure-aro/#optional-validate-the-configuration","title":"Optional: validate the configuration","text":"<p>If you only want to validate the configuration, you can run the dpeloyer with the <code>--check-only</code> argument. This will run the first stage to validate variables and vault secrets and then execute the generators.</p> <pre><code>cp-deploy.sh env apply --check-only --accept-all-licenses\n</code></pre>"},{"location":"10-use-deployer/3-run/azure-aro/#run-the-cloud-pak-deployer","title":"Run the Cloud Pak Deployer","text":"<p>To run the container using a local configuration input directory and a data directory where temporary and state is kept, use the example below. If you don't specify the status directory, the deployer will automatically create a temporary directory. Please note that the status directory will also hold secrets if you have configured a flat file vault. If you lose the directory, you will not be able to make changes to the configuration and adjust the deployment. It is best to specify a permanent directory that you can reuse later. If you specify an existing directory the current user must be the owner of the directory. Failing to do so may cause the container to fail with insufficient permissions.</p> <pre><code>cp-deploy.sh env apply --accept-all-licenses\n</code></pre> <p>You can also specify extra variables such as <code>env_id</code> to override the names of the objects referenced in the <code>.yaml</code> configuration files as <code>{{ env_id }}-xxxx</code>. For more information about the extra (dynamic) variables, see advanced configuration.</p> <p>The <code>--accept-all-licenses</code> flag is optional and confirms that you accept all licenses of the installed cartridges and instances. Licenses must be either accepted in the configuration files or at the command line.</p> <p>When running the command, the container will start as a daemon and the command will tail-follow the logs. You can press Ctrl-C at any time to interrupt the logging but the container will continue to run in the background.</p> <p>You can return to view the logs as follows:</p> <pre><code>cp-deploy.sh env logs\n</code></pre> <p>Deploying the infrastructure, preparing OpenShift and installing the Cloud Pak will take a long time, typically between 1-5 hours,dependent on which Cloud Pak cartridges you configured. For estimated duration of the steps, refer to Timings.</p> <p>If you need to interrupt the automation, use CTRL-C to stop the logging output and then use:</p> <pre><code>cp-deploy.sh env kill\n</code></pre>"},{"location":"10-use-deployer/3-run/azure-aro/#on-failure","title":"On failure","text":"<p>If the Cloud Pak Deployer fails, for example because certain infrastructure components are temporarily not available, fix the cause if needed and then just re-run it with the same <code>CONFIG_DIR</code> and <code>STATUS_DIR</code> as well extra variables. The provisioning process has been designed to be idempotent and it will not redo actions that have already completed successfully.</p>"},{"location":"10-use-deployer/3-run/azure-aro/#finishing-up","title":"Finishing up","text":"<p>Once the process has finished, it will output the URLs by which you can access the deployed Cloud Pak. You can also find this information under the <code>cloud-paks</code> directory in the status directory you specified.</p> <p>To retrieve the Cloud Pak URL(s):</p> <pre><code>cat $STATUS_DIR/cloud-paks/*\n</code></pre> <p>This will show the Cloud Pak URLs:</p> <pre><code>Cloud Pak for Data URL for cluster pluto-01 and project cpd (domain name specified was example.com):\nhttps://cpd-cpd.apps.pluto-01.example.com\n</code></pre> <p>The <code>admin</code> password can be retrieved from the vault as follows:</p> <p>List the secrets in the vault:</p> <pre><code>cp-deploy.sh vault list\n</code></pre> <p>This will show something similar to the following:</p> <pre><code>Secret list for group sample:\n- ibm_cp_entitlement_key\n- sample-provision-ssh-key\n- sample-provision-ssh-pub-key\n- cp4d_admin_zen_sample_sample\n</code></pre> <p>You can then retrieve the Cloud Pak for Data admin password like this:</p> <pre><code>cp-deploy.sh vault get --vault-secret cp4d_admin_zen_sample_sample\n</code></pre> <pre><code>PLAY [Secrets] *****************************************************************\nincluded: /automation_script/automation-roles/99-generic/vault/vault-get-secret/tasks/get-secret-file.yml for localhost\ncp4d_admin_zen_sample_sample: gelGKrcgaLatBsnAdMEbmLwGr\n</code></pre>"},{"location":"10-use-deployer/3-run/azure-aro/#post-install-configuration","title":"Post-install configuration","text":"<p>You can find examples of a couple of typical changes you may want to do here: Post-run changes.</p>"},{"location":"10-use-deployer/3-run/azure-self-managed/","title":"Running the Cloud Pak Deployer on Microsoft Azure - Self-managed","text":"<p>On Azure, OpenShift can be set up in various ways, managed by Red Hat (ARO) or self-managed. The steps below are applicable to the self-managed Red Hat OpenShift.</p> <p>There are 5 main steps to run the deployer for Azure:</p> <ol> <li>Configure deployer</li> <li>Prepare the cloud environment</li> <li>Obtain entitlement keys and secrets</li> <li>Set environment variables and secrets</li> <li>Run the deployer</li> <li>Post-install configuration (Add GPU nodes)</li> </ol>"},{"location":"10-use-deployer/3-run/azure-self-managed/#topology","title":"Topology","text":"<p>A typical setup of the OpenShift cluster on Azure is pictured below: </p>"},{"location":"10-use-deployer/3-run/azure-self-managed/#public-self-managed-openshift","title":"Public self-managed OpenShift","text":"<p>When deploying a public self-managed OpenShift on Azure, the <code>openshift.domain_name</code> domain name must be registered with a registrar. OpenShift will create a public DNS zone with additional entries to reach the OpenShift API and the applications (Cloud Paks). If you don't have a domain yet, you buy one from Azure: https://learn.microsoft.com/en-us/azure/app-service/manage-custom-dns-buy-domain.</p>"},{"location":"10-use-deployer/3-run/azure-self-managed/#private-self-managed-openshift","title":"Private self-managed OpenShift","text":"<p>When deploying a self-managed OpenShift in a private network on Azure, the <code>openshift.domain_name</code> references the private DNS zone that is created within the resource group that also holds the OpenShift control plane VMs, compute VMs and many other resources. For a private deployment, the virtual network (vnet) must exist already in a different resource group and it must have subnets for the control plane and compute plane. </p> <p>Info</p> <p>To ensure that the deployer can do a lookup of the OpenShift API server, it is easiest to run it on a server in the same existing vnet. If the OpenShift installer cannot contact the API server during the installation, it will fail and so will the deployer.</p>"},{"location":"10-use-deployer/3-run/azure-self-managed/#1-configure-deployer","title":"1. Configure deployer","text":""},{"location":"10-use-deployer/3-run/azure-self-managed/#deployer-configuration-and-status-directories","title":"Deployer configuration and status directories","text":"<p>Deployer reads the configuration from a directory you set in the <code>CONFIG_DIR</code> environment variable. A status directory (<code>STATUS_DIR</code> environment variable) is used to log activities, store temporary files, scripts. If you use a File Vault (default), the secrets are kept in the <code>$STATUS_DIR/vault</code> directory.</p> <p>You can find OpenShift and Cloud Pak sample configuration (yaml) files here: sample configuration. For Azure self-managed installations, copy one of <code>ocp-azure-self-managed*.yaml</code> files into the <code>$CONFIG_DIR/config</code> directory. If you also want to install a Cloud Pak, copy one of the <code>cp4*.yaml</code> files.</p> <p>Example: <pre><code>mkdir -p $HOME/cpd-config/config\ncp sample-configurations/sample-dynamic/config-samples/ocp-azure-self-managed.yaml $HOME/cpd-config/config/\ncp sample-configurations/sample-dynamic/config-samples/cp4d-471.yaml $HOME/cpd-config/config/\n</code></pre></p>"},{"location":"10-use-deployer/3-run/azure-self-managed/#set-configuration-and-status-directories-environment-variables","title":"Set configuration and status directories environment variables","text":"<p>Cloud Pak Deployer uses the status directory to log its activities and also to keep track of its running state. For a given environment you're provisioning or destroying, you should always specify the same status directory to avoid contention between different deploy runs. </p> <pre><code>export CONFIG_DIR=$HOME/cpd-config\nexport STATUS_DIR=$HOME/cpd-status\n</code></pre> <ul> <li><code>CONFIG_DIR</code>: Directory that holds the configuration, it must have a <code>config</code> subdirectory which contains the configuration <code>yaml</code> files.</li> <li><code>STATUS_DIR</code>: The directory where the Cloud Pak Deployer keeps all status information and logs files.</li> </ul>"},{"location":"10-use-deployer/3-run/azure-self-managed/#optional-advanced-configuration","title":"Optional: advanced configuration","text":"<p>If the deployer configuration is kept on GitHub, follow the instructions in GitHub configuration.</p> <p>For special configuration with defaults and dynamic variables, refer to Advanced configuration.</p>"},{"location":"10-use-deployer/3-run/azure-self-managed/#2-prepare-the-cloud-environment","title":"2. Prepare the cloud environment","text":""},{"location":"10-use-deployer/3-run/azure-self-managed/#install-the-azure-cli-tool","title":"Install the Azure CLI tool","text":"<p>Install Azure CLI tool, and run the commands in your operating system.</p>"},{"location":"10-use-deployer/3-run/azure-self-managed/#verify-your-quota-and-permissions-in-microsoft-azure","title":"Verify your quota and permissions in Microsoft Azure","text":"<ul> <li>Check Azure resource quota of the subscription - Azure Red Hat OpenShift requires a minimum of 40 cores to create and run an OpenShift cluster.</li> <li>The self-managed cluster is provisioned using the IPI installer command. Ideally, one has to have <code>Contributor</code> permissions on the subscription (Azure resources) and <code>Application administrator</code> role assigned in the Azure Active Directory. See details here.</li> </ul>"},{"location":"10-use-deployer/3-run/azure-self-managed/#set-environment-variables-for-azure","title":"Set environment variables for Azure","text":"<pre><code>export AZURE_RESOURCE_GROUP=pluto-01-rg\nexport AZURE_LOCATION=westeurope\nexport AZURE_SP=pluto-01-sp\n</code></pre> <ul> <li><code>AZURE_RESOURCE_GROUP</code>: The Azure resource group that will hold all resources belonging to the cluster: VMs, load balancers, virtual networks, subnets, etc.. Typically you will create a resource group for every OpenShift cluster you provision.</li> <li><code>AZURE_LOCATION</code>: The Azure location of the resource group, for example <code>useast</code> or <code>westeurope</code>.</li> <li><code>AZURE_SP</code>: Azure service principal that is used to create the resources on Azure. You will get the service principal from the Azure administrator.</li> </ul>"},{"location":"10-use-deployer/3-run/azure-self-managed/#create-or-check-existence-of-the-virtual-network-for-private-openshift-installations","title":"Create or check existence of the virtual network for private OpenShift installations","text":"<p>If you install OpenShift with private endpoints, the vnet must already exist in a different resource gruop from the <code>$AZURE_RESOURCE_GROUP</code>. For convenience, this resource group is referred to as the <code>AZURE_NETWORK_RESOURCE_GROUP</code>. Plese check the following:</p> <ul> <li>The vnet exists and is in the same <code>AZURE_LOCATION</code> as the OpenShift cluster</li> <li>The vnet has 2 subnets, one for the control plane VMs and one for the compute VMs</li> <li>The service principal has <code>Contributor</code> and <code>User Access Administrator</code> permissions on the <code>AZURE_NETWORK_RESOURCE_GROUP</code> that holds the vnet. Check the instructions to create the role assignments</li> </ul>"},{"location":"10-use-deployer/3-run/azure-self-managed/#create-the-resource-group-if-not-already-done","title":"Create the resource group (if not already done)","text":"<p>First the resource group must be created; this resource group must match the one configured in your OpenShift yaml config file. Create the resource group if this was not already done by the Azure administrator. <pre><code>az group create \\\n  --name ${AZURE_RESOURCE_GROUP} \\\n  --location ${AZURE_LOCATION}\n</code></pre></p>"},{"location":"10-use-deployer/3-run/azure-self-managed/#store-service-principal-credentials","title":"Store Service Principal credentials","text":"<p>You must run the OpenShift installation using an Azure Service Principal with sufficient permissions. The Azure account administrator will share the SP credentials as a JSON file. If you have subscription-level access you can also create the Service Principal yourself. See steps in Create Azure service principal.</p> <p>Example output in credentials file: <pre><code>{\n  \"appId\": \"a4c39ae9-f9d1-4038-b4a4-ab011e769111\",\n  \"displayName\": \"pluto-01-sp\",\n  \"password\": \"xyz-xyz\",\n  \"tenant\": \"869930ac-17ee-4dda-bbad-7354c3e7629c8\"\n}\n</code></pre></p> <p>Store this file as <code>/tmp/${AZURE_SP}-credentials.json</code>.</p>"},{"location":"10-use-deployer/3-run/azure-self-managed/#3-acquire-entitlement-keys-and-secrets","title":"3. Acquire entitlement keys and secrets","text":""},{"location":"10-use-deployer/3-run/azure-self-managed/#acquire-ibm-cloud-pak-entitlement-key","title":"Acquire IBM Cloud Pak entitlement key","text":"<p>If you want to pull the Cloud Pak images from the entitled registry (i.e. an online install), or if you want to mirror the images to your private registry, you need to download the entitlement key. You can skip this step if you're installing from a private registry and all Cloud Pak images have already been downloaded to the private registry.</p> <ul> <li>Navigate to https://myibm.ibm.com/products-services/containerlibrary and login with your IBMId credentials</li> <li>Select Get Entitlement Key and create a new key (or copy your existing key)</li> <li>Copy the key value</li> </ul> <p>Warning</p> <p>As stated for the API key, you can choose to download the entitlement key to a file. However, when we reference the entitlement key, we mean the 80+ character string that is displayed, not the file.</p>"},{"location":"10-use-deployer/3-run/azure-self-managed/#acquire-an-openshift-pull-secret","title":"Acquire an OpenShift pull secret","text":"<p>To install OpenShift you need an OpenShift pull secret which holds your entitlement.</p> <ul> <li>Navigate to https://console.redhat.com/openshift/install/pull-secret and download the pull secret into file <code>/tmp/ocp_pullsecret.json</code></li> </ul>"},{"location":"10-use-deployer/3-run/azure-self-managed/#optional-locate-or-generate-a-public-ssh-key","title":"Optional: Locate or generate a public SSH Key","text":"<p>To obtain access to the OpenShift nodes post-installation, you will need to specify the public SSH key of your server; typically this is <code>~/.ssh/id_rsa.pub</code>, where <code>~</code> is the home directory of your user. If you don't have an SSH key-pair yet, you can generate one using the steps documented here: https://cloud.ibm.com/docs/ssh-keys?topic=ssh-keys-generating-and-using-ssh-keys-for-remote-host-authentication#generating-ssh-keys-on-linux. Alternatively, deployer can generate SSH key-pair automatically if credential <code>ocp-ssh-pub-key</code> is not in the vault.</p>"},{"location":"10-use-deployer/3-run/azure-self-managed/#4-set-environment-variables-and-secrets","title":"4. Set environment variables and secrets","text":""},{"location":"10-use-deployer/3-run/azure-self-managed/#set-the-cloud-pak-entitlement-key","title":"Set the Cloud Pak entitlement key","text":"<p>If you want the Cloud Pak images to be pulled from the entitled registry, set the Cloud Pak entitlement key.</p> <pre><code>export CP_ENTITLEMENT_KEY=your_cp_entitlement_key\n</code></pre> <ul> <li><code>CP_ENTITLEMENT_KEY</code>: This is the entitlement key you acquired as per the instructions above, this is a 80+ character string. You don't need to set this environment variable when you install the Cloud Pak(s) from a private registry</li> </ul>"},{"location":"10-use-deployer/3-run/azure-self-managed/#create-the-secrets-needed-for-self-managed-openshift-cluster","title":"Create the secrets needed for self-managed OpenShift cluster","text":"<p>You need to store the OpenShift pull secret and service principal credentials in the vault so that the deployer has access to it.</p> <pre><code>cp-deploy.sh vault set \\\n    --vault-secret ocp-pullsecret \\\n    --vault-secret-file /tmp/ocp_pullsecret.json\n\n\ncp-deploy.sh vault set \\\n    --vault-secret ${AZURE_SP}-credentials \\\n    --vault-secret-file /tmp/${AZURE_SP}-credentials.json\n</code></pre>"},{"location":"10-use-deployer/3-run/azure-self-managed/#optional-create-secret-for-public-ssh-key","title":"Optional: Create secret for public SSH key","text":"<p>If you want to use your SSH key to access nodes in the cluster, set the Vault secret with the public SSH key. <pre><code>cp-deploy.sh vault set \\\n    --vault-secret ocp-ssh-pub-key \\\n    --vault-secret-file ~/.ssh/id_rsa.pub\n</code></pre></p>"},{"location":"10-use-deployer/3-run/azure-self-managed/#optional-set-the-github-personal-access-token-pat","title":"Optional: Set the GitHub Personal Access Token (PAT)","text":"<p>In some cases, download of the <code>cloudctl</code> and <code>cpd-cli</code> clients from @IBM will fail because GitHub limits the number of API calls from non-authenticated clients. You can remediate this issue by creating a Personal Access Token on github.com and creating a secret in the vault.</p> <pre><code>cp-deploy.sh vault set -vs github-ibm-pat=&lt;your PAT&gt;\n</code></pre> <p>Alternatively, you can set the secret by adding <code>-vs github-ibm-pat=&lt;your PAT&gt;</code> to the <code>cp-deploy.sh env apply</code> command.</p>"},{"location":"10-use-deployer/3-run/azure-self-managed/#5-run-the-deployer","title":"5. Run the deployer","text":""},{"location":"10-use-deployer/3-run/azure-self-managed/#set-path-and-alias-for-the-deployer","title":"Set path and alias for the deployer","text":"<pre><code>source ./set-env.sh\n</code></pre>"},{"location":"10-use-deployer/3-run/azure-self-managed/#optional-validate-the-configuration","title":"Optional: validate the configuration","text":"<p>If you only want to validate the configuration, you can run the dpeloyer with the <code>--check-only</code> argument. This will run the first stage to validate variables and vault secrets and then execute the generators.</p> <pre><code>cp-deploy.sh env apply --check-only --accept-all-licenses\n</code></pre>"},{"location":"10-use-deployer/3-run/azure-self-managed/#run-the-cloud-pak-deployer","title":"Run the Cloud Pak Deployer","text":"<p>To run the container using a local configuration input directory and a data directory where temporary and state is kept, use the example below. If you don't specify the status directory, the deployer will automatically create a temporary directory. Please note that the status directory will also hold secrets if you have configured a flat file vault. If you lose the directory, you will not be able to make changes to the configuration and adjust the deployment. It is best to specify a permanent directory that you can reuse later. If you specify an existing directory the current user must be the owner of the directory. Failing to do so may cause the container to fail with insufficient permissions.</p> <pre><code>cp-deploy.sh env apply --accept-all-licenses\n</code></pre> <p>You can also specify extra variables such as <code>env_id</code> to override the names of the objects referenced in the <code>.yaml</code> configuration files as <code>{{ env_id }}-xxxx</code>. For more information about the extra (dynamic) variables, see advanced configuration.</p> <p>The <code>--accept-all-licenses</code> flag is optional and confirms that you accept all licenses of the installed cartridges and instances. Licenses must be either accepted in the configuration files or at the command line.</p> <p>When running the command, the container will start as a daemon and the command will tail-follow the logs. You can press Ctrl-C at any time to interrupt the logging but the container will continue to run in the background.</p> <p>You can return to view the logs as follows:</p> <pre><code>cp-deploy.sh env logs\n</code></pre> <p>Deploying the infrastructure, preparing OpenShift and installing the Cloud Pak will take a long time, typically between 1-5 hours,dependent on which Cloud Pak cartridges you configured. For estimated duration of the steps, refer to Timings.</p> <p>If you need to interrupt the automation, use CTRL-C to stop the logging output and then use:</p> <pre><code>cp-deploy.sh env kill\n</code></pre>"},{"location":"10-use-deployer/3-run/azure-self-managed/#on-failure","title":"On failure","text":"<p>If the Cloud Pak Deployer fails, for example because certain infrastructure components are temporarily not available, fix the cause if needed and then just re-run it with the same <code>CONFIG_DIR</code> and <code>STATUS_DIR</code> as well extra variables. The provisioning process has been designed to be idempotent and it will not redo actions that have already completed successfully.</p>"},{"location":"10-use-deployer/3-run/azure-self-managed/#finishing-up","title":"Finishing up","text":"<p>Once the process has finished, it will output the URLs by which you can access the deployed Cloud Pak. You can also find this information under the <code>cloud-paks</code> directory in the status directory you specified.</p> <p>To retrieve the Cloud Pak URL(s):</p> <pre><code>cat $STATUS_DIR/cloud-paks/*\n</code></pre> <p>This will show the Cloud Pak URLs:</p> <pre><code>Cloud Pak for Data URL for cluster pluto-01 and project cpd (domain name specified was example.com):\nhttps://cpd-cpd.apps.pluto-01.example.com\n</code></pre> <p>The <code>admin</code> password can be retrieved from the vault as follows:</p> <p>List the secrets in the vault:</p> <pre><code>cp-deploy.sh vault list\n</code></pre> <p>This will show something similar to the following:</p> <pre><code>Secret list for group sample:\n- ibm_cp_entitlement_key\n- sample-provision-ssh-key\n- sample-provision-ssh-pub-key\n- cp4d_admin_cpd_demo\n</code></pre> <p>You can then retrieve the Cloud Pak for Data admin password like this:</p> <pre><code>cp-deploy.sh vault get --vault-secret cp4d_admin_zen_sample_sample\n</code></pre> <pre><code>PLAY [Secrets] *****************************************************************\nincluded: /automation_script/automation-roles/99-generic/vault/vault-get-secret/tasks/get-secret-file.yml for localhost\ncp4d_admin_zen_sample_sample: gelGKrcgaLatBsnAdMEbmLwGr\n</code></pre>"},{"location":"10-use-deployer/3-run/azure-self-managed/#6-post-install-configuration","title":"6. Post-install configuration","text":"<p>You can find examples of a couple of typical changes you may want to do here: Post-run changes:</p> <ul> <li>Update the Cloud Pak for Data administrator password</li> <li>Add GPU node(s) to your OpenShift cluster</li> </ul>"},{"location":"10-use-deployer/3-run/azure-service-principal/","title":"Create an Azure Service Principal","text":""},{"location":"10-use-deployer/3-run/azure-service-principal/#login-to-azure","title":"Login to Azure","text":"<p>Login to the Microsoft Azure using your subscription-level credentials. <pre><code>az login\n</code></pre></p> <p>If you have a subscription with multiple tenants, use: <pre><code>az login --tenant &lt;TENANT_ID&gt;\n</code></pre></p> <p>Example: <pre><code>az login --tenant 869930ac-17ee-4dda-bbad-7354c3e7629c8\nTo sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code AXWFQQ5FJ to authenticate.\n[\n  {\n    \"cloudName\": \"AzureCloud\",\n    \"homeTenantId\": \"869930ac-17ee-4dda-bbad-7354c3e7629c8\",\n    \"id\": \"72281667-6d54-46cb-8423-792d7bcb1234\",\n    \"isDefault\": true,\n    \"managedByTenants\": [],\n    \"name\": \"Azure Account\",\n    \"state\": \"Enabled\",\n    \"tenantId\": \"869930ac-17ee-4dda-bbad-7354c3e7629c8\",\n    \"user\": {\n      \"name\": \"your_user@domain.com\",\n      \"type\": \"user\"\n    }\n  }\n]\n</code></pre></p>"},{"location":"10-use-deployer/3-run/azure-service-principal/#set-subscription-optional","title":"Set subscription (optional)","text":"<p>If you have multiple Azure subscriptions, specify the relevant subscription ID: <code>az account set --subscription &lt;SUBSCRIPTION_ID&gt;</code></p> <p>You can list the subscriptions via command: <pre><code>az account subscription list\n</code></pre></p> <pre><code>[\n  {\n    \"authorizationSource\": \"RoleBased\",\n    \"displayName\": \"IBM xxx\",\n    \"id\": \"/subscriptions/dcexxx\",\n    \"state\": \"Enabled\",\n    \"subscriptionId\": \"dcexxx\",\n    \"subscriptionPolicies\": {\n      \"locationPlacementId\": \"Public_2014-09-01\",\n      \"quotaId\": \"EnterpriseAgreement_2014-09-01\",\n      \"spendingLimit\": \"Off\"\n    }\n  }\n]\n</code></pre>"},{"location":"10-use-deployer/3-run/azure-service-principal/#create-service-principal","title":"Create service principal","text":"<p>Create the service principal that will do the installation and assign the <code>Contributor role</code></p>"},{"location":"10-use-deployer/3-run/azure-service-principal/#set-environment-variables-for-azure","title":"Set environment variables for Azure","text":"<pre><code>export AZURE_SUBSCRIPTION_ID=72281667-6d54-46cb-8423-792d7bcb1234\nexport AZURE_LOCATION=westeurope\nexport AZURE_RESOURCE_GROUP=pluto-01-rg\nexport AZURE_SP=pluto-01-sp\n</code></pre> <ul> <li><code>AZURE_SUBSCRIPTION_ID</code>: The id of your Azure subscription. Once logged in, you can retrieve this using the <code>az account show</code> command.</li> <li><code>AZURE_LOCATION</code>: The Azure location of the resource group, for example <code>useast</code> or <code>westeurope</code>.</li> <li><code>AZURE_SP</code>: Azure service principal that is used to create the resources on Azure.</li> </ul>"},{"location":"10-use-deployer/3-run/azure-service-principal/#create-a-service-principal-with-subscription-permissions","title":"Create a service principal with subscription permissions","text":"<p>In the situation where you have full access to the Azure subscription, you can create a service principal with subscription-level roles. <pre><code>az ad sp create-for-rbac \\\n  --role Contributor \\\n  --name ${AZURE_SP} \\\n  --scopes /subscriptions/${AZURE_SUBSCRIPTION_ID} | tee /tmp/${AZURE_SP}-credentials.json\n</code></pre></p> <p>Example output: <pre><code>{\n  \"appId\": \"a4c39ae9-f9d1-4038-b4a4-ab011e769111\",\n  \"displayName\": \"pluto-01-sp\",\n  \"password\": \"xyz-xyz\",\n  \"tenant\": \"869930ac-17ee-4dda-bbad-7354c3e7629c8\"\n}\n</code></pre></p> <p>Set the permissions of the service principal to allow creation of the OpenShift cluster. <pre><code>az role assignment create \\\n  --role \"User Access Administrator\" \\\n  --scope /subscriptions/${AZURE_SUBSCRIPTION_ID} \\\n  --assignee-object-id $(az ad sp list --display-name=${AZURE_SP} --query='[].id' -o tsv)\n</code></pre></p>"},{"location":"10-use-deployer/3-run/azure-service-principal/#create-a-service-principal-with-resource-group-permissions","title":"Create a service principal with resource group permissions","text":"<p>If the service principal must not have permissions at the subscription-level, you can grant permissions for the resource group(s) the service principal needs to create resources in.</p> <p>First you have to make sure that the resource group exists: <pre><code>az group create \\\n  --name ${AZURE_RESOURCE_GROUP} \\\n  --location ${AZURE_LOCATION}\n</code></pre></p> <pre><code>az ad sp create-for-rbac \\\n  --role Contributor \\\n  --name ${AZURE_SP} \\\n  --scopes /subscriptions/${AZURE_SUBSCRIPTION_ID}/resourceGroups/${AZURE_RESOURCE_GROUP} | tee /tmp/${AZURE_SP}-credentials.json\n</code></pre> <p>Example output: <pre><code>{\n  \"appId\": \"a4c39ae9-f9d1-4038-b4a4-ab011e769111\",\n  \"displayName\": \"pluto-01-sp\",\n  \"password\": \"xyz-xyz\",\n  \"tenant\": \"869930ac-17ee-4dda-bbad-7354c3e7629c8\"\n}\n</code></pre></p> <p>Set the permissions of the service principal to allow creation of the OpenShift cluster. <pre><code>az role assignment create \\\n  --role \"User Access Administrator\" \\\n  --scope /subscriptions/${AZURE_SUBSCRIPTION_ID}/resourceGroups/${AZURE_RESOURCE_GROUP} \\\n  --assignee-object-id $(az ad sp list --display-name=${AZURE_SP} --query='[].id' -o tsv)\n</code></pre></p>"},{"location":"10-use-deployer/3-run/azure-service-principal/#set-permissions-for-additional-resource-groups","title":"Set permissions for additional resource groups","text":"<p>There are scenarios where the permissions must be set for additional resource groups, for example when the Azure virtual network is in a different resource group than the OpenShift cluster. This is common in private network installations.</p> <pre><code>export AZURE_NETWORK_RESOURCE_GROUP=pluto-01-network-rg\n</code></pre> <p>Set the permissions of the service principal to allow updates to the network resource group and allow creation of the OpenShift cluster. <pre><code>az role assignment create \\\n  --role \"Contributor\" \\\n  --scope /subscriptions/${AZURE_SUBSCRIPTION_ID}/resourceGroups/${AZURE_NETWORK_RESOURCE_GROUP} \\\n  --assignee-object-id $(az ad sp list --display-name=${AZURE_SP} --query='[].id' -o tsv)\n\naz role assignment create \\\n  --role \"User Access Administrator\" \\\n  --scope /subscriptions/${AZURE_SUBSCRIPTION_ID}/resourceGroups/${AZURE_NETWORK_RESOURCE_GROUP} \\\n  --assignee-object-id $(az ad sp list --display-name=${AZURE_SP} --query='[].id' -o tsv)\n</code></pre></p>"},{"location":"10-use-deployer/3-run/existing-openshift-console-debug-job/","title":"Existing openshift console debug job","text":""},{"location":"10-use-deployer/3-run/existing-openshift-console-debug-job/#start-the-deployer-debug-job-only","title":"Start the deployer debug job only","text":"<ul> <li>Go to the OpenShift console</li> <li>Click the \"+\" sign at the top of the page</li> <li>Paste the following block into the window. You can update the image on line 11 and the same value will be used for image for the Deployer Job (From release v3.0.2 onwards).</li> </ul> Start the deployer debug job <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: cloud-pak-deployer-start\n  generateName: cloud-pak-deployer-start-\n  namespace: cloud-pak-deployer\nspec:\n  containers:\n  - name: cloud-pak-deployer\n    image: quay.io/cloud-pak-deployer/cloud-pak-deployer:latest\n    imagePullPolicy: Always\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    command: [\"/bin/sh\",\"-xc\"]\n    args: \n      - /cloud-pak-deployer/scripts/deployer/cpd-start-deployer.sh --debug-only\n    envFrom:\n    - configMapRef:\n        name: cloud-pak-deployer-env\n        optional: true\n  restartPolicy: Never\n  securityContext:\n    runAsUser: 0\n  serviceAccountName: cloud-pak-deployer-sa\n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift-console/","title":"Running deployer on OpenShift using console","text":"<p>See the deployer in action deploying IBM watsonx.ai on an existing OpenShift cluster in this video:</p>"},{"location":"10-use-deployer/3-run/existing-openshift-console/#log-in-to-the-openshift-cluster","title":"Log in to the OpenShift cluster","text":"<p>Log in as a cluster administrator to be able to run the deployer with the correct permissions.</p>"},{"location":"10-use-deployer/3-run/existing-openshift-console/#techzone-clusters-and-watson-studio-pipelines-pipeline-orchestration","title":"TechZone clusters and Watson Studio Pipelines (Pipeline Orchestration)","text":"<p>OpenShift Pipelines must be removed manually</p> <p>The Watson Studio Pipelines cartridge (also referred to as Pipeline Orchestration) installs its own Tekton controllers. TechZone OpenShift clusters already include the Red Hat OpenShift Pipelines operator that owns the same Tekton resources, so the install fails if both are present. The Cloud Pak Deployer does not uninstall shared cluster components. Remove OpenShift Pipelines yourself before you add <code>ws-pipelines</code> to the configuration.</p> <p>Follow these steps once per cluster:</p> <ol> <li>Sign in to the OpenShift web console with a cluster-admin user.</li> <li>Go to <code>Operators</code> \u2192 <code>Installed Operators</code>, open OpenShift Pipelines (namespace <code>openshift-operators</code>), choose Actions \u2192 Uninstall, keep Delete operand resources selected, and confirm.</li> <li>Wait until the operator disappears from the list and the <code>openshift-pipelines-operator</code> CSV is removed.</li> <li>Verify that no <code>TektonConfig</code> instance is left. The following command should return No resources found:    <pre><code>oc get tektonconfig\n</code></pre></li> <li>If a <code>tekton-pipelines</code> project still exists, delete it to remove the remaining webhooks.</li> </ol> <p>TechZone automation itself runs on OpenShift Pipelines, so uninstalling it stops the TechZone pipeline that would normally start the deployer. After completing the above steps, start the deployer directly from the OpenShift console (as described below). You can reinstall OpenShift Pipelines from OperatorHub again after Watson Studio Pipelines has been successfully deployed.</p>"},{"location":"10-use-deployer/3-run/existing-openshift-console/#prepare-the-deployer-project","title":"Prepare the deployer project","text":"<ul> <li>Go to the OpenShift console</li> <li>Click the \"+\" sign at the top of the page</li> <li>Paste the following block (exactly) into the window</li> </ul> Prepare the deployer project <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: null\n  name: cloud-pak-deployer\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: cloud-pak-deployer-sa\n  namespace: cloud-pak-deployer\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: system:openshift:scc:privileged\n  namespace: cloud-pak-deployer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:openshift:scc:privileged\nsubjects:\n- kind: ServiceAccount\n  name: cloud-pak-deployer-sa\n  namespace: cloud-pak-deployer\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: cloud-pak-deployer-cluster-admin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: cloud-pak-deployer-sa\n  namespace: cloud-pak-deployer\n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift-console/#set-the-entitlement-key","title":"Set the entitlement key","text":"<ul> <li>Update the secret below with your container software Entitlement key from https://myibm.ibm.com/products-services/containerlibrary. Make sure the key is indented exactly as below.</li> <li>Go to the OpenShift console</li> <li>Click the \"+\" sign at the top of the page</li> <li>Paste the following block with replaced YOUR_ENTITLEMENT_KEY on line 10</li> </ul> Set the entitlement key <pre><code>---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: cloud-pak-entitlement-key\n  namespace: cloud-pak-deployer\ntype: Opaque\nstringData:\n  cp-entitlement-key: |\n    YOUR_ENTITLEMENT_KEY\n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift-console/#configure-the-cloud-paks-and-services-to-be-deployed","title":"Configure the Cloud Paks and services to be deployed","text":"<ul> <li>Update the configuration below to match what you want to deploy, do not change indent</li> <li>Go to the OpenShift console</li> <li>Click the \"+\" sign at the top of the page</li> <li>Paste the below block (exactly into the window)</li> <li>Update the <code>cp4d</code> element and select the version and cartridges you want to install</li> </ul> <p>Info</p> <p>The below is an example of a Software Hub installation. Other example configurations:</p> <ul> <li> <p>Software Hub with Keycloak as the identity provider</p> </li> <li> <p>Cloud Pak for Integration</p> </li> </ul> Sample CP4D configuration <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cloud-pak-deployer-config\n  namespace: cloud-pak-deployer\ndata:\n  cpd-config.yaml: |\n    global_config:\n      environment_name: sample\n      cloud_platform: existing-ocp\n      env_id: pluto-01\n      confirm_destroy: False\n      optimize_deploy: True\n    \n    openshift:\n    - name: \"{{ env_id }}\"\n      ocp_version: detect\n      cluster_name: \"{{ env_id }}\"\n      domain_name: example.com\n      mcg:\n        install: True\n        storage_type: storage-class\n        storage_class: managed-nfs-storage\n      gpu:\n        install: auto\n      openshift_ai:\n        install: auto\n        channel: auto\n      openshift_storage:\n      - storage_name: auto-storage\n        storage_type: auto\n\n    \n    cp4d:\n    - project: cpd\n      openshift_cluster_name: \"{{ env_id }}\"\n      cp4d_version: latest\n      cp4d_entitlement: \n      - cpd-enterprise\n      # - cpd-standard\n      # - cognos-analytics\n      # - data-product-hub\n      # - datastage\n      # - data-integration-unstructured-data\n      # - data-lineage\n      # - ikc-premium\n      # - ikc-standard\n      # - openpages\n      # - planning-analytics\n      # - product-master\n      # - speech-to-text\n      # - text-to-speech\n      # - watson-assistant\n      # - watson-discovery\n      # - watsonx-ai\n      # - watsonx-code-assistant-ansible\n      # - watsonx-code-assistant-z\n      # - watsonx-data\n      # - watsonx-gov-mm\n      # - watsonx-gov-rc\n      # - watsonx-orchestrate\n      cp4d_production_license: True\n      accept_licenses: True\n      db2u_limited_privileges: False\n      use_fs_iam: True\n      operators_project: cpd-operators\n      ibm_cert_manager: False\n      state: installed\n      cartridges:\n      - name: cp-foundation\n        scale: level_1\n        license_service:\n          threads_per_core: 2\n      \n      - name: lite\n    \n      - name: scheduler \n        state: removed\n        \n    #\n    # All tested cartridges. To install, change the \"state\" property to \"installed\". To uninstall, change the state\n    # to \"removed\" or comment out the entire cartridge. Make sure that the \"-\" and properties are aligned with the lite\n    # cartridge; the \"-\" is at position 3 and the property starts at position 5.\n    #\n    # If a cartridge has dependencies and you want to install it, you must ensure that the dependent cartridge is also\n    # installed.\n    #\n    \n      - name: analyticsengine \n        description: Analytics Engine Powered by Apache Spark \n        size: small \n        state: removed\n        installation_options:\n          sparkAdvEnabled: true\n          jobAutoDeleteEnabled: true\n          kernelCullTime: 30\n          imagePullParallelism: \"40\"\n          imagePullCompletions: \"20\"\n          kernelCleanupSchedule: \"*/30 * * * *\"\n          jobCleanupSchedule: \"*/30 * * * *\"\n          skipSelinuxRelabeling: false\n          mountCustomizationsFromCchome: false\n          maxDriverCpuCores: 5\n          maxExecutorCpuCores: 5\n          maxDriveMemory: \"50g\"\n          maxExecutorMemory: \"50g\"\n          maxNumWorkers: 50\n          localDirScaleFactor: 10\n    \n      - name: bigsql\n        description: Db2 Big SQL\n        state: removed\n    \n      - name: ca\n        description: Cognos Analytics\n        size: small\n        instances:\n        - name: ca-instance\n          metastore_ref: ca-metastore\n        state: removed\n    \n      - name: dashboard\n        description: Cognos Dashboards\n        state: removed\n    \n      - name: datagate\n        description: Db2 Data Gate\n        state: removed\n    \n      - name: datalineage\n        description: IBM MANTA Data Lineage\n        size: small\n        state: removed\n    \n      - name: dataproduct\n        description: Data Product Hub\n        state: removed\n        \n      - name: datastage-ent\n        description: DataStage Enterprise\n        state: removed\n    \n      - name: datastage-ent-plus\n        description: DataStage Enterprise Plus\n        state: removed\n        # instances:\n        #   - name: ds-instance\n        #     # Optional settings\n        #     description: \"datastage ds-instance\"\n        #     size: medium\n        #     storage_class: efs-nfs-client\n        #     storage_size_gb: 60\n        #     # Custom Scale options\n        #     scale_px_runtime:\n        #       replicas: 2\n        #       cpu_request: 500m\n        #       cpu_limit: 2\n        #       memory_request: 2Gi\n        #       memory_limit: 4Gi\n        #     scale_px_compute:\n        #       replicas: 2\n        #       cpu_request: 1\n        #       cpu_limit: 3\n        #       memory_request: 4Gi\n        #       memory_limit: 12Gi    \n    \n      - name: db2\n        description: Db2 OLTP\n        size: small\n        instances:\n        - name: ca-metastore\n          metadata_size_gb: 20\n          data_size_gb: 20\n          backup_size_gb: 20  \n          transactionlog_size_gb: 20\n        state: removed\n    \n      - name: db2wh\n        description: Db2 Warehouse\n        state: removed\n    \n      - name: dmc\n        description: Db2 Data Management Console\n        state: removed\n        instances:\n        - name: data-management-console\n          description: Data Management Console\n          size: medium\n          storage_size_gb: 50\n    \n      - name: dods\n        description: Decision Optimization\n        size: small\n        state: removed\n    \n      - name: dp\n        description: Data Privacy\n        size: small\n        state: removed\n    \n      - name: dpra\n        description: Data Privacy Risk Assessment\n        state: removed\n    \n      - name: dv\n        description: Data Virtualization\n        size: small \n        instances:\n        - name: data-virtualization\n        state: removed\n    \n      # Please note that for EDB Postgress, a secret edb-postgres-license-key must be created in the vault\n      # before deploying\n      - name: edb_cp4d\n        description: EDB Postgres\n        state: removed\n        instances:\n        - name: instance1\n          version: \"15.4\"\n          #type: Standard\n          #members: 1\n          #size_gb: 50\n          #resource_request_cpu: 1\n          #resource_request_memory: 4Gi\n          #resource_limit_cpu: 1\n          #resource_limit_memory: 4Gi\n    \n      - name: factsheet\n        description: AI Factsheets\n        size: small\n        state: removed\n    \n      - name: hee\n        description: Execution Engine for Apache Hadoop\n        size: small\n        state: removed\n    \n      - name: mantaflow\n        description: MANTA Automated Lineage\n        size: small\n        state: removed\n    \n      - name: match360\n        description: IBM Match 360\n        size: small\n        wkc_enabled: true\n        state: removed\n    \n      - name: mongodb\n        description: MongoDB for Cloud Pak for Data\n        state: removed\n        \n      - name: openpages\n        description: OpenPages\n        state: removed\n    \n      - name: planning-analytics\n        description: Planning Analytics\n        state: removed\n        instances:\n        - name: pa-instance\n          size: small\n          mysql_size_gb: 20\n          couchdb_size_gb: 20\n          mongo_size_gb: 20\n          redis_size_gb: 20\n    \n      - name: replication\n        description: Data Replication\n        size: small\n        state: removed\n        installation_options:\n          replication_license_type: IDRC    \n          \n      - name: rstudio\n        description: RStudio Server with R 3.6\n        size: small\n        state: removed\n    \n      - name: spss\n        description: SPSS Modeler\n        state: removed\n    \n      - name: streamsets\n        description: IBM StreamSets\n        state: removed\n    \n      - name: syntheticdata\n        description: Synthetic Data Generator\n        state: removed\n    \n      - name: voice-gateway\n        description: Voice Gateway\n        replicas: 1\n        state: removed\n    \n      # In case watsonx Orchestrate is installed, Watson Assistant should not be explicitly set to installed\n      - name: watson-assistant\n        description: Watson Assistant\n        size: small\n        # noobaa_account_secret: noobaa-admin\n        # noobaa_cert_secret: noobaa-s3-serving-cert\n        state: removed\n        # instances:\n        # - name: wa-instance\n        #   description: \"Watson Assistant instance\"\n        installation_options:\n          size: Production\n          bigpv: false\n          analytics: true\n          watsonxAiType: embedded\n          syomModels: []\n          ootbModels: []\n    \n      - name: watson-discovery\n        description: Watson Discovery\n        # noobaa_account_secret: noobaa-admin\n        # noobaa_cert_secret: noobaa-s3-serving-cert\n        state: removed\n        instances:\n        - name: wd-instance\n          description: \"Watson Discovery instance\"\n        installation_options:\n          discovery_deployment_type: Production\n    \n      - name: watson-openscale\n        description: Watson OpenScale\n        size: small\n        state: removed\n    \n      - name: watson-speech\n        description: Watson Speech (STT and TTS)\n        stt_size: xsmall\n        tts_size: xsmall\n        # noobaa_account_secret: noobaa-admin\n        # noobaa_cert_secret: noobaa-s3-serving-cert\n        state: removed\n        installation_options:\n          tags:\n            sttRuntime: true\n            sttAsync: false\n            sttCustomization: false\n            ttsRuntime: true\n            ttsCustomization: false\n          scaleConfig:\n            stt:\n              size: xsmall\n            tts:\n              size: xsmall\n          sttModels:\n          - enUsBroadbandModel\n          - enUsNarrowbandModel\n          - enUsShortFormNarrowbandModel\n          - enUsTelephony\n          - enUsMultimedia\n          ttsVoices:\n          - enUSAllisonV3Voice\n          - enUSLisaV3Voice\n          - enUSMichaelV3Voice\n    \n      - name: watsonx_ai\n        description: watsonx.ai\n        state: removed\n        installation_options:\n          tuning_disabled: true\n          liteInstall: false\n        models:\n        - model_id: allam-1-13b-instruct\n          state: removed\n          # model_install_parameters can be used for sharding and node pinning according to\n          # instructions at https://www.ibm.com/docs/en/software-hub/5.3.x?topic=setup-adding-foundation-models\n          #model_install_parameters:\n          #  shards: 2\n          #  nodeSelector:\n          #    kubernetes.io/hostname: hostname\n        - model_id: codellama-codellama-34b-instruct-hf\n          state: removed\n        - model_id: codestral-2501\n          state: removed\n        - model_id: codestral-2508\n          state: removed\n        - model_id: codestral-22b\n          state: removed\n        - model_id: devstral-medium-2507\n          state: removed\n        - model_id: google-flan-t5-xl\n          state: removed\n        - model_id: gpt-oss-20b\n          state: removed\n        - model_id: gpt-oss-120b\n          state: removed\n        - model_id: granite-4-h-micro\n          state: removed\n        - model_id: granite-4-h-tiny\n          state: removed\n        - model_id: granite-4-h-small\n          state: removed\n        - model_id: ibm-granite-13b-instruct-v2\n          state: removed\n        - model_id: granite-3-2-8b-instruct\n          state: removed\n        - model_id: granite-3-3-8b-instruct\n          state: removed\n        - model_id: granite-3-2b-instruct\n          state: removed\n        - model_id: granite-3-8b-instruct\n          state: removed\n        - model_id: granite-guardian-3-2b\n          state: removed\n        - model_id: granite-guardian-3-8b\n          state: removed\n        - model_id: granite-guardian-3-2-5b\n          state: removed\n        - model_id: granite-3b-code-instruct\n          state: removed\n        - model_id: granite-8b-code-instruct\n          state: removed\n        - model_id: granite-20b-code-instruct\n          state: removed\n        - model_id: granite-20b-code-base-schema-linking\n          state: removed\n        - model_id: granite-20b-code-base-sql-gen\n          state: removed\n        - model_id: granite-34b-code-instruct\n          state: removed\n        - model_id: granite-docling-258M\n          state: removed\n        - model_id: granite-vision-3-2-2b\n          state: removed\n        - model_id: granite-vision-3-3-2b\n          state: removed\n        - model_id: ibm-defense-3-3-8b-instruct\n          state: removed\n        - model_id: ibm-defense-4-0-micro\n          state: removed\n        - model_id: core42-jais-13b-chat\n          state: removed\n        - model_id: llama-3-2-1b-instruct\n          state: removed\n        - model_id: llama-4-maverick-17b-128e-instruct-fp8\n          state: removed\n        - model_id: llama-4-maverick-17b-128e-instruct-int4\n          state: removed\n        - model_id: llama-4-scout-17b-16e-instruct\n          state: removed\n        - model_id: llama-4-scout-17b-16e-instruct-int4\n          state: removed\n        - model_id: llama-3-3-70b-instruct\n          state: removed\n        - model_id: llama-3-2-1b-instruct\n          state: removed\n        - model_id: llama-3-2-3b-instruct\n          state: removed\n        - model_id: llama-3-2-11b-vision-instruct\n          state: removed\n        - model_id: llama-3-2-90b-vision-instruct\n          state: removed\n        - model_id: llama-guard-3-11b-vision\n          state: removed\n        - model_id: llama-3-1-8b-instruct\n          state: removed\n        - model_id: llama-3-1-70b-instruct\n          state: removed\n        - model_id: ministral-8b-instruct\n          state: removed\n        - model_id: mistral-small-3-2-24b-instruct-2506\n          state: removed\n        - model_id: mistral-small-3-1-24b-instruct-2503\n          state: removed\n        - model_id: mistral-medium-2505\n          state: removed\n        - model_id: mistral-medium-2508\n          state: removed\n        - model_id: mistral-large-instruct-2411\n          state: removed\n        - model_id: pixtral-large-instruct\n          state: removed\n        - model_id: voxtral-small-2507\n          state: removed\n        # Embedding models\n        - model_id: all-minilm-l6-v2\n          state: removed\n        - model_id: all-minilm-l12-v2\n          state: removed\n        - model_id: granite-embedding-107m-multilingual\n          state: removed\n        - model_id: granite-embedding-278m-multilingual\n          state: removed\n        - model_id: granite-embedding-english-reranker-r2\n          state: removed\n        - model_id: multilingual-e5-large\n          state: removed\n        - model_id: ibm-slate-30m-english-rtrvr\n          state: removed\n        - model_id: ibm-slate-125m-english-rtrvr\n          state: removed\n        - model_id: ms-marco-minilm-l-12-v2\n          state: removed\n    \n      - name: watsonx_data\n        description: watsonx.data\n        state: removed\n    \n      - name: watsonx_dataintegration\n        description: watsonx.data integration\n        state: removed\n        installation_options:\n          enableBatchBulkETL: true\n          enableRealtimeStreaming: true\n          enableDataObservability: true\n          enableUnstructuredDataIntegration: true\n          enableReplication: true\n    \n      - name: watsonx_dataintelligence\n        description: watsonx.data intelligence\n        state: removed\n        installation_options:\n          enableAISearch: false\n          enableContentLinkingForTextToSql: false\n          enableDataGovernanceCatalog: true\n          enableDataLineage: true\n          enableDataProduct: true\n          enableDataQuality: false\n          enableGenerativeAICapabilities: true\n          enableKnowledgeGraph: true\n          enableModelsOn: cpu\n          enableSemanticEmbedding: false\n          enableSemanticEnrichment: true\n          enableTextToSql: false\n    \n      - name: watsonx_data_premium\n        description: watsonx.data Premium\n        state: removed\n    \n      - name: watsonx_governance\n        description: watsonx.governance\n        state: removed\n        installation_options:\n          installType: all\n          enableFactsheet: true\n          enableOpenpages: true\n          enableOpenscale: true\n    \n      - name: watsonx_orchestrate\n        description: watsonx.orchestrate\n        state: removed\n        instances:\n        - name: wxo-instance\n          description: \"watsonx Orchestrate instance\"\n        installation_options:\n          installMode: \"agentic\"  \n          watsonxAI:\n            watsonxaiifm: true\n            syomModels: []\n            ootbModels: []\n    \n      - name: wca\n        description: watsonx Code Assistant\n        installation_options:\n          similarity_feature:\n            enabled: false\n          rag_enabled: true\n        state: removed\n    \n      - name: wca-ansible\n        description: watsonx Code Assistant for Red Hat Ansible Lightspeed\n        state: removed\n    \n      - name: wca-z\n        description: watsonx Code Assistant for Z\n        state: removed\n    \n      - name: wca-z-ce\n        description: watsonx Code Assistant for Z Code Explanation\n        state: removed\n    \n      # For the IBM Knowledge Catalog, you can specify 3 editions: wkx, ikc_premium, or ikc_standard\n      # Choose the correct IBM Knowledge Catalog edition below\n      - name: wkc\n        description: IBM Knowledge Catalog\n        size: small\n        state: removed\n        installation_options:\n          enableKnowledgeGraph: True\n          enableDataQuality: True\n    \n      - name: ikc_premium\n        description: IBM Knowledge Catalog - Premium edition\n        size: small\n        state: removed\n        installation_options:\n          enableDataQuality: False\n          enableKnowledgeGraph: False\n          useFDB: False\n          enableAISearch: False\n          enableSemanticAutomation: False\n          enableSemanticEnrichment: True\n          enableSemanticEmbedding: False\n          enableTextToSql: False\n          enableModelsOn: 'cpu'\n          customModelTextToSQL: granite-3-3-8b-instruct\n          \n      - name: ikc_standard\n        description: IBM Knowledge Catalog - Standard edition\n        size: small\n        state: removed\n        installation_options:\n          enableKnowledgeGraph: False\n          useFDB: False\n          enableAISearch: False\n          enableSemanticAutomation: False\n          enableSemanticEnrichment: True\n          enableSemanticEmbedding: False\n          enableTextToSql: False\n          enableModelsOn: 'cpu'\n          customModelTextToSQL: granite-3-3-8b-instruct\n    \n      - name: wml\n        description: Watson Machine Learning\n        size: small\n        state: removed\n    \n      - name: wml-accelerator\n        description: Watson Machine Learning Accelerator\n        replicas: 1\n        size: small\n        state: removed\n    \n      - name: ws\n        description: Watson Studio\n        state: removed\n    \n      - name: ws-pipelines\n        description: Watson Studio Pipelines\n        state: removed\n    \n      - name: ws-runtimes\n        description: Watson Studio Runtimes\n        state: removed \n        installation_options:\n          kinds:\n          - ibm-cpd-ws-runtime-241-pygpu\n          - ibm-cpd-ws-runtime-251-pygpu\n          - ibm-cpd-ws-runtime-241-r\n          - ibm-cpd-ws-runtime-251-r\n    \n    #\n    # Cartridges where extra work is needed (will not install automatically)\n    # \n      # Product Master requires set up of the Db2 instance secret before install\n      - name: productmaster\n        description: Product Master\n        size: small  \n    \n        state: removed\n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift-console/#start-the-deployer","title":"Start the deployer","text":"<ul> <li>Go to the OpenShift console</li> <li>Click the \"+\" sign at the top of the page</li> <li>Paste the following block into the window. You can update the image on line 11 and the same value will be used for image for the Deployer Job (From release v3.0.2 onwards).</li> </ul> <p>Info</p> <p>The below starts the deployer apply process using the above configuration. Additionally, you can:</p> <ul> <li>Start only the debug job and run the deployer in the debug pod. See Start deployer debug job</li> <li>Configure environment variables such as proxy server. See Configure environment variables for proxy and other settings</li> </ul> Start the deployer <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: cloud-pak-deployer-start\n  generateName: cloud-pak-deployer-start-\n  namespace: cloud-pak-deployer\nspec:\n  containers:\n  - name: cloud-pak-deployer\n    image: quay.io/cloud-pak-deployer/cloud-pak-deployer:latest\n    imagePullPolicy: Always\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    command: [\"/bin/sh\",\"-xc\"]\n    args: \n      - /cloud-pak-deployer/scripts/deployer/cpd-start-deployer.sh\n    envFrom:\n    - configMapRef:\n        name: cloud-pak-deployer-env\n        optional: true\n  restartPolicy: Never\n  securityContext:\n    runAsUser: 0\n  serviceAccountName: cloud-pak-deployer-sa\n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift-console/#follow-the-logs-of-the-deployment","title":"Follow the logs of the deployment","text":"<ul> <li>Open the OpenShift console</li> <li>Go to Workloads \u2192 Pods</li> <li>Select <code>cloud-pak-deployer</code> as the project at the top of the page</li> <li>Click the deployer Pod</li> <li>Click Logs tab</li> </ul> <p>Info</p> <p>When running the deployer installing Cloud Pak for Data, the first run will fail. This is because the deployer applies the node configuration to OpenShift, which will cause all nodes to restart one by one, including the node that runs the deployer. Because of the Job setting, a new deployer pod will automatically start and resume from where it was stopped.  </p>"},{"location":"10-use-deployer/3-run/existing-openshift-console/#finishing-up","title":"Finishing up","text":"<p>Once the process has finished, it will output the URLs by which you can access the deployed Cloud Pak.  <pre><code>--- Cloud Pak for Data project cpd ---\nCP4D URL: https://cpd-cpd.apps.6759f8089266ae8f450d554f.ocp.techzone.ibm.com\nCP4D User: cpadmin\nCP4D cpadmin password: &lt;your-cpadmin-password&gt;\n</code></pre></p> <p>You can also find this information under the <code>cloud-paks</code> directory in the status directory you specified. The following commands can be run from the debug pod terminal that is in the <code>cloud-pak-deployer</code> project.</p> <p>To retrieve the Cloud Pak URL(s):</p> <pre><code>cat $STATUS_DIR/cloud-paks/*\n</code></pre> <p>This will show the Cloud Pak URLs:</p> <pre><code>Cloud Pak for Data URL for cluster pluto-01 and project cpd (domain name specified was example.com):\nhttps://cpd-cpd.apps.pluto-01.example.com\n</code></pre> <p>The <code>admin</code> password can be retrieved from the vault as follows:</p> <p>List the secrets in the vault:</p> <pre><code>cp-deploy.sh vault list\n</code></pre> <p>This will show something similar to the following:</p> <pre><code>Secret list for group sample:\n- ibm_cp_entitlement_key\n- oc-login\n- cp4d_admin_cpd_demo\n</code></pre> <p>You can then retrieve the Cloud Pak for Data admin password like this:</p> <pre><code>cp-deploy.sh vault get --vault-secret cp4d_admin_cpd_sample\n</code></pre> <pre><code>PLAY [Secrets] *****************************************************************\nincluded: /cloud-pak-deployer/automation-roles/99-generic/vault/vault-get-secret/tasks/get-secret-file.yml for localhost\ncp4d_admin_zen_sample_sample: gelGKrcgaLatBsnAdMEbmLwGr\n</code></pre> <p>Once the process has finished, it will output the URLs by which you can access the deployed Cloud Pak.  <pre><code>--- Cloud Pak for Data project cpd ---\nCP4D URL: https://cpd-cpd.apps.6759f8089266ae8f450d554f.ocp.techzone.ibm.com\nCP4D User: cpadmin\nCP4D cpadmin password: &lt;your-cpadmin-password&gt;\n</code></pre></p>"},{"location":"10-use-deployer/3-run/existing-openshift-console/#re-run-deployer-when-failed-or-if-you-want-to-update-the-configuration","title":"Re-run deployer when failed or if you want to update the configuration","text":"<p>If the deployer has failed or if you want to make changes to the configuration after the successful run, you can do the following:</p> <ul> <li>Open the OpenShift console</li> <li>Go to Workloads \u2192 Jobs</li> <li>Check the logs of the <code>cloud-pak-deployer</code> job</li> <li>If needed, make changes to the <code>cloud-pak-deployer-config</code> Config Map by going to Workloads \u2192 ConfigMaps</li> <li>Re-run the deployer</li> </ul>"},{"location":"10-use-deployer/3-run/existing-openshift-cp4i/","title":"Existing openshift cp4i","text":"<pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cloud-pak-deployer-config\n  namespace: cloud-pak-deployer\ndata:\n  cpd-config.yaml: |\n    global_config:\n      environment_name: sample\n      cloud_platform: existing-ocp\n      env_id: pluto-01\n      confirm_destroy: False\n      optimize_deploy: True\n    \n    openshift:\n    - name: \"{{ env_id }}\"\n      ocp_version: detect\n      cluster_name: \"{{ env_id }}\"\n      domain_name: example.com\n      mcg:\n        install: True\n        storage_type: storage-class\n        storage_class: managed-nfs-storage\n      gpu:\n        install: auto\n      openshift_ai:\n        install: auto\n        channel: auto\n      openshift_storage:\n      - storage_name: auto-storage\n        storage_type: auto\n\n\n    cp4i:\n    - project: cp4i\n      openshift_cluster_name: \"{{ env_id }}\"\n      cp4i_version: 16.1.3.0\n      accept_licenses: true\n    \n      instances:\n    \n      - type: platform-navigator\n        state: installed # installed/removed\n    \n      - type: api-management\n        state: removed\n    \n      - type: automation-assets\n        state: removed\n    \n      - type: enterprise-gateway\n        state: removed\n    \n      - type: event-endpoint-management\n        state: removed\n    \n      - type: event-streams\n        state: removed\n      \n      - type: high-speed-transfer-server\n        state: removed\n    \n      - type: integration-dashboard\n        state: removed\n    \n      - type: integration-design\n        state: removed\n    \n      - type: messaging\n        state: removed\n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift-environment-variables-and-proxy/","title":"Setting environment variables for the deployer","text":"<p>Some environments may have a need for special settings when running the deployer. Many of these such as the use of a proxy server for external communications can be configured using environment variables. You can create configuration map (ConfigMap) <code>cloud-pak-deployer-env</code> to set environment variables for the deployer jobs.</p>"},{"location":"10-use-deployer/3-run/existing-openshift-environment-variables-and-proxy/#create-the-environment-configmap-for-external-connections-via-a-proxy-server","title":"Create the environment ConfigMap for external connections via a proxy server","text":"<ul> <li>Go to the OpenShift console</li> <li>Click the \"+\" sign at the top of the page</li> <li>Update the environment variables and/or add new ones</li> </ul> Set proxy variables <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cloud-pak-deployer-env\n  namespace: cloud-pak-deployer\ndata:\n  http_proxy: http://192.168.217.111:31288\n  https_proxy: http://192.168.217.111:3128\n  no_proxy: kubernetes.default,.default,.cluster.local,.example.com,.svc,10.0.0.0/16,10.0.0.0/8,10.128.0.0/16,127.0.0.1,172.16.0.0/12,172.18.10.0/27,172.30.0.0/16,192.168.0.0/16,api-int.cpd.example.com,localhost\n</code></pre> <p>Warning</p> <p>Please ensure that you specify the correct proxy server and also include the servers that must not be connected via the proxy server. The OpenShift-internal host names such as <code>kubernetes.default</code> and IP addresses such as <code>172.30.0.0/16</code> must always be configured in <code>no_proxy</code>, otherwise the installation of services will not work.</p>"},{"location":"10-use-deployer/3-run/existing-openshift-environment-variables-and-proxy/#create-environment-variable-to-run-deployer-with-dry-run","title":"Create environment variable to run deployer with dry-run","text":"<ul> <li>Go to the OpenShift console</li> <li>Click the \"+\" sign at the top of the page</li> <li>Update the environment variables and/or add new ones</li> </ul> Run deployer with dry-run <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cloud-pak-deployer-env\n  namespace: cloud-pak-deployer\ndata:\n  CPD_DRY_RUN: true\n</code></pre> <p>For a full list of environment variables, open the debug pod and run <code>/cloud-pak-deployer/cp-deploy.sh --help</code>.</p>"},{"location":"10-use-deployer/3-run/existing-openshift-environment-variables-and-proxy/#create-environment-variable-to-set-log-level-for-the-deployer-wizard","title":"Create environment variable to set log level for the deployer wizard","text":"<ul> <li>Go to the OpenShift console</li> <li>Click the \"+\" sign at the top of the page</li> <li>Update the environment variables and/or add new ones</li> </ul> Set log level for deployer wizard <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cloud-pak-deployer-env\n  namespace: cloud-pak-deployer\ndata:\n  CPD_WIZARD_LOG_LEVEL: DEBUG\n</code></pre> <p>For a full list of environment variables, open the debug pod and run <code>/cloud-pak-deployer/cp-deploy.sh --help</code>.</p>"},{"location":"10-use-deployer/3-run/existing-openshift-software-hub-keycloak/","title":"Existing openshift software hub keycloak","text":"<pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cloud-pak-deployer-config\n  namespace: cloud-pak-deployer\ndata:\n  cpd-config.yaml: |\n    global_config:\n      environment_name: sample\n      cloud_platform: existing-ocp\n      env_id: pluto-01\n      confirm_destroy: False\n      optimize_deploy: True\n    \n    openshift:\n    - name: \"{{ env_id }}\"\n      ocp_version: detect\n      cluster_name: \"{{ env_id }}\"\n      domain_name: example.com\n      mcg:\n        install: True\n        storage_type: storage-class\n        storage_class: managed-nfs-storage\n      gpu:\n        install: auto\n      openshift_ai:\n        install: auto\n        channel: auto\n      openshift_storage:\n      - storage_name: auto-storage\n        storage_type: auto\n\n\n    openshift_redhat_sso:\n    - openshift_cluster_name: \"{{ env_id }}\"\n      keycloak_name: ibm-keycloak\n      groups:\n      - name: kc-cp4d-admins\n        state: present\n      - name: kc-cp4d-data-engineers\n        state: present\n      - name: kc-cp4d-data-scientists\n        state: present\n      - name: kc-cp4d-monitors\n        state: present\n      users:\n      - username: ttoussaint\n        firstName: Tara\n        lastName: Toussaint\n        email: ttoussaint@cp.internal\n        state: present\n        groups:\n        - name: kc-cp4d-admins\n      - username: rramones\n        firstName: Rosa\n        lastName: Ramones\n        email: rramones@cp.internal\n        state: present\n        groups:\n        - name: kc-cp4d-data-engineers\n      - username: ssharpe\n        firstName: Shelly\n        lastName: Sharpe\n        email: ssharpe@cp.internal\n        state: present\n        groups:\n        - name: kc-cp4d-data-engineers\n        - name: kc-cp4d-data-scientists    \n      - username: pprimo\n        firstName: Paco\n        lastName: Primo\n        email: pprimo@cp.internal\n        state: present\n        groups:\n        - name: kc-cp4d-data-scientists\n        - name: kc-cp4d-monitors    \n      - username: rroller\n        firstName: Rico\n        lastName: Roller\n        email: rroller@cp.internal\n        # password: specific_password_for_the_user\n        state: present\n        groups:\n        - name: kc-cp4d-data-scientists    \n    \n    zen_role:\n    - name: cp4d-monitoring-role\n      description: Cloud Pak for Data monitoring role\n      state: installed\n      permissions:\n      - monitor_platform\n    \n    zen_access_control:\n    - project: cpd\n      openshift_cluster_name: \"{{ env_id }}\"\n      keycloak_name: ibm-keycloak\n      user_groups:\n      - name: cp4d-admins\n        description: Cloud Pak for Data Administrators\n        roles:\n        - Administrator\n        keycloak_groups:\n        - kc-cp4d-admins\n      - name: cp4d-data-engineers\n        description: Cloud Pak for Data Data Engineers\n        roles:\n        - User\n        keycloak_groups:\n        - kc-cp4d-data-engineers\n      - name: cp4d-data-scientists\n        description: Cloud Pak for Data Data Scientists\n        roles:\n        - User\n        keycloak_groups:\n        - kc-cp4d-data-scientists\n      - name: cp4d-monitors\n        description: Cloud Pak for Data Monitoring\n        roles:\n        - cp4d-monitoring-role\n        keycloak_groups:\n        - kc-cp4d-monitors\n\n    cp4d:\n    - project: cpd\n      openshift_cluster_name: \"{{ env_id }}\"\n      cp4d_version: latest\n      cp4d_entitlement: \n      - cpd-enterprise\n      # - cpd-standard\n      # - cognos-analytics\n      # - data-product-hub\n      # - datastage\n      # - data-integration-unstructured-data\n      # - data-lineage\n      # - ikc-premium\n      # - ikc-standard\n      # - openpages\n      # - planning-analytics\n      # - product-master\n      # - speech-to-text\n      # - text-to-speech\n      # - watson-assistant\n      # - watson-discovery\n      # - watsonx-ai\n      # - watsonx-code-assistant-ansible\n      # - watsonx-code-assistant-z\n      # - watsonx-data\n      # - watsonx-gov-mm\n      # - watsonx-gov-rc\n      # - watsonx-orchestrate\n      cp4d_production_license: True\n      accept_licenses: True\n      db2u_limited_privileges: False\n      use_fs_iam: True\n      operators_project: cpd-operators\n      ibm_cert_manager: False\n      state: installed\n      cartridges:\n      - name: cp-foundation\n        scale: level_1\n        license_service:\n          threads_per_core: 2\n      \n      - name: lite\n    \n      - name: scheduler \n        state: removed\n        \n    #\n    # All tested cartridges. To install, change the \"state\" property to \"installed\". To uninstall, change the state\n    # to \"removed\" or comment out the entire cartridge. Make sure that the \"-\" and properties are aligned with the lite\n    # cartridge; the \"-\" is at position 3 and the property starts at position 5.\n    #\n    # If a cartridge has dependencies and you want to install it, you must ensure that the dependent cartridge is also\n    # installed.\n    #\n    \n      - name: analyticsengine \n        description: Analytics Engine Powered by Apache Spark \n        size: small \n        state: removed\n        installation_options:\n          sparkAdvEnabled: true\n          jobAutoDeleteEnabled: true\n          kernelCullTime: 30\n          imagePullParallelism: \"40\"\n          imagePullCompletions: \"20\"\n          kernelCleanupSchedule: \"*/30 * * * *\"\n          jobCleanupSchedule: \"*/30 * * * *\"\n          skipSelinuxRelabeling: false\n          mountCustomizationsFromCchome: false\n          maxDriverCpuCores: 5\n          maxExecutorCpuCores: 5\n          maxDriveMemory: \"50g\"\n          maxExecutorMemory: \"50g\"\n          maxNumWorkers: 50\n          localDirScaleFactor: 10\n    \n      - name: bigsql\n        description: Db2 Big SQL\n        state: removed\n    \n      - name: ca\n        description: Cognos Analytics\n        size: small\n        instances:\n        - name: ca-instance\n          metastore_ref: ca-metastore\n        state: removed\n    \n      - name: dashboard\n        description: Cognos Dashboards\n        state: removed\n    \n      - name: datagate\n        description: Db2 Data Gate\n        state: removed\n    \n      - name: datalineage\n        description: IBM MANTA Data Lineage\n        size: small\n        state: removed\n    \n      - name: dataproduct\n        description: Data Product Hub\n        state: removed\n        \n      - name: datastage-ent\n        description: DataStage Enterprise\n        state: removed\n    \n      - name: datastage-ent-plus\n        description: DataStage Enterprise Plus\n        state: removed\n        # instances:\n        #   - name: ds-instance\n        #     # Optional settings\n        #     description: \"datastage ds-instance\"\n        #     size: medium\n        #     storage_class: efs-nfs-client\n        #     storage_size_gb: 60\n        #     # Custom Scale options\n        #     scale_px_runtime:\n        #       replicas: 2\n        #       cpu_request: 500m\n        #       cpu_limit: 2\n        #       memory_request: 2Gi\n        #       memory_limit: 4Gi\n        #     scale_px_compute:\n        #       replicas: 2\n        #       cpu_request: 1\n        #       cpu_limit: 3\n        #       memory_request: 4Gi\n        #       memory_limit: 12Gi    \n    \n      - name: db2\n        description: Db2 OLTP\n        size: small\n        instances:\n        - name: ca-metastore\n          metadata_size_gb: 20\n          data_size_gb: 20\n          backup_size_gb: 20  \n          transactionlog_size_gb: 20\n        state: removed\n    \n      - name: db2wh\n        description: Db2 Warehouse\n        state: removed\n    \n      - name: dmc\n        description: Db2 Data Management Console\n        state: removed\n        instances:\n        - name: data-management-console\n          description: Data Management Console\n          size: medium\n          storage_size_gb: 50\n    \n      - name: dods\n        description: Decision Optimization\n        size: small\n        state: removed\n    \n      - name: dp\n        description: Data Privacy\n        size: small\n        state: removed\n    \n      - name: dpra\n        description: Data Privacy Risk Assessment\n        state: removed\n    \n      - name: dv\n        description: Data Virtualization\n        size: small \n        instances:\n        - name: data-virtualization\n        state: removed\n    \n      # Please note that for EDB Postgress, a secret edb-postgres-license-key must be created in the vault\n      # before deploying\n      - name: edb_cp4d\n        description: EDB Postgres\n        state: removed\n        instances:\n        - name: instance1\n          version: \"15.4\"\n          #type: Standard\n          #members: 1\n          #size_gb: 50\n          #resource_request_cpu: 1\n          #resource_request_memory: 4Gi\n          #resource_limit_cpu: 1\n          #resource_limit_memory: 4Gi\n    \n      - name: factsheet\n        description: AI Factsheets\n        size: small\n        state: removed\n    \n      - name: hee\n        description: Execution Engine for Apache Hadoop\n        size: small\n        state: removed\n    \n      - name: mantaflow\n        description: MANTA Automated Lineage\n        size: small\n        state: removed\n    \n      - name: match360\n        description: IBM Match 360\n        size: small\n        wkc_enabled: true\n        state: removed\n    \n      - name: mongodb\n        description: MongoDB for Cloud Pak for Data\n        state: removed\n        \n      - name: openpages\n        description: OpenPages\n        state: removed\n    \n      - name: planning-analytics\n        description: Planning Analytics\n        state: removed\n        instances:\n        - name: pa-instance\n          size: small\n          mysql_size_gb: 20\n          couchdb_size_gb: 20\n          mongo_size_gb: 20\n          redis_size_gb: 20\n    \n      - name: replication\n        description: Data Replication\n        size: small\n        state: removed\n        installation_options:\n          replication_license_type: IDRC    \n          \n      - name: rstudio\n        description: RStudio Server with R 3.6\n        size: small\n        state: removed\n    \n      - name: spss\n        description: SPSS Modeler\n        state: removed\n    \n      - name: streamsets\n        description: IBM StreamSets\n        state: removed\n    \n      - name: syntheticdata\n        description: Synthetic Data Generator\n        state: removed\n    \n      - name: voice-gateway\n        description: Voice Gateway\n        replicas: 1\n        state: removed\n    \n      # In case watsonx Orchestrate is installed, Watson Assistant should not be explicitly set to installed\n      - name: watson-assistant\n        description: Watson Assistant\n        size: small\n        # noobaa_account_secret: noobaa-admin\n        # noobaa_cert_secret: noobaa-s3-serving-cert\n        state: removed\n        # instances:\n        # - name: wa-instance\n        #   description: \"Watson Assistant instance\"\n        installation_options:\n          size: Production\n          bigpv: false\n          analytics: true\n          watsonxAiType: embedded\n          syomModels: []\n          ootbModels: []\n    \n      - name: watson-discovery\n        description: Watson Discovery\n        # noobaa_account_secret: noobaa-admin\n        # noobaa_cert_secret: noobaa-s3-serving-cert\n        state: removed\n        instances:\n        - name: wd-instance\n          description: \"Watson Discovery instance\"\n        installation_options:\n          discovery_deployment_type: Production\n    \n      - name: watson-openscale\n        description: Watson OpenScale\n        size: small\n        state: removed\n    \n      - name: watson-speech\n        description: Watson Speech (STT and TTS)\n        stt_size: xsmall\n        tts_size: xsmall\n        # noobaa_account_secret: noobaa-admin\n        # noobaa_cert_secret: noobaa-s3-serving-cert\n        state: removed\n        installation_options:\n          tags:\n            sttRuntime: true\n            sttAsync: false\n            sttCustomization: false\n            ttsRuntime: true\n            ttsCustomization: false\n          scaleConfig:\n            stt:\n              size: xsmall\n            tts:\n              size: xsmall\n          sttModels:\n          - enUsBroadbandModel\n          - enUsNarrowbandModel\n          - enUsShortFormNarrowbandModel\n          - enUsTelephony\n          - enUsMultimedia\n          ttsVoices:\n          - enUSAllisonV3Voice\n          - enUSLisaV3Voice\n          - enUSMichaelV3Voice\n    \n      - name: watsonx_ai\n        description: watsonx.ai\n        state: removed\n        installation_options:\n          tuning_disabled: true\n          liteInstall: false\n        models:\n        - model_id: allam-1-13b-instruct\n          state: removed\n          # model_install_parameters can be used for sharding and node pinning according to\n          # instructions at https://www.ibm.com/docs/en/software-hub/5.3.x?topic=setup-adding-foundation-models\n          #model_install_parameters:\n          #  shards: 2\n          #  nodeSelector:\n          #    kubernetes.io/hostname: hostname\n        - model_id: codellama-codellama-34b-instruct-hf\n          state: removed\n        - model_id: codestral-2501\n          state: removed\n        - model_id: codestral-2508\n          state: removed\n        - model_id: codestral-22b\n          state: removed\n        - model_id: devstral-medium-2507\n          state: removed\n        - model_id: google-flan-t5-xl\n          state: removed\n        - model_id: gpt-oss-20b\n          state: removed\n        - model_id: gpt-oss-120b\n          state: removed\n        - model_id: granite-4-h-micro\n          state: removed\n        - model_id: granite-4-h-tiny\n          state: removed\n        - model_id: granite-4-h-small\n          state: removed\n        - model_id: ibm-granite-13b-instruct-v2\n          state: removed\n        - model_id: granite-3-2-8b-instruct\n          state: removed\n        - model_id: granite-3-3-8b-instruct\n          state: removed\n        - model_id: granite-3-2b-instruct\n          state: removed\n        - model_id: granite-3-8b-instruct\n          state: removed\n        - model_id: granite-guardian-3-2b\n          state: removed\n        - model_id: granite-guardian-3-8b\n          state: removed\n        - model_id: granite-guardian-3-2-5b\n          state: removed\n        - model_id: granite-3b-code-instruct\n          state: removed\n        - model_id: granite-8b-code-instruct\n          state: removed\n        - model_id: granite-20b-code-instruct\n          state: removed\n        - model_id: granite-20b-code-base-schema-linking\n          state: removed\n        - model_id: granite-20b-code-base-sql-gen\n          state: removed\n        - model_id: granite-34b-code-instruct\n          state: removed\n        - model_id: granite-docling-258M\n          state: removed\n        - model_id: granite-vision-3-2-2b\n          state: removed\n        - model_id: granite-vision-3-3-2b\n          state: removed\n        - model_id: ibm-defense-3-3-8b-instruct\n          state: removed\n        - model_id: ibm-defense-4-0-micro\n          state: removed\n        - model_id: core42-jais-13b-chat\n          state: removed\n        - model_id: llama-3-2-1b-instruct\n          state: removed\n        - model_id: llama-4-maverick-17b-128e-instruct-fp8\n          state: removed\n        - model_id: llama-4-maverick-17b-128e-instruct-int4\n          state: removed\n        - model_id: llama-4-scout-17b-16e-instruct\n          state: removed\n        - model_id: llama-4-scout-17b-16e-instruct-int4\n          state: removed\n        - model_id: llama-3-3-70b-instruct\n          state: removed\n        - model_id: llama-3-2-1b-instruct\n          state: removed\n        - model_id: llama-3-2-3b-instruct\n          state: removed\n        - model_id: llama-3-2-11b-vision-instruct\n          state: removed\n        - model_id: llama-3-2-90b-vision-instruct\n          state: removed\n        - model_id: llama-guard-3-11b-vision\n          state: removed\n        - model_id: llama-3-1-8b-instruct\n          state: removed\n        - model_id: llama-3-1-70b-instruct\n          state: removed\n        - model_id: ministral-8b-instruct\n          state: removed\n        - model_id: mistral-small-3-2-24b-instruct-2506\n          state: removed\n        - model_id: mistral-small-3-1-24b-instruct-2503\n          state: removed\n        - model_id: mistral-medium-2505\n          state: removed\n        - model_id: mistral-medium-2508\n          state: removed\n        - model_id: mistral-large-instruct-2411\n          state: removed\n        - model_id: pixtral-large-instruct\n          state: removed\n        - model_id: voxtral-small-2507\n          state: removed\n        # Embedding models\n        - model_id: all-minilm-l6-v2\n          state: removed\n        - model_id: all-minilm-l12-v2\n          state: removed\n        - model_id: granite-embedding-107m-multilingual\n          state: removed\n        - model_id: granite-embedding-278m-multilingual\n          state: removed\n        - model_id: granite-embedding-english-reranker-r2\n          state: removed\n        - model_id: multilingual-e5-large\n          state: removed\n        - model_id: ibm-slate-30m-english-rtrvr\n          state: removed\n        - model_id: ibm-slate-125m-english-rtrvr\n          state: removed\n        - model_id: ms-marco-minilm-l-12-v2\n          state: removed\n    \n      - name: watsonx_data\n        description: watsonx.data\n        state: removed\n    \n      - name: watsonx_dataintegration\n        description: watsonx.data integration\n        state: removed\n        installation_options:\n          enableBatchBulkETL: true\n          enableRealtimeStreaming: true\n          enableDataObservability: true\n          enableUnstructuredDataIntegration: true\n          enableReplication: true\n    \n      - name: watsonx_dataintelligence\n        description: watsonx.data intelligence\n        state: removed\n        installation_options:\n          enableAISearch: false\n          enableContentLinkingForTextToSql: false\n          enableDataGovernanceCatalog: true\n          enableDataLineage: true\n          enableDataProduct: true\n          enableDataQuality: false\n          enableGenerativeAICapabilities: true\n          enableKnowledgeGraph: true\n          enableModelsOn: cpu\n          enableSemanticEmbedding: false\n          enableSemanticEnrichment: true\n          enableTextToSql: false\n    \n      - name: watsonx_data_premium\n        description: watsonx.data Premium\n        state: removed\n    \n      - name: watsonx_governance\n        description: watsonx.governance\n        state: removed\n        installation_options:\n          installType: all\n          enableFactsheet: true\n          enableOpenpages: true\n          enableOpenscale: true\n    \n      - name: watsonx_orchestrate\n        description: watsonx.orchestrate\n        state: removed\n        instances:\n        - name: wxo-instance\n          description: \"watsonx Orchestrate instance\"\n        installation_options:\n          installMode: \"agentic\"  \n          watsonxAI:\n            watsonxaiifm: true\n            syomModels: []\n            ootbModels: []\n    \n      - name: wca\n        description: watsonx Code Assistant\n        installation_options:\n          similarity_feature:\n            enabled: false\n          rag_enabled: true\n        state: removed\n    \n      - name: wca-ansible\n        description: watsonx Code Assistant for Red Hat Ansible Lightspeed\n        state: removed\n    \n      - name: wca-z\n        description: watsonx Code Assistant for Z\n        state: removed\n    \n      - name: wca-z-ce\n        description: watsonx Code Assistant for Z Code Explanation\n        state: removed\n    \n      # For the IBM Knowledge Catalog, you can specify 3 editions: wkx, ikc_premium, or ikc_standard\n      # Choose the correct IBM Knowledge Catalog edition below\n      - name: wkc\n        description: IBM Knowledge Catalog\n        size: small\n        state: removed\n        installation_options:\n          enableKnowledgeGraph: True\n          enableDataQuality: True\n    \n      - name: ikc_premium\n        description: IBM Knowledge Catalog - Premium edition\n        size: small\n        state: removed\n        installation_options:\n          enableDataQuality: False\n          enableKnowledgeGraph: False\n          useFDB: False\n          enableAISearch: False\n          enableSemanticAutomation: False\n          enableSemanticEnrichment: True\n          enableSemanticEmbedding: False\n          enableTextToSql: False\n          enableModelsOn: 'cpu'\n          customModelTextToSQL: granite-3-3-8b-instruct\n          \n      - name: ikc_standard\n        description: IBM Knowledge Catalog - Standard edition\n        size: small\n        state: removed\n        installation_options:\n          enableKnowledgeGraph: False\n          useFDB: False\n          enableAISearch: False\n          enableSemanticAutomation: False\n          enableSemanticEnrichment: True\n          enableSemanticEmbedding: False\n          enableTextToSql: False\n          enableModelsOn: 'cpu'\n          customModelTextToSQL: granite-3-3-8b-instruct\n    \n      - name: wml\n        description: Watson Machine Learning\n        size: small\n        state: removed\n    \n      - name: wml-accelerator\n        description: Watson Machine Learning Accelerator\n        replicas: 1\n        size: small\n        state: removed\n    \n      - name: ws\n        description: Watson Studio\n        state: removed\n    \n      - name: ws-pipelines\n        description: Watson Studio Pipelines\n        state: removed\n    \n      - name: ws-runtimes\n        description: Watson Studio Runtimes\n        state: removed \n        installation_options:\n          kinds:\n          - ibm-cpd-ws-runtime-241-pygpu\n          - ibm-cpd-ws-runtime-251-pygpu\n          - ibm-cpd-ws-runtime-241-r\n          - ibm-cpd-ws-runtime-251-r\n    \n    #\n    # Cartridges where extra work is needed (will not install automatically)\n    # \n      # Product Master requires set up of the Db2 instance secret before install\n      - name: productmaster\n        description: Product Master\n        size: small  \n    \n        state: removed\n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift-software-hub-watsonx-ai/","title":"Existing openshift software hub watsonx ai","text":"<pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: cloud-pak-deployer-config\n  namespace: cloud-pak-deployer\ndata:\n  cpd-config.yaml: |\n    global_config:\n      environment_name: demo\n      cloud_platform: existing-ocp\n      confirm_destroy: False\n      optimize_deploy: True\n      env_id: cpd-demo\n\n    openshift:\n    - name: \"{{ env_id }}\"\n      ocp_version: detect\n      cluster_name: \"{{ env_id }}\"\n      domain_name: example.com\n      mcg:\n        install: False\n        storage_type: storage-class\n        storage_class: managed-nfs-storage\n      gpu:\n        install: auto\n      openshift_ai:\n        install: auto\n        channel: auto\n      openshift_storage:\n      - storage_name: auto-storage\n        storage_type: auto\n\n    cp4d:\n    - project: cpd\n      openshift_cluster_name: \"{{ env_id }}\"\n      cp4d_version: latest\n      db2u_limited_privileges: False\n      use_fs_iam: True\n      accept_licenses: True\n      cartridges:\n      - name: cp-foundation\n        scale: level_1\n        license_service:\n          threads_per_core: 2\n      \n      - name: lite\n\n      - name: scheduler \n        state: removed\n        \n      - name: analyticsengine \n        description: Analytics Engine Powered by Apache Spark \n        size: small \n        state: removed\n\n      - name: bigsql\n        description: Db2 Big SQL\n        state: removed\n\n      - name: ca\n        description: Cognos Analytics\n        size: small\n        instances:\n        - name: ca-instance\n          metastore_ref: ca-metastore\n        state: removed\n\n      - name: dashboard\n        description: Cognos Dashboards\n        state: removed\n\n      - name: datagate\n        description: Db2 Data Gate\n        state: removed\n\n      - name: datalineage\n        description: IBM MANTA Data Lineage\n        size: small\n        state: removed\n\n      - name: dataproduct\n        description: Data Product Hub\n        state: removed\n        \n      - name: datastage-ent\n        description: DataStage Enterprise\n        state: removed\n\n      - name: datastage-ent-plus\n        description: DataStage Enterprise Plus\n        state: removed\n\n        # The default instance is created automatically with the DataStage installation. If you want to create additional instances\n        # uncomment the section below and specify the various scaling options.\n\n        # instances:\n        #   - name: ds-instance\n        #     # Optional settings\n        #     description: \"datastage ds-instance\"\n        #     size: medium\n        #     storage_class: efs-nfs-client\n        #     storage_size_gb: 60\n        #     # Custom Scale options\n        #     scale_px_runtime:\n        #       replicas: 2\n        #       cpu_request: 500m\n        #       cpu_limit: 2\n        #       memory_request: 2Gi\n        #       memory_limit: 4Gi\n        #     scale_px_compute:\n        #       replicas: 2\n        #       cpu_request: 1\n        #       cpu_limit: 3\n        #       memory_request: 4Gi\n        #       memory_limit: 12Gi    \n\n      - name: db2\n        description: Db2 OLTP\n        size: small\n        instances:\n        - name: ca-metastore\n          metadata_size_gb: 20\n          data_size_gb: 20\n          backup_size_gb: 20  \n          transactionlog_size_gb: 20\n        state: removed\n\n      - name: db2wh\n        description: Db2 Warehouse\n        state: removed\n\n      - name: dmc\n        description: Db2 Data Management Console\n        state: removed\n        instances:\n        - name: data-management-console\n          description: Data Management Console\n          size: medium\n          storage_size_gb: 50\n\n      - name: dods\n        description: Decision Optimization\n        size: small\n        state: removed\n\n      - name: dp\n        description: Data Privacy\n        size: small\n        state: removed\n\n      - name: dpra\n        description: Data Privacy Risk Assessment\n        state: removed\n\n      - name: dv\n        description: Data Virtualization\n        size: small \n        instances:\n        - name: data-virtualization\n        state: removed\n\n      # Please note that for EDB Postgress, a secret edb-postgres-license-key must be created in the vault\n      # before deploying\n      - name: edb_cp4d\n        description: EDB Postgres\n        state: removed\n        instances:\n        - name: instance1\n          version: \"15.4\"\n          #type: Standard\n          #members: 1\n          #size_gb: 50\n          #resource_request_cpu: 1\n          #resource_request_memory: 4Gi\n          #resource_limit_cpu: 1\n          #resource_limit_memory: 4Gi\n\n      - name: factsheet\n        description: AI Factsheets\n        size: small\n        state: removed\n\n      - name: hee\n        description: Execution Engine for Apache Hadoop\n        size: small\n        state: removed\n\n      - name: mantaflow\n        description: MANTA Automated Lineage\n        size: small\n        state: removed\n\n      - name: match360\n        description: IBM Match 360\n        size: small\n        wkc_enabled: true\n        state: removed\n\n      - name: openpages\n        description: OpenPages\n        state: removed\n\n      # For Planning Analytics, the case version is needed due to defect in olm utils\n      - name: planning-analytics\n        description: Planning Analytics\n        state: removed\n\n      - name: replication\n        description: Data Replication\n        license: IDRC\n        size: small\n        state: removed\n\n      - name: rstudio\n        description: RStudio Server with R 3.6\n        size: small\n        state: removed\n\n      - name: spss\n        description: SPSS Modeler\n        state: removed\n\n      - name: streamsets\n        description: IBM StreamSets\n        state: removed\n\n      - name: syntheticdata\n        description: Synthetic Data Generator\n        state: removed\n\n      - name: voice-gateway\n        description: Voice Gateway\n        replicas: 1\n        state: removed\n\n      # In case watsonx Orchestrate is installed, no instances must be created for Watson Assistant\n      - name: watson-assistant\n        description: Watson Assistant\n        size: small\n        # noobaa_account_secret: noobaa-admin\n        # noobaa_cert_secret: noobaa-s3-serving-cert\n        state: removed\n        # instances:\n        # - name: wa-instance\n        #   description: \"Watson Assistant instance\"\n\n      - name: watson-discovery\n        description: Watson Discovery\n        # noobaa_account_secret: noobaa-admin\n        # noobaa_cert_secret: noobaa-s3-serving-cert\n        state: removed\n        instances:\n        - name: wd-instance\n          description: \"Watson Discovery instance\"\n\n      - name: watson-openscale\n        description: Watson OpenScale\n        size: small\n        state: removed\n\n      - name: watson-speech\n        description: Watson Speech (STT and TTS)\n        stt_size: xsmall\n        tts_size: xsmall\n        # noobaa_account_secret: noobaa-admin\n        # noobaa_cert_secret: noobaa-s3-serving-cert\n        state: removed\n\n      - name: watsonx_ai\n        description: watsonx.ai\n        state: installed\n        installation_options:\n          tuning_disabled: true\n          lite_install: false\n        models:\n        - model_id: allam-1-13b-instruct\n          state: removed\n        - model_id: codellama-codellama-34b-instruct-hf\n          state: removed\n        - model_id: codestral-22b\n          state: removed\n        - model_id: elyza-japanese-llama-2-7b-instruct\n          state: removed\n        - model_id: google-flan-ul2\n          state: removed\n        - model_id: google-flan-t5-xl\n          state: removed\n        - model_id: google-flan-t5-xxl\n          state: removed\n        - model_id: ibm-granite-7b-lab\n          state: removed\n        - model_id: ibm-granite-8b-japanese\n          state: removed\n        - model_id: ibm-granite-13b-chat-v2\n          state: removed\n        - model_id: ibm-granite-13b-instruct-v2\n          state: removed\n        - model_id: ibm-granite-20b-multilingual\n          state: removed\n        - model_id: granite-3-2b-instruct\n          state: removed\n        - model_id: granite-3-8b-instruct\n          state: removed\n        - model_id: granite-guardian-3-2b-instruct\n          state: removed\n        - model_id: granite-guardian-3-8b-instruct\n          state: removed\n        - model_id: granite-3b-code-instruct\n          state: removed\n        - model_id: granite-8b-code-instruct\n          state: removed\n        - model_id: granite-20b-code-instruct\n          state: removed\n        - model_id: granite-20b-code-base-schema-linking\n          state: removed\n        - model_id: granite-20b-code-base-sql-gen\n          state: removed\n        - model_id: granite-34b-code-instruct\n          state: removed\n        - model_id: core42-jais-13b-chat\n          state: removed\n        - model_id: llama-3-2-1b-instruct\n          state: removed\n        - model_id: llama-3-2-3b-instruct\n          state: removed\n        - model_id: llama-3-2-11b-vision-instruct\n          state: removed\n        - model_id: llama-3-2-90b-vision-instruct\n          state: removed\n        - model_id: llama-guard-3-11b-vision\n          state: removed\n        - model_id: llama-3-1-8b-instruct\n          state: removed\n        - model_id: llama-3-1-70b-instruct\n          state: removed\n        - model_id: llama-3-405b-instruct\n          state: removed\n        - model_id: meta-llama-llama-3-8b-instruct\n          state: removed\n        - model_id: meta-llama-llama-3-70b-instruct\n          state: removed\n        - model_id: meta-llama-llama-2-13b-chat\n          state: removed\n        - model_id: mncai-llama-2-13b-dpo-v7\n          state: removed\n        - model_id: ministral-8b-instruct\n          state: removed\n        - model_id: mistral-small-instruct\n          state: removed\n        - model_id: mistral-large\n          state: removed\n        - model_id: mistralai-mixtral-8x7b-instruct-v01\n          state: removed\n        - model_id: bigscience-mt0-xxl\n          state: removed\n        - model_id: pixtral-12b\n          state: removed\n        # Embedding models\n        - model_id: all-minilm-l6-v2\n          state: removed\n        - model_id: all-minilm-l12-v2\n          state: removed\n        - model_id: ms-marco-minilm-l-12-v2\n          state: removed\n        - model_id: multilingual-e5-large\n          state: removed\n        - model_id: ibm-slate-30m-english-rtrvr\n          state: removed\n        - model_id: ibm-slate-125m-english-rtrvr\n          state: removed\n\n      - name: watsonx_data\n        description: watsonx.data\n        state: removed\n\n      - name: watsonx_governance\n        description: watsonx.governance\n        state: removed\n        installation_options:\n          installType: all\n          enableFactsheet: true\n          enableOpenpages: true\n          enableOpenscale: true\n\n      - name: watsonx_orchestrate\n        description: watsonx.orchestrate\n        app_connect:\n          app_connect_project: ibm-app-connect\n          app_connect_case_version: 12.5.0\n          app_connect_channel_version: v12.5\n        installation_options:\n          watsonx_orchestrate_watsonx_ai_type: false\n        instances:\n        - name: wxo-instance\n          description: \"watsonx Orchestrate instance\"\n        state: removed\n\n      - name: wca\n        description: watsonx Code Assistant\n        installation_options:\n          similarity_feature:\n            enabled: false\n          rag_enabled:\n            enabled: true\n        state: removed\n\n      - name: wca-ansible\n        description: watsxonx Code Assistant for Red Hat Ansible Lightspeed\n        state: removed\n\n      - name: wca-z\n        description: watsxonx Code Assistant for Z\n        state: removed\n\n      # For the IBM Knowledge Catalog, you can specify 3 editions: wkx, ikc_premium, or ikc_standard\n      # Choose the correct IBM Knowledge Catalog edition below\n      - name: wkc\n        description: IBM Knowledge Catalog\n        size: small\n        state: removed\n        installation_options:\n          enableKnowledgeGraph: False\n          enableDataQuality: False\n\n      - name: ikc_premium\n        description: IBM Knowledge Catalog - Premium edition\n        size: small\n        state: removed\n        installation_options:\n          enableKnowledgeGraph: False\n          enableDataQuality: False\n\n      - name: ikc_standard\n        description: IBM Knowledge Catalog - Standard edition\n        size: small\n        state: removed\n        installation_options:\n          enableKnowledgeGraph: False\n          enableDataQuality: False\n\n      - name: wml\n        description: Watson Machine Learning\n        size: small\n        state: removed\n\n      - name: wml-accelerator\n        description: Watson Machine Learning Accelerator\n        replicas: 1\n        size: small\n        state: removed\n\n      - name: ws\n        description: Watson Studio\n        state: removed\n\n      - name: ws-pipelines\n        description: Watson Studio Pipelines\n        state: removed\n\n      - name: ws-runtimes\n        description: Watson Studio Runtimes\n        runtimes:\n        - ibm-cpd-ws-runtime-241-py\n        - ibm-cpd-ws-runtime-241-pygpu\n        - ibm-cpd-ws-runtime-241-r\n        state: removed \n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift-wizard/","title":"Run deployer wizard on OpenShift","text":""},{"location":"10-use-deployer/3-run/existing-openshift-wizard/#log-in-to-the-openshift-cluster","title":"Log in to the OpenShift cluster","text":"<p>Log is as a cluster administrator to be able to run the deployer with the correct permissions.</p>"},{"location":"10-use-deployer/3-run/existing-openshift-wizard/#prepare-the-deployer-project","title":"Prepare the deployer project","text":"<ul> <li>Go to the OpenShift console</li> <li>Click the \"+\" sign at the top of the page</li> <li>Paste the following block (exactly) into the window</li> </ul> Prepare the deployer project <pre><code>---\napiVersion: v1\nkind: Namespace\nmetadata:\n  creationTimestamp: null\n  name: cloud-pak-deployer\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: cloud-pak-deployer-sa\n  namespace: cloud-pak-deployer\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: system:openshift:scc:privileged\n  namespace: cloud-pak-deployer\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: system:openshift:scc:privileged\nsubjects:\n- kind: ServiceAccount\n  name: cloud-pak-deployer-sa\n  namespace: cloud-pak-deployer\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: cloud-pak-deployer-cluster-admin\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: cloud-pak-deployer-sa\n  namespace: cloud-pak-deployer\n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift-wizard/#start-the-deployer","title":"Start the deployer","text":"<ul> <li>Go to the OpenShift console</li> <li>Click the \"+\" sign at the top of the page</li> <li>Paste the following block into the window. You can update the image on line 11 and the same value will be used for image for the Deployer Job</li> </ul> <p>Info</p> <p>The below starts the deployer wizard using the standard configuration. Additionally, you can:</p> <ul> <li>Configure environment variables such as proxy server or the wizard logging level. See Configure environment variables for proxy and other settings</li> </ul> Start the deployer wizard <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app: cloud-pak-deployer-start\n  generateName: cloud-pak-deployer-start-\n  namespace: cloud-pak-deployer\nspec:\n  containers:\n  - name: cloud-pak-deployer\n    image: quay.io/cloud-pak-deployer/cloud-pak-deployer:latest\n    imagePullPolicy: Always\n    terminationMessagePath: /dev/termination-log\n    terminationMessagePolicy: File\n    command: [\"/bin/sh\",\"-xc\"]\n    args: \n      - /cloud-pak-deployer/scripts/deployer/cpd-start-deployer.sh --wizard\n    envFrom:\n    - configMapRef:\n        name: cloud-pak-deployer-env\n        optional: true\n  restartPolicy: Never\n  securityContext:\n    runAsUser: 0\n  serviceAccountName: cloud-pak-deployer-sa\n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift-wizard/#open-the-wizard","title":"Open the wizard","text":"<p>Once the start pod finishes, you can access the deployer wizard using the route created in the <code>cloud-pak-deployer</code> project.</p> <p>Info</p> <p>It may take a few minutes (5-10 minutes) until the route is created by the <code>deployer-start</code> pod and then an additional few minutes before the wizard has been started. If you don't see the route immediately, or if the deployer wizard page does not show up, please be patient.</p> <ul> <li>Open the OpenShift console</li> <li>Go to Networking \u2192 Routes</li> <li>Click the Cloud Pak Deployer <code>wizard</code> route</li> <li>Log in using your OpenShift cluster admin credentials (typically <code>kubeadmin</code>)</li> <li>Accept the information that will be shared with Cloud Pak Deployer</li> </ul>"},{"location":"10-use-deployer/3-run/existing-openshift/","title":"Running the Cloud Pak Deployer on an existing OpenShift cluster","text":"<p>When running the Cloud Pak Deployer on an existing OpenShift cluster, the following is assumed:</p> <ul> <li>The OpenShift cluster is up and running with sufficient compute nodes</li> <li>The appropriate storage class(es) have been pre-created</li> <li>You have cluster administrator permissions to OpenShift (except when using <code>--dry-run</code>)</li> </ul> <p>Remove OpenShift Pipelines before installing Watson Studio Pipelines</p> <p>The Watson Studio Pipelines (a.k.a. Pipeline Orchestration) cartridge deploys its own Tekton controllers. If the cluster already contains the Red Hat OpenShift Pipelines operator (common on TechZone clusters), Tekton resources conflict and the install fails. Cloud Pak Deployer will now check for <code>TektonConfig</code> objects and abort early, but you must remove OpenShift Pipelines yourself before you add <code>ws-pipelines</code> to a configuration.</p> <ol> <li>Run <code>oc get tektonconfig</code>. If it reports No resources found, you are ready to proceed.</li> <li>Otherwise remove Tekton and the operator components:    <pre><code>oc delete tektonconfig --all\noc delete csv -n openshift-operators \\\n  \"$(oc get csv -n openshift-operators -o jsonpath='{.items[?(@.spec.displayName==\"OpenShift Pipelines\")].metadata.name}')\"\noc delete subscription -n openshift-operators \\\n  \"$(oc get subscription -n openshift-operators -o jsonpath='{.items[?(@.spec.name==\"openshift-pipelines-operator\")].metadata.name}')\"\noc delete project tekton-pipelines --ignore-not-found\n</code></pre></li> <li>Re-run <code>oc get tektonconfig</code> and continue when it reports No resources found.</li> </ol> <p>You can reinstall the OpenShift Pipelines operator from OperatorHub after Watson Studio Pipelines is deployed.</p> <p>Info</p> <p>If you don't want to make changes to the OpenShift cluster and only want to review the steps deployer will run, you can use the <code>--dry-run</code> option with <code>cp-deploy.sh</code>. This will generate a log file <code>$STATUS_DIR/log/deployer-activities.log</code>, which lists the steps deployer will execute when running without <code>--dry-run</code>. Please note that the dry-run option has only been implemented for Cloud Pak for Data i.e. watsonx.</p> <p>Info</p> <p>You can also choose to run Cloud Pak Deployer as a job on the OpenShift cluster. This removes the dependency on a separate server or workstation to run the deployer. Please note that you may need unrestricted OpenShift entitlements for this. To run the deployer on OpenShift via the OpenShift console, see Run on OpenShift using console</p> <p>With the Existing OpenShift type of deployment you can install and configure the Cloud Pak(s) both on connected and disconnected (air-gapped) cluster. When using the deployer for a disconnected cluster, make sure you specify <code>--air-gapped</code> for the <code>cp-deploy.sh</code> command.</p> <p>There are 5 main steps to run the deployer for existing OpenShift:</p> <ol> <li>Configure deployer</li> <li>Prepare the cloud environment</li> <li>Obtain entitlement keys and secrets</li> <li>Set environment variables and secrets</li> <li>Run the deployer</li> </ol>"},{"location":"10-use-deployer/3-run/existing-openshift/#1-configure-deployer","title":"1. Configure deployer","text":""},{"location":"10-use-deployer/3-run/existing-openshift/#deployer-configuration-and-status-directories","title":"Deployer configuration and status directories","text":"<p>Deployer reads the configuration from a directory you set in the <code>CONFIG_DIR</code> environment variable. A status directory (<code>STATUS_DIR</code> environment variable) is used to log activities, store temporary files, scripts. If you use a File Vault (default), the secrets are kept in the <code>$STATUS_DIR/vault</code> directory.</p> <p>You can find OpenShift and Cloud Pak sample configuration (yaml) files here: sample configuration. For existing OpenShift installations, copy one of <code>ocp-existing-ocp-*.yaml</code> files into the <code>$CONFIG_DIR/config</code> directory. If you also want to install a Cloud Pak, copy one of the <code>cp4*.yaml</code> files.</p> <p>Example: <pre><code>mkdir -p $HOME/cpd-config/config\ncp sample-configurations/sample-dynamic/config-samples/ocp-existing-ocp-auto.yaml $HOME/cpd-config/config/\ncp sample-configurations/sample-dynamic/config-samples/cp4d-471.yaml $HOME/cpd-config/config/\n</code></pre></p>"},{"location":"10-use-deployer/3-run/existing-openshift/#set-configuration-and-status-directories-environment-variables","title":"Set configuration and status directories environment variables","text":"<p>Cloud Pak Deployer uses the status directory to log its activities and also to keep track of its running state. For a given environment you're provisioning or destroying, you should always specify the same status directory to avoid contention between different deploy runs. </p> <pre><code>export CONFIG_DIR=$HOME/cpd-config\nexport STATUS_DIR=$HOME/cpd-status\n</code></pre> <ul> <li><code>CONFIG_DIR</code>: Directory that holds the configuration, it must have a <code>config</code> subdirectory which contains the configuration <code>yaml</code> files.</li> <li><code>STATUS_DIR</code>: The directory where the Cloud Pak Deployer keeps all status information and logs files.</li> </ul>"},{"location":"10-use-deployer/3-run/existing-openshift/#optional-advanced-configuration","title":"Optional: advanced configuration","text":"<p>If the deployer configuration is kept on GitHub, follow the instructions in GitHub configuration.</p> <p>For special configuration with defaults and dynamic variables, refer to Advanced configuration.</p>"},{"location":"10-use-deployer/3-run/existing-openshift/#2-prepare-the-cloud-environment","title":"2. Prepare the cloud environment","text":"<p>No steps should be required to prepare the infrastructure; this type of installation expects the OpenShift cluster to be up and running with the supported storage classes.</p>"},{"location":"10-use-deployer/3-run/existing-openshift/#3-acquire-entitlement-keys-and-secrets","title":"3. Acquire entitlement keys and secrets","text":"<p>If you want to pull the Cloud Pak images from the entitled registry (i.e. an online install), or if you want to mirror the images to your private registry, you need to download the entitlement key. You can skip this step if you're installing from a private registry and all Cloud Pak images have already been downloaded to the private registry.</p> <ul> <li>Navigate to https://myibm.ibm.com/products-services/containerlibrary and login with your IBMId credentials</li> <li>Select Get Entitlement Key and create a new key (or copy your existing key)</li> <li>Copy the key value</li> </ul> <p>Warning</p> <p>As stated for the API key, you can choose to download the entitlement key to a file. However, when we reference the entitlement key, we mean the 80+ character string that is displayed, not the file.</p>"},{"location":"10-use-deployer/3-run/existing-openshift/#4-set-environment-variables-and-secrets","title":"4. Set environment variables and secrets","text":""},{"location":"10-use-deployer/3-run/existing-openshift/#set-the-cloud-pak-entitlement-key","title":"Set the Cloud Pak entitlement key","text":"<p>If you want the Cloud Pak images to be pulled from the entitled registry, set the Cloud Pak entitlement key.</p> <pre><code>export CP_ENTITLEMENT_KEY=your_cp_entitlement_key\n</code></pre> <ul> <li><code>CP_ENTITLEMENT_KEY</code>: This is the entitlement key you acquired as per the instructions above, this is a 80+ character string. You don't need to set this environment variable when you install the Cloud Pak(s) from a private registry</li> </ul>"},{"location":"10-use-deployer/3-run/existing-openshift/#store-the-openshift-login-command-or-configuration","title":"Store the OpenShift login command or configuration","text":"<p>Because you will be deploying the Cloud Pak on an existing OpenShift cluster, the deployer needs to be able to access OpenShift. There are thre methods for passing the login credentials of your OpenShift cluster(s) to the deployer process:</p> <ol> <li>Generic <code>oc login</code> command (preferred)</li> <li>Specific <code>oc login</code> command(s)</li> <li><code>kubeconfig</code> file</li> </ol> <p>Regardless of which authentication option you choose, the deployer will retrieve the secret from the vault when it requires access to OpenShift. If the secret cannot be found or if it is invalid or the OpenShift login token has expired, the deployer will fail and you will need to update the secret of your choice.</p> <p>For most OpenShift installations, you can retrieve the <code>oc login</code> command with a temporary token from the OpenShift console. Go to the OpenShift console and click on your user at the top right of the page to get the login command. Typically this command looks something like this: <code>oc login --server=https://api.pluto-01.coc.ibm.com:6443 --token=sha256~NQUUMroU4B6q_GTBAMS18Y3EIba1KHnJ08L2rBHvTHA</code></p> <p>Before passing the <code>oc login</code> command or the <code>kubeconfig</code> file, make sure you can login to your cluster using the command or the config file. If the cluster's API server has a self-signed certificate, make sure you specify the <code>--insecure-skip-tls-verify</code> flag for the <code>oc login</code> command.</p> <p>Example: <pre><code>oc login api.pluto-01.coc.ibm.com:6443 -u kubeadmin -p BmxQ5-KjBFx-FgztG-gpTF3 --insecure-skip-tls-verify\n</code></pre></p> <p>Output: <pre><code>Login successful.\n\nYou have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n</code></pre></p>"},{"location":"10-use-deployer/3-run/existing-openshift/#option-1---generic-oc-login-command","title":"Option 1 - Generic <code>oc login</code> command","text":"<p>This is the most straightforward option if you only have 1 OpenShift cluster in your configuration.</p> <p>Set the environment variable for the <code>oc login</code> command <pre><code>export CPD_OC_LOGIN=\"oc login api.pluto-01.coc.ibm.com:6443 -u kubeadmin -p BmxQ5-KjBFx-FgztG-gpTF3 --insecure-skip-tls-verify\"\n</code></pre></p> <p>Info</p> <p>Make sure you put the oc login command between quotes (single or double) to make sure the full command is stored.</p> <p>When the deployer is run, it automatically sets the <code>oc-login</code> vault secret to the specified <code>oc login</code> command. When logging in to OpenShift, the deployer first checks if there is a specific <code>oc login</code> secret for the cluster in question (see option 2). If there is not, it will default to the generic <code>oc-login</code> secret (option 1).</p>"},{"location":"10-use-deployer/3-run/existing-openshift/#option-2---specific-oc-login-commands","title":"Option 2 - Specific <code>oc login</code> command(s)","text":"<p>Use this option if you have multiple OpenShift clusters configured in th deployer configuration.</p> <p>Store the login command in secret <code>&lt;cluster name&gt;-oc-login</code> <pre><code>cp-deploy.sh vault set \\\n  -vs pluto-01-oc-login \\\n  -vsv \"oc login api.pluto-01.coc.ibm.com:6443 -u kubeadmin -p BmxQ5-KjBFx-FgztG-gpTF3 --insecure-skip-tls-verify\"\n</code></pre></p> <p>Info</p> <p>Make sure you put the oc login command between quotes (single or double) to make sure the full command is stored.</p>"},{"location":"10-use-deployer/3-run/existing-openshift/#option-3---use-a-kubeconfig-file","title":"Option 3 - Use a kubeconfig file","text":"<p>If you already have a \"kubeconfig\" file that holds the credentials of your cluster, you can use this, otherwise: - Log in to OpenShift as a cluster administrator using your method of choice - Locate the Kubernetes config file. If you have logged in with the OpenShift client, this is typically <code>~/.kube/config</code></p> <p>If you did not just login to the cluster, the current context of the kubeconfig file may not point to your cluster. The deployer will check that the server the current context points to matches the <code>cluster_name</code> and <code>domain_name</code> of the configured <code>openshift</code> object. To check the current context, run the following command: <pre><code>oc config current-context\n</code></pre></p> <p>Now, store the Kubernetes config file as a vault secret. <pre><code>cp-deploy.sh vault set \\\n    --vault-secret kubeconfig \\\n    --vault-secret-file ~/.kube/config\n</code></pre></p> <p>If the deployer manages multiple OpenShift clusters, you can specify a kubeconfig file for each of the clusters by prefixing the <code>kubeconfig</code> with the name of the <code>openshift</code> object, for example: <pre><code>cp-deploy.sh vault set \\\n    --vault-secret pluto-01-kubeconfig \\\n    --vault-secret-file /data/pluto-01/kubeconfig\n\ncp-deploy.sh vault set \\\n    --vault-secret venus-02-kubeconfig \\\n    --vault-secret-file /data/venus-02/kubeconfig\n</code></pre> When connecting to the OpenShift cluster, a cluster-specific kubeconfig vault secret will take precedence over the generic <code>kubeconfig</code> secret.</p>"},{"location":"10-use-deployer/3-run/existing-openshift/#optional-set-the-github-personal-access-token-pat","title":"Optional: Set the GitHub Personal Access Token (PAT)","text":"<p>In some cases, download of the <code>cloudctl</code> and <code>cpd-cli</code> clients from @IBM will fail because GitHub limits the number of API calls from non-authenticated clients. You can remediate this issue by creating a Personal Access Token on github.com and creating a secret in the vault.</p> <pre><code>cp-deploy.sh vault set -vs github-ibm-pat=&lt;your PAT&gt;\n</code></pre> <p>Alternatively, you can set the secret by adding <code>-vs github-ibm-pat=&lt;your PAT&gt;</code> to the <code>cp-deploy.sh env apply</code> command.</p>"},{"location":"10-use-deployer/3-run/existing-openshift/#5-run-the-deployer","title":"5. Run the deployer","text":""},{"location":"10-use-deployer/3-run/existing-openshift/#set-path-and-alias-for-the-deployer","title":"Set path and alias for the deployer","text":"<pre><code>source ./set-env.sh\n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift/#optional-validate-the-configuration","title":"Optional: validate the configuration","text":"<p>If you only want to validate the configuration, you can run the dpeloyer with the <code>--check-only</code> argument. This will run the first stage to validate variables and vault secrets and then execute the generators.</p> <pre><code>cp-deploy.sh env apply --check-only --accept-all-licenses\n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift/#run-the-cloud-pak-deployer","title":"Run the Cloud Pak Deployer","text":"<p>To run the container using a local configuration input directory and a data directory where temporary and state is kept, use the example below. If you don't specify the status directory, the deployer will automatically create a temporary directory. Please note that the status directory will also hold secrets if you have configured a flat file vault. If you lose the directory, you will not be able to make changes to the configuration and adjust the deployment. It is best to specify a permanent directory that you can reuse later. If you specify an existing directory the current user must be the owner of the directory. Failing to do so may cause the container to fail with insufficient permissions.</p> <pre><code>cp-deploy.sh env apply --accept-all-licenses\n</code></pre> <p>You can also specify extra variables such as <code>env_id</code> to override the names of the objects referenced in the <code>.yaml</code> configuration files as <code>{{ env_id }}-xxxx</code>. For more information about the extra (dynamic) variables, see advanced configuration.</p> <p>The <code>--accept-all-licenses</code> flag is optional and confirms that you accept all licenses of the installed cartridges and instances. Licenses must be either accepted in the configuration files or at the command line.</p> <p>When running the command, the container will start as a daemon and the command will tail-follow the logs. You can press Ctrl-C at any time to interrupt the logging but the container will continue to run in the background.</p> <p>You can return to view the logs as follows:</p> <pre><code>cp-deploy.sh env logs\n</code></pre> <p>Deploying the infrastructure, preparing OpenShift and installing the Cloud Pak will take a long time, typically between 1-5 hours,dependent on which Cloud Pak cartridges you configured. For estimated duration of the steps, refer to Timings.</p> <p>If you need to interrupt the automation, use CTRL-C to stop the logging output and then use:</p> <pre><code>cp-deploy.sh env kill\n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift/#on-failure","title":"On failure","text":"<p>If the Cloud Pak Deployer fails, for example because certain infrastructure components are temporarily not available, fix the cause if needed and then just re-run it with the same <code>CONFIG_DIR</code> and <code>STATUS_DIR</code> as well extra variables. The provisioning process has been designed to be idempotent and it will not redo actions that have already completed successfully.</p>"},{"location":"10-use-deployer/3-run/existing-openshift/#finishing-up","title":"Finishing up","text":"<p>Once the process has finished, it will output the URLs by which you can access the deployed Cloud Pak.  <pre><code>--- Cloud Pak for Data project cpd ---\nCP4D URL: https://cpd-cpd.apps.6759f8089266ae8f450d554f.ocp.techzone.ibm.com\nCP4D User: cpadmin\nCP4D cpadmin password: &lt;your-cpadmin-password&gt;\n</code></pre></p> <p>You can also find this information under the <code>cloud-paks</code> directory in the status directory you specified.</p> <p>To retrieve the Cloud Pak URL(s):</p> <pre><code>cat $STATUS_DIR/cloud-paks/*\n</code></pre> <p>This will show the Cloud Pak URLs:</p> <pre><code>Cloud Pak for Data URL for cluster pluto-01 and project cpd (domain name specified was example.com):\nhttps://cpd-cpd.apps.pluto-01.example.com\n</code></pre> <p>The <code>admin</code> password can be retrieved from the vault as follows:</p> <p>List the secrets in the vault:</p> <pre><code>cp-deploy.sh vault list\n</code></pre> <p>This will show something similar to the following:</p> <pre><code>Secret list for group sample:\n- ibm_cp_entitlement_key\n- oc-login\n- cp4d_admin_cpd_demo\n</code></pre> <p>You can then retrieve the Cloud Pak for Data admin password like this:</p> <pre><code>cp-deploy.sh vault get --vault-secret cp4d_admin_cpd_sample\n</code></pre> <pre><code>PLAY [Secrets] *****************************************************************\nincluded: /cloud-pak-deployer/automation-roles/99-generic/vault/vault-get-secret/tasks/get-secret-file.yml for localhost\ncp4d_admin_zen_sample_sample: gelGKrcgaLatBsnAdMEbmLwGr\n</code></pre>"},{"location":"10-use-deployer/3-run/existing-openshift/#post-install-configuration","title":"Post-install configuration","text":"<p>You can find examples of a couple of typical changes you may want to do here: Post-run changes.</p>"},{"location":"10-use-deployer/3-run/ibm-cloud/","title":"Running the Cloud Pak Deployer on IBM Cloud","text":"<p>You can use Cloud Pak Deployer to create a ROKS (Red Hat OpenShift Kubernetes Service) on IBM Cloud.</p> <p>There are 5 main steps to run the deployer for IBM Cloud:</p> <ol> <li>Configure deployer</li> <li>Prepare the cloud environment</li> <li>Obtain entitlement keys and secrets</li> <li>Set environment variables and secrets</li> <li>Run the deployer</li> </ol> <p>See the deployer in action in this video:</p>"},{"location":"10-use-deployer/3-run/ibm-cloud/#topology","title":"Topology","text":"<p>A typical setup of the ROKS cluster on IBM Cloud VPC is pictured below: </p>"},{"location":"10-use-deployer/3-run/ibm-cloud/#1-configure-deployer","title":"1. Configure deployer","text":""},{"location":"10-use-deployer/3-run/ibm-cloud/#deployer-configuration-and-status-directories","title":"Deployer configuration and status directories","text":"<p>Deployer reads the configuration from a directory you set in the <code>CONFIG_DIR</code> environment variable. A status directory (<code>STATUS_DIR</code> environment variable) is used to log activities, store temporary files, scripts. If you use a File Vault (default), the secrets are kept in the <code>$STATUS_DIR/vault</code> directory.</p> <p>You can find OpenShift and Cloud Pak sample configuration (yaml) files here: sample configuration. For IBM Cloud installations, copy one of <code>ocp-ibm-cloud-roks*.yaml</code> files into the <code>$CONFIG_DIR/config</code> directory. If you also want to install a Cloud Pak, copy one of the <code>cp4*.yaml</code> files.</p> <p>Example: <pre><code>mkdir -p $HOME/cpd-config/config\ncp sample-configurations/sample-dynamic/config-samples/ocp-ibm-cloud-roks-odf.yaml $HOME/cpd-config/config/\ncp sample-configurations/sample-dynamic/config-samples/cp4d-471.yaml $HOME/cpd-config/config/\n</code></pre></p>"},{"location":"10-use-deployer/3-run/ibm-cloud/#set-configuration-and-status-directories-environment-variables","title":"Set configuration and status directories environment variables","text":"<p>Cloud Pak Deployer uses the status directory to log its activities and also to keep track of its running state. For a given environment you're provisioning or destroying, you should always specify the same status directory to avoid contention between different deploy runs. </p> <pre><code>export CONFIG_DIR=$HOME/cpd-config\nexport STATUS_DIR=$HOME/cpd-status\n</code></pre> <ul> <li><code>CONFIG_DIR</code>: Directory that holds the configuration, it must have a <code>config</code> subdirectory which contains the configuration <code>yaml</code> files.</li> <li><code>STATUS_DIR</code>: The directory where the Cloud Pak Deployer keeps all status information and logs files.</li> </ul>"},{"location":"10-use-deployer/3-run/ibm-cloud/#optional-advanced-configuration","title":"Optional: advanced configuration","text":"<p>If the deployer configuration is kept on GitHub, follow the instructions in GitHub configuration.</p> <p>For special configuration with defaults and dynamic variables, refer to Advanced configuration.</p>"},{"location":"10-use-deployer/3-run/ibm-cloud/#2-prepare-the-cloud-environment","title":"2. Prepare the cloud environment","text":""},{"location":"10-use-deployer/3-run/ibm-cloud/#create-an-ibm-cloud-api-key","title":"Create an IBM Cloud API Key","text":"<p>In order for the Cloud Pak Deployer to create the infrastructure and deploy IBM Cloud Pak for Data, it must perform tasks on IBM Cloud. In order to do so it requires an IBM Cloud API Key. This can be created by following these steps:</p> <ul> <li>Go to https://cloud.ibm.com/iam/apikeys and login with your IBMid credentials</li> <li>Ensure you have selected the correct IBM Cloud Account for which you wish to use the Cloud Pak Deployer</li> <li>Click Create an IBM Cloud API Key and provide a name and description</li> <li>Copy the IBM Cloud API key using the Copy button and store it in a safe place, as you will not be able to retrieve it later</li> </ul> <p>Warning</p> <p>You can choose to download the API key for later reference. However, when we reference the API key, we mean the IBM Cloud API key as a 40+ character string.</p>"},{"location":"10-use-deployer/3-run/ibm-cloud/#set-environment-variables-for-ibm-cloud","title":"Set environment variables for IBM Cloud","text":"<p>Set the environment variables specific to IBM Cloud deployments. <pre><code>export IBM_CLOUD_API_KEY=your_api_key\n</code></pre></p> <ul> <li><code>IBM_CLOUD_API_KEY</code>: This is the API key you generated using your IBM Cloud account, this is a 40+ character string</li> </ul>"},{"location":"10-use-deployer/3-run/ibm-cloud/#3-acquire-entitlement-keys-and-secrets","title":"3. Acquire entitlement keys and secrets","text":"<p>If you want to pull the Cloud Pak images from the entitled registry (i.e. an online install), or if you want to mirror the images to your private registry, you need to download the entitlement key. You can skip this step if you're installing from a private registry and all Cloud Pak images have already been downloaded to the private registry.</p> <ul> <li>Navigate to https://myibm.ibm.com/products-services/containerlibrary and login with your IBMId credentials</li> <li>Select Get Entitlement Key and create a new key (or copy your existing key)</li> <li>Copy the key value</li> </ul> <p>Warning</p> <p>As stated for the API key, you can choose to download the entitlement key to a file. However, when we reference the entitlement key, we mean the 80+ character string that is displayed, not the file.</p>"},{"location":"10-use-deployer/3-run/ibm-cloud/#4-set-environment-variables-and-secrets","title":"4. Set environment variables and secrets","text":""},{"location":"10-use-deployer/3-run/ibm-cloud/#set-the-cloud-pak-entitlement-key","title":"Set the Cloud Pak entitlement key","text":"<p>If you want the Cloud Pak images to be pulled from the entitled registry, set the Cloud Pak entitlement key.</p> <pre><code>export CP_ENTITLEMENT_KEY=your_cp_entitlement_key\n</code></pre> <ul> <li><code>CP_ENTITLEMENT_KEY</code>: This is the entitlement key you acquired as per the instructions above, this is a 80+ character string. You don't need to set this environment variable when you install the Cloud Pak(s) from a private registry</li> </ul>"},{"location":"10-use-deployer/3-run/ibm-cloud/#optional-set-the-github-personal-access-token-pat","title":"Optional: Set the GitHub Personal Access Token (PAT)","text":"<p>In some cases, download of the <code>cloudctl</code> and <code>cpd-cli</code> clients from @IBM will fail because GitHub limits the number of API calls from non-authenticated clients. You can remediate this issue by creating a Personal Access Token on github.com and creating a secret in the vault.</p> <pre><code>cp-deploy.sh vault set -vs github-ibm-pat=&lt;your PAT&gt;\n</code></pre> <p>Alternatively, you can set the secret by adding <code>-vs github-ibm-pat=&lt;your PAT&gt;</code> to the <code>cp-deploy.sh env apply</code> command.</p>"},{"location":"10-use-deployer/3-run/ibm-cloud/#5-run-the-deployer","title":"5. Run the deployer","text":""},{"location":"10-use-deployer/3-run/ibm-cloud/#set-path-and-alias-for-the-deployer","title":"Set path and alias for the deployer","text":"<pre><code>source ./set-env.sh\n</code></pre>"},{"location":"10-use-deployer/3-run/ibm-cloud/#optional-validate-the-configuration","title":"Optional: validate the configuration","text":"<p>If you only want to validate the configuration, you can run the dpeloyer with the <code>--check-only</code> argument. This will run the first stage to validate variables and vault secrets and then execute the generators.</p> <pre><code>cp-deploy.sh env apply --check-only --accept-all-licenses\n</code></pre>"},{"location":"10-use-deployer/3-run/ibm-cloud/#run-the-cloud-pak-deployer","title":"Run the Cloud Pak Deployer","text":"<p>To run the container using a local configuration input directory and a data directory where temporary and state is kept, use the example below. If you don't specify the status directory, the deployer will automatically create a temporary directory. Please note that the status directory will also hold secrets if you have configured a flat file vault. If you lose the directory, you will not be able to make changes to the configuration and adjust the deployment. It is best to specify a permanent directory that you can reuse later. If you specify an existing directory the current user must be the owner of the directory. Failing to do so may cause the container to fail with insufficient permissions.</p> <pre><code>cp-deploy.sh env apply --accept-all-licenses\n</code></pre> <p>You can also specify extra variables such as <code>env_id</code> to override the names of the objects referenced in the <code>.yaml</code> configuration files as <code>{{ env_id }}-xxxx</code>. For more information about the extra (dynamic) variables, see advanced configuration.</p> <p>The <code>--accept-all-licenses</code> flag is optional and confirms that you accept all licenses of the installed cartridges and instances. Licenses must be either accepted in the configuration files or at the command line.</p> <p>When running the command, the container will start as a daemon and the command will tail-follow the logs. You can press Ctrl-C at any time to interrupt the logging but the container will continue to run in the background.</p> <p>You can return to view the logs as follows:</p> <pre><code>cp-deploy.sh env logs\n</code></pre> <p>Deploying the infrastructure, preparing OpenShift and installing the Cloud Pak will take a long time, typically between 1-5 hours,dependent on which Cloud Pak cartridges you configured. For estimated duration of the steps, refer to Timings.</p> <p>If you need to interrupt the automation, use CTRL-C to stop the logging output and then use:</p> <pre><code>cp-deploy.sh env kill\n</code></pre>"},{"location":"10-use-deployer/3-run/ibm-cloud/#on-failure","title":"On failure","text":"<p>If the Cloud Pak Deployer fails, for example because certain infrastructure components are temporarily not available, fix the cause if needed and then just re-run it with the same <code>CONFIG_DIR</code> and <code>STATUS_DIR</code> as well extra variables. The provisioning process has been designed to be idempotent and it will not redo actions that have already completed successfully.</p>"},{"location":"10-use-deployer/3-run/ibm-cloud/#finishing-up","title":"Finishing up","text":"<p>Once the process has finished, it will output the URLs by which you can access the deployed Cloud Pak. You can also find this information under the <code>cloud-paks</code> directory in the status directory you specified.</p> <p>To retrieve the Cloud Pak URL(s):</p> <pre><code>cat $STATUS_DIR/cloud-paks/*\n</code></pre> <p>This will show the Cloud Pak URLs:</p> <pre><code>Cloud Pak for Data URL for cluster pluto-01 and project cpd (domain name specified was example.com):\nhttps://cpd-cpd.apps.pluto-01.example.com\n</code></pre> <p>The <code>admin</code> password can be retrieved from the vault as follows:</p> <p>List the secrets in the vault:</p> <pre><code>cp-deploy.sh vault list\n</code></pre> <p>This will show something similar to the following:</p> <pre><code>Secret list for group sample:\n- ibm_cp_entitlement_key\n- sample-provision-ssh-key\n- sample-provision-ssh-pub-key\n- sample-terraform-tfstate\n- cp4d_admin_cpd_demo\n</code></pre> <p>You can then retrieve the Cloud Pak for Data admin password like this:</p> <pre><code>cp-deploy.sh vault get --vault-secret cp4d_admin_cpd_demo\n</code></pre> <pre><code>PLAY [Secrets] *****************************************************************\nincluded: /cloud-pak-deployer/automation-roles/99-generic/vault/vault-get-secret/tasks/get-secret-file.yml for localhost\ncp4d_admin_zen_sample_sample: gelGKrcgaLatBsnAdMEbmLwGr\n</code></pre>"},{"location":"10-use-deployer/3-run/ibm-cloud/#post-install-configuration","title":"Post-install configuration","text":"<p>You can find examples of a couple of typical changes you may want to do here: Post-run changes.</p>"},{"location":"10-use-deployer/3-run/ibm-fusion/","title":"Running the Cloud Pak Deployer on Fusion Storage and Fusion HCI","text":"<p>This guide provides detailed instructions on using Cloud Pak Deployer on Fusion Storage and Fusion Hyper-Converged Infrastructure (HCI).</p> <p>Fusion Storage (Software Defined Storage) is an IBM software offering that provides different storage options for file, block and object storage. The two storage options are: </p> <ul> <li>Fusion Data Foundation - this integrates Ceph for block and file storage with rook-ceph for object storage and NooBaa as a gateway to external object storage providers.</li> <li>IBM Storage Scale - this is the high-performance clustered file system, formerly known as Spectrum Scale and GPFS.</li> </ul> <p>Fusion HCI (Hyper-Converged Infrastructure) is an IBM offering that combines compute, storage, and networking resources into a single, pre-configured system. It simplifies IT infrastructure management and is optimized for deploying containerized applications. Notably, Fusion HCI integrates OpenShift, a leading container orchestration platform, for streamlined development and deployment. The stack is very simple : </p> <ul> <li>Bare Metal</li> <li>RedHat CoreOS</li> <li>OpenShift</li> </ul> <p></p> <p>Info</p> <p>This guide detail the process of installing Cloud Pak for Data on a Fusion HCI using Storage Scale as the underlying storage solution. Fusion Storage also offers the option to deploy FDF. IBM Storage Scale uses ibm-storage-fusion-cp-sc storage class and is created by default on this environment.</p> <p>There are 5 main steps to run the deployer for Fusion HCI</p> <ol> <li>Configure deployer</li> <li>Prepare the infrastructure</li> <li>Obtain entitlement keys and secrets</li> <li>Set environment variables and secrets</li> <li>Run the deployer</li> </ol>"},{"location":"10-use-deployer/3-run/ibm-fusion/#1-configure-deployer","title":"1. Configure deployer","text":""},{"location":"10-use-deployer/3-run/ibm-fusion/#deployer-configuration-and-status-directories","title":"Deployer configuration and status directories","text":"<p>Deployer reads the configuration from a directory you set in the <code>CONFIG_DIR</code> environment variable. A status directory (<code>STATUS_DIR</code> environment variable) is used to log activities, store temporary files, scripts. If you use a File Vault (default), the secrets are kept in the <code>$STATUS_DIR/vault</code> directory.</p> <p>Discover sample configuration YAML files for OpenShift and Cloud Pak here: sample configuration. To set up Fusion HCI, transfer the ocp-existing-fusion-storage-scale.yaml file to the $CONFIG_DIR/config directory. If you're interested in installing watsonx as well, choose one of the watsonx-*.yaml files and copy it accordingly.</p> <p>Example: <pre><code>mkdir -p $HOME/cpd-config/config\ncp sample-configurations/sample-dynamic/config-samples/ocp-existing-fusion-storage-scale.yaml $HOME/cpd-config/config/\ncp sample-configurations/sample-dynamic/config-samples/watsonx-480.yaml $HOME/cpd-config/config/\n</code></pre></p> <p>Attention</p> <p>Make sure you review the storage classes in the config files to match your cluster.</p>"},{"location":"10-use-deployer/3-run/ibm-fusion/#set-configuration-and-status-directories-environment-variables","title":"Set configuration and status directories environment variables","text":"<p>Cloud Pak Deployer uses the status directory to log its activities and also to keep track of its running state. For a given environment you're provisioning or destroying, you should always specify the same status directory to avoid contention between different deploy runs. </p> <pre><code>export CONFIG_DIR=$HOME/cpd-config\nexport STATUS_DIR=$HOME/cpd-status\n</code></pre> <ul> <li><code>CONFIG_DIR</code>: Directory that holds the configuration, it must have a <code>config</code> subdirectory which contains the configuration <code>yaml</code> files.</li> <li><code>STATUS_DIR</code>: The directory where the Cloud Pak Deployer keeps all status information and logs files.</li> </ul>"},{"location":"10-use-deployer/3-run/ibm-fusion/#optional-advanced-configuration","title":"Optional: advanced configuration","text":"<p>If the deployer configuration is kept on GitHub, follow the instructions in GitHub configuration.</p> <p>For special configuration with defaults and dynamic variables, refer to Advanced configuration.</p>"},{"location":"10-use-deployer/3-run/ibm-fusion/#2-prepare-the-infrastructure","title":"2. Prepare the infrastructure","text":"<p>Fusion HCI/Storage is expected to be configured already with the required storage classes, the user should not require to run any steps in this bullet.</p> <p>For more information:  Fusion HCI: https://www.ibm.com/docs/en/fusion-hci-systems Fusion Storage: https://www.ibm.com/docs/en/fusion-software</p>"},{"location":"10-use-deployer/3-run/ibm-fusion/#3-acquire-entitlement-keys-and-secrets","title":"3. Acquire entitlement keys and secrets","text":"<p>If you want to pull the Cloud Pak images from the entitled registry (i.e. an online install), or if you want to mirror the images to your private registry, you need to download the entitlement key. You can skip this step if you're installing from a private registry and all Cloud Pak images have already been downloaded to the private registry.</p> <ul> <li>Navigate to https://myibm.ibm.com/products-services/containerlibrary and login with your IBMId credentials</li> <li>Select Get Entitlement Key and create a new key (or copy your existing key)</li> <li>Copy the key value</li> </ul> <p>Warning</p> <p>As stated for the API key, you can choose to download the entitlement key to a file. However, when we reference the entitlement key, we mean the 80+ character string that is displayed, not the file.</p>"},{"location":"10-use-deployer/3-run/ibm-fusion/#4-set-environment-variables-and-secrets","title":"4. Set environment variables and secrets","text":""},{"location":"10-use-deployer/3-run/ibm-fusion/#set-the-cloud-pak-entitlement-key","title":"Set the Cloud Pak entitlement key","text":"<p>If you want the Cloud Pak images to be pulled from the entitled registry, set the Cloud Pak entitlement key.</p> <pre><code>export CP_ENTITLEMENT_KEY=your_cp_entitlement_key\n</code></pre> <ul> <li><code>CP_ENTITLEMENT_KEY</code>: This is the entitlement key you acquired as per the instructions above, this is a 80+ character string. You don't need to set this environment variable when you install the Cloud Pak(s) from a private registry</li> </ul>"},{"location":"10-use-deployer/3-run/ibm-fusion/#store-the-openshift-login-command-or-configuration","title":"Store the OpenShift login command or configuration","text":"<p>Because you will be deploying the Cloud Pak on the Fusion HCI/Storage cluster, the deployer needs to be able to access OpenShift. There are thre methods for passing the login credentials of your OpenShift cluster(s) to the deployer process:</p> <ol> <li>Generic <code>oc login</code> command (preferred)</li> <li>Specific <code>oc login</code> command(s)</li> <li><code>kubeconfig</code> file</li> </ol> <p>Regardless of which authentication option you choose, the deployer will retrieve the secret from the vault when it requires access to OpenShift. If the secret cannot be found or if it is invalid or the OpenShift login token has expired, the deployer will fail and you will need to update the secret of your choice.</p> <p>For most OpenShift installations, you can retrieve the <code>oc login</code> command with a temporary token from the OpenShift console. Go to the OpenShift console and click on your user at the top right of the page to get the login command. Typically this command looks something like this: <code>oc login --server=https://api.pluto-01.coc.ibm.com:6443 --token=sha256~NQUUMroU4B6q_GTBAMS18Y3EIba1KHnJ08L2rBHvTHA</code></p> <p>Before passing the <code>oc login</code> command or the <code>kubeconfig</code> file, make sure you can login to your cluster using the command or the config file. If the cluster's API server has a self-signed certificate, make sure you specify the <code>--insecure-skip-tls-verify</code> flag for the <code>oc login</code> command.</p> <p>Example: <pre><code>oc login api.pluto-01.coc.ibm.com:6443 -u kubeadmin -p BmxQ5-KjBFx-FgztG-gpTF3 --insecure-skip-tls-verify\n</code></pre></p> <p>Output: <pre><code>Login successful.\n\nYou have access to 65 projects, the list has been suppressed. You can list all projects with 'oc projects'\n\nUsing project \"default\".\n</code></pre></p>"},{"location":"10-use-deployer/3-run/ibm-fusion/#option-1---generic-oc-login-command","title":"Option 1 - Generic <code>oc login</code> command","text":"<p>This is the most straightforward option if you only have 1 OpenShift cluster in your configuration.</p> <p>Set the environment variable for the <code>oc login</code> command <pre><code>export CPD_OC_LOGIN=\"oc login api.pluto-01.coc.ibm.com:6443 -u kubeadmin -p BmxQ5-KjBFx-FgztG-gpTF3 --insecure-skip-tls-verify\"\n</code></pre></p> <p>Info</p> <p>Make sure you put the oc login command between quotes (single or double) to make sure the full command is stored.</p> <p>When the deployer is run, it automatically sets the <code>oc-login</code> vault secret to the specified <code>oc login</code> command. When logging in to OpenShift, the deployer first checks if there is a specific <code>oc login</code> secret for the cluster in question (see option 2). If there is not, it will default to the generic <code>oc-login</code> secret (option 1).</p>"},{"location":"10-use-deployer/3-run/ibm-fusion/#option-2---specific-oc-login-commands","title":"Option 2 - Specific <code>oc login</code> command(s)","text":"<p>Use this option if you have multiple OpenShift clusters configured in th deployer configuration.</p> <p>Store the login command in secret <code>&lt;cluster name&gt;-oc-login</code> <pre><code>cp-deploy.sh vault set \\\n  -vs pluto-01-oc-login \\\n  -vsv \"oc login api.pluto-01.coc.ibm.com:6443 -u kubeadmin -p BmxQ5-KjBFx-FgztG-gpTF3 --insecure-skip-tls-verify\"\n</code></pre></p> <p>Info</p> <p>Make sure you put the oc login command between quotes (single or double) to make sure the full command is stored.</p>"},{"location":"10-use-deployer/3-run/ibm-fusion/#option-3---use-a-kubeconfig-file","title":"Option 3 - Use a kubeconfig file","text":"<p>If you already have a \"kubeconfig\" file that holds the credentials of your cluster, you can use this, otherwise: - Log in to OpenShift as a cluster administrator using your method of choice - Locate the Kubernetes config file. If you have logged in with the OpenShift client, this is typically <code>~/.kube/config</code></p> <p>If you did not just login to the cluster, the current context of the kubeconfig file may not point to your cluster. The deployer will check that the server the current context points to matches the <code>cluster_name</code> and <code>domain_name</code> of the configured <code>openshift</code> object. To check the current context, run the following command: <pre><code>oc config current-context\n</code></pre></p> <p>Now, store the Kubernetes config file as a vault secret. <pre><code>cp-deploy.sh vault set \\\n    --vault-secret kubeconfig \\\n    --vault-secret-file ~/.kube/config\n</code></pre></p> <p>If the deployer manages multiple OpenShift clusters, you can specify a kubeconfig file for each of the clusters by prefixing the <code>kubeconfig</code> with the name of the <code>openshift</code> object, for example: <pre><code>cp-deploy.sh vault set \\\n    --vault-secret pluto-01-kubeconfig \\\n    --vault-secret-file /data/pluto-01/kubeconfig\n\ncp-deploy.sh vault set \\\n    --vault-secret venus-02-kubeconfig \\\n    --vault-secret-file /data/venus-02/kubeconfig\n</code></pre> When connecting to the OpenShift cluster, a cluster-specific kubeconfig vault secret will take precedence over the generic <code>kubeconfig</code> secret.</p>"},{"location":"10-use-deployer/3-run/ibm-fusion/#5-run-the-deployer","title":"5. Run the deployer","text":""},{"location":"10-use-deployer/3-run/ibm-fusion/#set-path-and-alias-for-the-deployer","title":"Set path and alias for the deployer","text":"<pre><code>source ./set-env.sh\n</code></pre>"},{"location":"10-use-deployer/3-run/ibm-fusion/#optional-validate-the-configuration","title":"Optional: validate the configuration","text":"<p>If you only want to validate the configuration, you can run the dpeloyer with the <code>--check-only</code> argument. This will run the first stage to validate variables and vault secrets and then execute the generators.</p> <pre><code>cp-deploy.sh env apply --check-only --accept-all-licenses\n</code></pre>"},{"location":"10-use-deployer/3-run/ibm-fusion/#run-the-cloud-pak-deployer","title":"Run the Cloud Pak Deployer","text":"<p>To run the container using a local configuration input directory and a data directory where temporary and state is kept, use the example below. If you don't specify the status directory, the deployer will automatically create a temporary directory. Please note that the status directory will also hold secrets if you have configured a flat file vault. If you lose the directory, you will not be able to make changes to the configuration and adjust the deployment. It is best to specify a permanent directory that you can reuse later. If you specify an existing directory the current user must be the owner of the directory. Failing to do so may cause the container to fail with insufficient permissions.</p> <pre><code>cp-deploy.sh env apply --accept-all-licenses\n</code></pre> <p>You can also specify extra variables such as <code>env_id</code> to override the names of the objects referenced in the <code>.yaml</code> configuration files as <code>{{ env_id }}-xxxx</code>. For more information about the extra (dynamic) variables, see advanced configuration.</p> <p>The <code>--accept-all-licenses</code> flag is optional and confirms that you accept all licenses of the installed cartridges and instances. Licenses must be either accepted in the configuration files or at the command line.</p> <p>When running the command, the container will start as a daemon and the command will tail-follow the logs. You can press Ctrl-C at any time to interrupt the logging but the container will continue to run in the background.</p> <p>You can return to view the logs as follows:</p> <pre><code>cp-deploy.sh env logs\n</code></pre> <p>Deploying the infrastructure, preparing OpenShift and installing the Cloud Pak will take a long time, typically between 1-5 hours,dependent on which Cloud Pak cartridges you configured. For estimated duration of the steps, refer to Timings.</p> <p>If you need to interrupt the automation, use CTRL-C to stop the logging output and then use:</p> <pre><code>cp-deploy.sh env kill\n</code></pre>"},{"location":"10-use-deployer/3-run/ibm-fusion/#on-failure","title":"On failure","text":"<p>If the Cloud Pak Deployer fails, for example because certain infrastructure components are temporarily not available, fix the cause if needed and then just re-run it with the same <code>CONFIG_DIR</code> and <code>STATUS_DIR</code> as well extra variables. The provisioning process has been designed to be idempotent and it will not redo actions that have already completed successfully.</p>"},{"location":"10-use-deployer/3-run/ibm-fusion/#finishing-up","title":"Finishing up","text":"<p>Once the process has finished, it will output the URLs by which you can access the deployed Cloud Pak. You can also find this information under the <code>cloud-paks</code> directory in the status directory you specified.</p> <p>To retrieve the Cloud Pak URL(s):</p> <pre><code>cat $STATUS_DIR/cloud-paks/*\n</code></pre> <p>This will show the Cloud Pak URLs:</p> <pre><code>Cloud Pak for Data URL for cluster pluto-01 and project cpd (domain name specified was example.com):\nhttps://cpd-cpd.apps.pluto-01.example.com\n</code></pre> <p>The <code>admin</code> password can be retrieved from the vault as follows:</p> <p>List the secrets in the vault:</p> <pre><code>cp-deploy.sh vault list\n</code></pre> <p>This will show something similar to the following:</p> <pre><code>Secret list for group sample:\n- ibm_cp_entitlement_key\n- oc-login\n- cp4d_admin_cpd_demo\n</code></pre> <p>You can then retrieve the Cloud Pak for Data admin password like this:</p> <pre><code>cp-deploy.sh vault get --vault-secret cp4d_admin_cpd_sample\n</code></pre> <pre><code>PLAY [Secrets] *****************************************************************\nincluded: /cloud-pak-deployer/automation-roles/99-generic/vault/vault-get-secret/tasks/get-secret-file.yml for localhost\ncp4d_admin_zen_sample_sample: gelGKrcgaLatBsnAdMEbmLwGr\n</code></pre>"},{"location":"10-use-deployer/3-run/ibm-fusion/#post-install-configuration","title":"Post-install configuration","text":"<p>You can find examples of a couple of typical changes you may want to do here: Post-run changes.</p>"},{"location":"10-use-deployer/3-run/run/","title":"Running the Cloud Pak Deployer","text":"<p>Cloud Pak Deployer supports various public and private cloud infrastructures. Click on the links below, or in the left menu to find details about running the deployer on each of the following infrastructures:</p> <ul> <li>Existing OpenShift</li> <li>Existing OpenShift using console</li> <li>Existing OpenShift using wizard</li> <li>IBM Cloud</li> <li>AWS - ROSA</li> <li>AWS - Self-managed</li> <li>Azure - ARO</li> <li>Azure - Self-managed</li> <li>Fusion HCI</li> <li>vSphere</li> </ul>"},{"location":"10-use-deployer/3-run/vsphere/","title":"Running the Cloud Pak Deployer on vSphere","text":"<p>You can use Cloud Pak Deployer to create an OpenShift cluster on VMWare infrastructure.</p> <p>There are 5 main steps to run the deployer for vSphere:</p> <ol> <li>Configure deployer</li> <li>Prepare the cloud environment</li> <li>Obtain entitlement keys and secrets</li> <li>Set environment variables and secrets</li> <li>Run the deployer</li> </ol>"},{"location":"10-use-deployer/3-run/vsphere/#topology","title":"Topology","text":"<p>A typical setup of the vSphere cluster with OpenShift is pictured below: </p> <p>When deploying OpenShift and the Cloud Pak(s) on VMWare vSphere, there is a dependency on a DHCP server for issuing IP addresses to the newly configured cluster nodes. Also, once the OpenShift cluster has been installed, valid fully qualified host names are required to connect to the OpenShift API server at port <code>6443</code> and applications running behind the ingress server at port <code>443</code>. The Cloud Pak deployer cannot set up a DHCP server or a DNS server and to be able to connect to OpenShift or to reach the Cloud Pak after installation, name entries must be set up.</p>"},{"location":"10-use-deployer/3-run/vsphere/#1-configure-deployer","title":"1. Configure deployer","text":""},{"location":"10-use-deployer/3-run/vsphere/#deployer-configuration-and-status-directories","title":"Deployer configuration and status directories","text":"<p>Deployer reads the configuration from a directory you set in the <code>CONFIG_DIR</code> environment variable. A status directory (<code>STATUS_DIR</code> environment variable) is used to log activities, store temporary files, scripts. If you use a File Vault (default), the secrets are kept in the <code>$STATUS_DIR/vault</code> directory.</p> <p>You can find OpenShift and Cloud Pak sample configuration (yaml) files here: sample configuration. For vSphere installations, copy one of <code>ocp-vsphere-*.yaml</code> files into the <code>$CONFIG_DIR/config</code> directory. If you also want to install a Cloud Pak, copy one of the <code>cp4*.yaml</code> files.</p> <p>Example: <pre><code>mkdir -p $HOME/cpd-config/config\ncp sample-configurations/sample-dynamic/config-samples/ocp-vsphere-odf-nfs.yaml $HOME/cpd-config/config/\ncp sample-configurations/sample-dynamic/config-samples/cp4d-471.yaml $HOME/cpd-config/config/\n</code></pre></p>"},{"location":"10-use-deployer/3-run/vsphere/#set-configuration-and-status-directories-environment-variables","title":"Set configuration and status directories environment variables","text":"<p>Cloud Pak Deployer uses the status directory to log its activities and also to keep track of its running state. For a given environment you're provisioning or destroying, you should always specify the same status directory to avoid contention between different deploy runs. </p> <pre><code>export CONFIG_DIR=$HOME/cpd-config\nexport STATUS_DIR=$HOME/cpd-status\n</code></pre> <ul> <li><code>CONFIG_DIR</code>: Directory that holds the configuration, it must have a <code>config</code> subdirectory which contains the configuration <code>yaml</code> files.</li> <li><code>STATUS_DIR</code>: The directory where the Cloud Pak Deployer keeps all status information and logs files.</li> </ul>"},{"location":"10-use-deployer/3-run/vsphere/#optional-advanced-configuration","title":"Optional: advanced configuration","text":"<p>If the deployer configuration is kept on GitHub, follow the instructions in GitHub configuration.</p> <p>For special configuration with defaults and dynamic variables, refer to Advanced configuration.</p>"},{"location":"10-use-deployer/3-run/vsphere/#2-prepare-the-cloud-environment","title":"2. Prepare the cloud environment","text":""},{"location":"10-use-deployer/3-run/vsphere/#pre-requisites-for-vsphere","title":"Pre-requisites for vSphere","text":"<p>In order to successfully install OpenShift on vSphere infrastructure, the following pre-requisites must have been met.</p> Pre-requisite Description Red Hat pull secret A pull secret is required to download and install OpenShift. See Acquire pull secret IBM Entitlement key When instaling an IBM Cloud Pak, you need an IBM entitlement key. See Acquire IBM Cloud Pak entitlement key vSphere credentials The OpenShift IPI installer requires vSphere credentials to create VMs and storage Firewall rules The OpenShift cluster's API server on port 6443 and application server on port 443 must be reachable. Whitelisted URLs The OpenShift and Cloud Pak download locations and registry must be accessible from the vSphere infrastructure. See Whitelisted locations DHCP When provisioning new VMs, IP addresses must be automatically assigned through DHCP DNS A DNS server that will resolve the OpenShift API server and applications is required. See DNS configuration Time server A time server to synchronize the time must be available in the network and configured through the DHCP server <p>There are also some optional settings, dependent on the specifics of the installation:</p> Pre-requisite Description Bastion server It can be useful to have a bastion/installation server to run the deployer. This (virtual) server must reside within the vSphere network NFS details If an NFS server is used for storage, it must be reacheable (firewall) and <code>no_root_squash</code> must be set Private registry If the installation must use a private registry for the Cloud Pak installation, it must be available and credentials shared Certificates If the Cloud Pak URL must have a CA-signed certificate, the key, certificate and CA bundle must be available at instlalation time Load balancer The OpenShift IPI install creates 2 VIPs and takes care of the routing to the services. In some implementations, a load balancer provided by the infrastructure team is preferred. This load balancer must be configured externally"},{"location":"10-use-deployer/3-run/vsphere/#dns-configuration","title":"DNS configuration","text":"<p>During the provisioning and configuration process, the deployer needs access to the OpenShift API and the ingress server for which the IP addresses are specified in the <code>openshift</code> object.</p> <p>Ensure that the DNS server has the following entries:</p> <ul> <li><code>api.openshift_name.domain_name</code> \u2192 Point to the <code>api_vip</code> address configured in the <code>openshift</code> object</li> <li><code>*.apps.openshift_name.domain_name</code> \u2192 Point to the <code>ingress_vip</code> address configured in the <code>openshift</code> object</li> </ul> <p>If you do not configure the DNS entries upfront, the deployer will still run and it will \"spoof\" the required entries in the container's <code>/etc/hosts</code> file. However to be able to connect to OpenShift and access the Cloud Pak, the DNS entries are required.</p>"},{"location":"10-use-deployer/3-run/vsphere/#obtain-the-vsphere-user-and-password","title":"Obtain the vSphere user and password","text":"<p>In order for the Cloud Pak Deployer to create the infrastructure and deploy the IBM Cloud Pak, it must have provisioning access to vSphere and it needs the vSphere user and password. The user must have permissions to create virtual machines.</p>"},{"location":"10-use-deployer/3-run/vsphere/#set-environment-variables-for-vsphere","title":"Set environment variables for vSphere","text":"<pre><code>export VSPHERE_USER=your_vsphere_user\nexport VSPHERE_PASSWORD=password_of_the_vsphere_user\n</code></pre> <ul> <li><code>VSPHERE_USER</code>: This is the user name of the vSphere user, often this is something like <code>admin@vsphere.local</code></li> <li><code>VSPHERE_PASSWORD</code>: The password of the vSphere user. Be careful with special characters like <code>$</code>, <code>!</code> as they are not accepted by the IPI provisioning of OpenShift</li> </ul>"},{"location":"10-use-deployer/3-run/vsphere/#3-acquire-entitlement-keys-and-secrets","title":"3. Acquire entitlement keys and secrets","text":""},{"location":"10-use-deployer/3-run/vsphere/#acquire-ibm-cloud-pak-entitlement-key","title":"Acquire IBM Cloud Pak entitlement key","text":"<p>If you want to pull the Cloud Pak images from the entitled registry (i.e. an online install), or if you want to mirror the images to your private registry, you need to download the entitlement key. You can skip this step if you're installing from a private registry and all Cloud Pak images have already been downloaded to the private registry.</p> <ul> <li>Navigate to https://myibm.ibm.com/products-services/containerlibrary and login with your IBMId credentials</li> <li>Select Get Entitlement Key and create a new key (or copy your existing key)</li> <li>Copy the key value</li> </ul> <p>Warning</p> <p>As stated for the API key, you can choose to download the entitlement key to a file. However, when we reference the entitlement key, we mean the 80+ character string that is displayed, not the file.</p>"},{"location":"10-use-deployer/3-run/vsphere/#acquire-an-openshift-pull-secret","title":"Acquire an OpenShift pull secret","text":"<p>To install OpenShift you need an OpenShift pull secret which holds your entitlement.</p> <ul> <li>Navigate to https://console.redhat.com/openshift/install/pull-secret and download the pull secret into file <code>/tmp/ocp_pullsecret.json</code></li> </ul>"},{"location":"10-use-deployer/3-run/vsphere/#optional-locate-or-generate-a-public-ssh-key","title":"Optional: Locate or generate a public SSH Key","text":"<p>To obtain access to the OpenShift nodes post-installation, you will need to specify the public SSH key of your server; typically this is <code>~/.ssh/id_rsa.pub</code>, where <code>~</code> is the home directory of your user. If you don't have an SSH key-pair yet, you can generate one using the steps documented here: https://cloud.ibm.com/docs/ssh-keys?topic=ssh-keys-generating-and-using-ssh-keys-for-remote-host-authentication#generating-ssh-keys-on-linux. Alternatively, deployer can generate SSH key-pair automatically if credential <code>ocp-ssh-pub-key</code> is not in the vault.</p>"},{"location":"10-use-deployer/3-run/vsphere/#4-set-environment-variables-and-secrets","title":"4. Set environment variables and secrets","text":""},{"location":"10-use-deployer/3-run/vsphere/#set-the-cloud-pak-entitlement-key","title":"Set the Cloud Pak entitlement key","text":"<p>If you want the Cloud Pak images to be pulled from the entitled registry, set the Cloud Pak entitlement key.</p> <pre><code>export CP_ENTITLEMENT_KEY=your_cp_entitlement_key\n</code></pre> <ul> <li><code>CP_ENTITLEMENT_KEY</code>: This is the entitlement key you acquired as per the instructions above, this is a 80+ character string. You don't need to set this environment variable when you install the Cloud Pak(s) from a private registry</li> </ul>"},{"location":"10-use-deployer/3-run/vsphere/#create-the-secrets-needed-for-vsphere-deployment","title":"Create the secrets needed for vSphere deployment","text":"<p>You need to store the OpenShift pull secret in the vault so that the deployer has access to it.</p> <pre><code>cp-deploy.sh vault set \\\n    --vault-secret ocp-pullsecret \\\n    --vault-secret-file /tmp/ocp_pullsecret.json\n</code></pre>"},{"location":"10-use-deployer/3-run/vsphere/#optional-create-secret-for-public-ssh-key","title":"Optional: Create secret for public SSH key","text":"<p>If you want to use your SSH key to access nodes in the cluster, set the Vault secret with the public SSH key. <pre><code>cp-deploy.sh vault set \\\n    --vault-secret ocp-ssh-pub-key \\\n    --vault-secret-file ~/.ssh/id_rsa.pub\n</code></pre></p>"},{"location":"10-use-deployer/3-run/vsphere/#optional-set-the-github-personal-access-token-pat","title":"Optional: Set the GitHub Personal Access Token (PAT)","text":"<p>In some cases, download of the <code>cloudctl</code> and <code>cpd-cli</code> clients from @IBM will fail because GitHub limits the number of API calls from non-authenticated clients. You can remediate this issue by creating a Personal Access Token on github.com and creating a secret in the vault.</p> <pre><code>cp-deploy.sh vault set -vs github-ibm-pat=&lt;your PAT&gt;\n</code></pre> <p>Alternatively, you can set the secret by adding <code>-vs github-ibm-pat=&lt;your PAT&gt;</code> to the <code>cp-deploy.sh env apply</code> command.</p>"},{"location":"10-use-deployer/3-run/vsphere/#5-run-the-deployer","title":"5. Run the deployer","text":""},{"location":"10-use-deployer/3-run/vsphere/#set-path-and-alias-for-the-deployer","title":"Set path and alias for the deployer","text":"<pre><code>source ./set-env.sh\n</code></pre>"},{"location":"10-use-deployer/3-run/vsphere/#optional-validate-the-configuration","title":"Optional: validate the configuration","text":"<p>If you only want to validate the configuration, you can run the dpeloyer with the <code>--check-only</code> argument. This will run the first stage to validate variables and vault secrets and then execute the generators.</p> <pre><code>cp-deploy.sh env apply --check-only --accept-all-licenses\n</code></pre>"},{"location":"10-use-deployer/3-run/vsphere/#run-the-cloud-pak-deployer","title":"Run the Cloud Pak Deployer","text":"<p>To run the container using a local configuration input directory and a data directory where temporary and state is kept, use the example below. If you don't specify the status directory, the deployer will automatically create a temporary directory. Please note that the status directory will also hold secrets if you have configured a flat file vault. If you lose the directory, you will not be able to make changes to the configuration and adjust the deployment. It is best to specify a permanent directory that you can reuse later. If you specify an existing directory the current user must be the owner of the directory. Failing to do so may cause the container to fail with insufficient permissions.</p> <pre><code>cp-deploy.sh env apply --accept-all-licenses\n</code></pre> <p>You can also specify extra variables such as <code>env_id</code> to override the names of the objects referenced in the <code>.yaml</code> configuration files as <code>{{ env_id }}-xxxx</code>. For more information about the extra (dynamic) variables, see advanced configuration.</p> <p>The <code>--accept-all-licenses</code> flag is optional and confirms that you accept all licenses of the installed cartridges and instances. Licenses must be either accepted in the configuration files or at the command line.</p> <p>When running the command, the container will start as a daemon and the command will tail-follow the logs. You can press Ctrl-C at any time to interrupt the logging but the container will continue to run in the background.</p> <p>You can return to view the logs as follows:</p> <pre><code>cp-deploy.sh env logs\n</code></pre> <p>Deploying the infrastructure, preparing OpenShift and installing the Cloud Pak will take a long time, typically between 1-5 hours,dependent on which Cloud Pak cartridges you configured. For estimated duration of the steps, refer to Timings.</p> <p>If you need to interrupt the automation, use CTRL-C to stop the logging output and then use:</p> <pre><code>cp-deploy.sh env kill\n</code></pre>"},{"location":"10-use-deployer/3-run/vsphere/#on-failure","title":"On failure","text":"<p>If the Cloud Pak Deployer fails, for example because certain infrastructure components are temporarily not available, fix the cause if needed and then just re-run it with the same <code>CONFIG_DIR</code> and <code>STATUS_DIR</code> as well extra variables. The provisioning process has been designed to be idempotent and it will not redo actions that have already completed successfully.</p>"},{"location":"10-use-deployer/3-run/vsphere/#finishing-up","title":"Finishing up","text":"<p>Once the process has finished, it will output the URLs by which you can access the deployed Cloud Pak. You can also find this information under the <code>cloud-paks</code> directory in the status directory you specified.</p> <p>To retrieve the Cloud Pak URL(s):</p> <pre><code>cat $STATUS_DIR/cloud-paks/*\n</code></pre> <p>This will show the Cloud Pak URLs:</p> <pre><code>Cloud Pak for Data URL for cluster pluto-01 and project cpd (domain name specified was example.com):\nhttps://cpd-cpd.apps.pluto-01.example.com\n</code></pre> <p>The <code>admin</code> password can be retrieved from the vault as follows:</p> <p>List the secrets in the vault:</p> <pre><code>cp-deploy.sh vault list\n</code></pre> <p>This will show something similar to the following:</p> <pre><code>Secret list for group sample:\n- vsphere-user\n- vsphere-password\n- ocp-pullsecret\n- ocp-ssh-pub-key\n- ibm_cp_entitlement_key\n- sample-kubeadmin-password\n- cp4d_admin_cpd_demo\n</code></pre> <p>You can then retrieve the Cloud Pak for Data admin password like this:</p> <pre><code>cp-deploy.sh vault get --vault-secret cp4d_admin_cpd_demo\n</code></pre> <pre><code>PLAY [Secrets] *****************************************************************\nincluded: /cloud-pak-deployer/automation-roles/99-generic/vault/vault-get-secret/tasks/get-secret-file.yml for localhost\ncp4d_admin_zen_sample_sample: gelGKrcgaLatBsnAdMEbmLwGr\n</code></pre>"},{"location":"10-use-deployer/3-run/vsphere/#post-install-configuration","title":"Post-install configuration","text":"<p>You can find examples of a couple of typical changes you may want to do here: Post-run changes.</p>"},{"location":"10-use-deployer/5-post-run/aws-self-managed-add-gpu/","title":"Adding GPU nodes to self-managed OpenShift on AWS","text":"<p>When deploying self-managed OpenShift on AWS, the compute nodes are represented as one or more OpenShift <code>MachineSet</code>s. If your cluster is deployed in a single zone, there will be 1 <code>MachineSet</code> which defines the number and type of ec2 instances created in the AWS account. For multi-zone clusters, there will be 3 <code>MachineSet</code>s.</p>"},{"location":"10-use-deployer/5-post-run/aws-self-managed-add-gpu/#find-the-compute-node-machineset","title":"Find the compute node <code>MachineSet</code>","text":"<p>Below is an example of a compute node (worker) <code>MachineSet</code> created by the OpenShift installer. The instance type defines the node that is spun up, with 32 vCPUs and 128 GB of memory. <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\n  annotations:\n    capacity.cluster-autoscaler.kubernetes.io/labels: kubernetes.io/arch=amd64\n    machine.openshift.io/GPU: '0'\n    machine.openshift.io/memoryMb: '131072'\n    machine.openshift.io/vCPU: '32'\n  resourceVersion: '22985'\n  name: fk-aws-sts-2th7t-worker-us-east-1a\n  uid: be5a9880-eaa0-4054-a77f-e5c4432eb51f\n  creationTimestamp: '2024-10-16T20:30:43Z'\n  generation: 1\n  managedFields:\n    - apiVersion: machine.openshift.io/v1beta1\n      fieldsType: FieldsV1\n      fieldsV1:\n        'f:metadata':\n          'f:labels':\n            .: {}\n            'f:machine.openshift.io/cluster-api-cluster': {}\n        'f:spec':\n          .: {}\n          'f:replicas': {}\n          'f:selector': {}\n          'f:template':\n            .: {}\n            'f:metadata':\n              .: {}\n              'f:labels':\n                .: {}\n                'f:machine.openshift.io/cluster-api-cluster': {}\n                'f:machine.openshift.io/cluster-api-machine-role': {}\n                'f:machine.openshift.io/cluster-api-machine-type': {}\n                'f:machine.openshift.io/cluster-api-machineset': {}\n            'f:spec':\n              .: {}\n              'f:lifecycleHooks': {}\n              'f:metadata': {}\n              'f:providerSpec':\n                .: {}\n                'f:value':\n                  'f:instanceType': {}\n                  'f:metadata':\n                    .: {}\n                    'f:creationTimestamp': {}\n                  'f:blockDevices': {}\n                  'f:kind': {}\n                  'f:securityGroups': {}\n                  'f:deviceIndex': {}\n                  'f:ami':\n                    .: {}\n                    'f:id': {}\n                  'f:metadataServiceOptions': {}\n                  'f:tags': {}\n                  .: {}\n                  'f:placement':\n                    .: {}\n                    'f:availabilityZone': {}\n                    'f:region': {}\n                  'f:subnet':\n                    .: {}\n                    'f:filters': {}\n                  'f:apiVersion': {}\n                  'f:iamInstanceProfile':\n                    .: {}\n                    'f:id': {}\n                  'f:credentialsSecret':\n                    .: {}\n                    'f:name': {}\n                  'f:userDataSecret':\n                    .: {}\n                    'f:name': {}\n      manager: cluster-bootstrap\n      operation: Update\n      time: '2024-10-16T20:30:43Z'\n    - apiVersion: machine.openshift.io/v1beta1\n      fieldsType: FieldsV1\n      fieldsV1:\n        'f:status': {}\n      manager: cluster-bootstrap\n      operation: Update\n      subresource: status\n      time: '2024-10-16T20:30:43Z'\n    - apiVersion: machine.openshift.io/v1beta1\n      fieldsType: FieldsV1\n      fieldsV1:\n        'f:metadata':\n          'f:annotations':\n            .: {}\n            'f:capacity.cluster-autoscaler.kubernetes.io/labels': {}\n            'f:machine.openshift.io/GPU': {}\n            'f:machine.openshift.io/memoryMb': {}\n            'f:machine.openshift.io/vCPU': {}\n      manager: machine-controller-manager\n      operation: Update\n      time: '2024-10-16T20:35:26Z'\n    - apiVersion: machine.openshift.io/v1beta1\n      fieldsType: FieldsV1\n      fieldsV1:\n        'f:status':\n          'f:availableReplicas': {}\n          'f:fullyLabeledReplicas': {}\n          'f:observedGeneration': {}\n          'f:readyReplicas': {}\n          'f:replicas': {}\n      manager: machineset-controller\n      operation: Update\n      subresource: status\n      time: '2024-10-16T20:41:50Z'\n  namespace: openshift-machine-api\n  labels:\n    machine.openshift.io/cluster-api-cluster: fk-aws-sts-2th7t\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      machine.openshift.io/cluster-api-cluster: fk-aws-sts-2th7t\n      machine.openshift.io/cluster-api-machineset: fk-aws-sts-2th7t-worker-us-east-1a\n  template:\n    metadata:\n      labels:\n        machine.openshift.io/cluster-api-cluster: fk-aws-sts-2th7t\n        machine.openshift.io/cluster-api-machine-role: worker\n        machine.openshift.io/cluster-api-machine-type: worker\n        machine.openshift.io/cluster-api-machineset: fk-aws-sts-2th7t-worker-us-east-1a\n    spec:\n      lifecycleHooks: {}\n      metadata: {}\n      providerSpec:\n        value:\n          userDataSecret:\n            name: worker-user-data\n          placement:\n            availabilityZone: us-east-1a\n            region: us-east-1\n          credentialsSecret:\n            name: aws-cloud-credentials\n          instanceType: m5.8xlarge\n          metadata:\n            creationTimestamp: null\n          blockDevices:\n            - ebs:\n                encrypted: true\n                iops: 0\n                kmsKey:\n                  arn: ''\n                volumeSize: 120\n                volumeType: gp3\n          securityGroups:\n            - filters:\n                - name: 'tag:Name'\n                  values:\n                    - fk-aws-sts-2th7t-worker-sg\n          kind: AWSMachineProviderConfig\n          metadataServiceOptions: {}\n          tags:\n            - name: kubernetes.io/cluster/fk-aws-sts-2th7t\n              value: owned\n          deviceIndex: 0\n          ami:\n            id: ami-0d653d86d4113326a\n          subnet:\n            filters:\n              - name: 'tag:Name'\n                values:\n                  - fk-aws-sts-2th7t-private-us-east-1a\n          apiVersion: machine.openshift.io/v1beta1\n          iamInstanceProfile:\n            id: fk-aws-sts-2th7t-worker-profile\nstatus:\n  availableReplicas: 3\n  fullyLabeledReplicas: 3\n  observedGeneration: 1\n  readyReplicas: 3\n  replicas: 3\n</code></pre></p>"},{"location":"10-use-deployer/5-post-run/aws-self-managed-add-gpu/#transform-to-gpu-machineset","title":"Transform to GPU <code>MachineSet</code>","text":"<p>To create a GPU <code>MachineSet</code> in the same region, copy the yaml into your favourite text editor and remove all the unnecessary properties. Below is an example of the resulting yaml, highlighting the items that have changed to define the GPU node(s). In this example there is only 1 GPU node of type <code>g6e.8xlarge</code>. Only the highlighted properties should be changed.</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\n  name: fk-aws-sts-2th7t-gpu-us-east-1a\n  namespace: openshift-machine-api\n  labels:\n    machine.openshift.io/cluster-api-cluster: fk-aws-sts-2th7t\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      machine.openshift.io/cluster-api-cluster: fk-aws-sts-2th7t\n      machine.openshift.io/cluster-api-machineset: fk-aws-sts-2th7t-gpu-us-east-1a\n  template:\n    metadata:\n      labels:\n        machine.openshift.io/cluster-api-cluster: fk-aws-sts-2th7t\n        machine.openshift.io/cluster-api-machine-role: worker\n        machine.openshift.io/cluster-api-machine-type: worker\n        machine.openshift.io/cluster-api-machineset: fk-aws-sts-2th7t-gpu-us-east-1a\n    spec:\n      providerSpec:\n        value:\n          userDataSecret:\n            name: worker-user-data\n          placement:\n            availabilityZone: us-east-1a\n            region: us-east-1\n          credentialsSecret:\n            name: aws-cloud-credentials\n          instanceType: g6e.8xlarge\n          metadata:\n            creationTimestamp: null\n          blockDevices:\n            - ebs:\n                encrypted: true\n                iops: 0\n                kmsKey:\n                  arn: ''\n                volumeSize: 120\n                volumeType: gp3\n          securityGroups:\n            - filters:\n                - name: 'tag:Name'\n                  values:\n                    - fk-aws-sts-2th7t-worker-sg\n          kind: AWSMachineProviderConfig\n          metadataServiceOptions: {}\n          tags:\n            - name: kubernetes.io/cluster/fk-aws-sts-2th7t\n              value: owned\n          deviceIndex: 0\n          ami:\n            id: ami-0d653d86d4113326a\n          subnet:\n            filters:\n              - name: 'tag:Name'\n                values:\n                  - fk-aws-sts-2th7t-private-us-east-1a\n          apiVersion: machine.openshift.io/v1beta1\n          iamInstanceProfile:\n            id: fk-aws-sts-2th7t-worker-profile\n</code></pre>"},{"location":"10-use-deployer/5-post-run/aws-self-managed-add-gpu/#create-the-gpu-machineset","title":"Create the GPU <code>MachineSet</code>","text":"<p>Once ready, do the following to create the <code>MachineSet</code>:</p> <ul> <li>Go to the OpenShift console</li> <li>Click the \"+\" sign at the top of the page</li> <li>Paste the updated yaml into the the window</li> </ul> <p>The <code>MachineSet</code> will create <code>Machine</code> CRs in the <code>openshift-machines</code> project. If the instance type is available in the selected AWS region and there is enough capacity, the AWS instance(s) will be created and after a 5-10 minutes, it/they will appear as nodes in the OpenShift cluster.</p>"},{"location":"10-use-deployer/5-post-run/azure-self-managed-add-gpu/","title":"Adding GPU nodes to self-managed OpenShift on Azure","text":"<p>When deploying self-managed OpenShift on Azure, the compute nodes are represented as one or more OpenShift <code>MachineSet</code>s. If your cluster is deployed in a single zone, there will be 1 <code>MachineSet</code> which defines the number and type of virtual machiness created in the Azure resource group. For multi-zone clusters, there will be 3 <code>MachineSet</code>s.</p>"},{"location":"10-use-deployer/5-post-run/azure-self-managed-add-gpu/#find-the-compute-node-machineset","title":"Find the compute node <code>MachineSet</code>","text":"<p>Below is an example of a compute node (worker) <code>MachineSet</code> created by the OpenShift installer. The instance type defines the node that is spun up, with 32 vCPUs and 128 GB of memory. <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\n  annotations:\n    capacity.cluster-autoscaler.kubernetes.io/labels: kubernetes.io/arch=amd64\n    machine.openshift.io/GPU: '0'\n    machine.openshift.io/memoryMb: '131072'\n    machine.openshift.io/vCPU: '32'\n  resourceVersion: '67642'\n  name: fk-openshift-shtmp-worker-germanywestcentral1\n  uid: 0104469b-71d9-413e-ad52-40a1a2168a84\n  creationTimestamp: '2024-12-06T14:24:56Z'\n  generation: 3\n  managedFields:\n    - apiVersion: machine.openshift.io/v1beta1\n      fieldsType: FieldsV1\n      fieldsV1:\n        'f:spec':\n          'f:replicas': {}\n      manager: Mozilla\n      operation: Update\n      subresource: scale\n    - apiVersion: machine.openshift.io/v1beta1\n      fieldsType: FieldsV1\n      fieldsV1:\n        'f:metadata':\n          'f:labels':\n            .: {}\n            'f:machine.openshift.io/cluster-api-cluster': {}\n            'f:machine.openshift.io/cluster-api-machine-role': {}\n            'f:machine.openshift.io/cluster-api-machine-type': {}\n        'f:spec':\n          .: {}\n          'f:selector': {}\n          'f:template':\n            .: {}\n            'f:metadata':\n              .: {}\n              'f:labels':\n                .: {}\n                'f:machine.openshift.io/cluster-api-cluster': {}\n                'f:machine.openshift.io/cluster-api-machine-role': {}\n                'f:machine.openshift.io/cluster-api-machine-type': {}\n                'f:machine.openshift.io/cluster-api-machineset': {}\n            'f:spec':\n              .: {}\n              'f:lifecycleHooks': {}\n              'f:metadata': {}\n              'f:providerSpec':\n                .: {}\n                'f:value':\n                  'f:zone': {}\n                  'f:metadata':\n                    .: {}\n                    'f:creationTimestamp': {}\n                  'f:publicIP': {}\n                  'f:image':\n                    .: {}\n                    'f:offer': {}\n                    'f:publisher': {}\n                    'f:resourceID': {}\n                    'f:sku': {}\n                    'f:version': {}\n                  'f:acceleratedNetworking': {}\n                  'f:kind': {}\n                  'f:location': {}\n                  'f:managedIdentity': {}\n                  'f:vmSize': {}\n                  .: {}\n                  'f:subnet': {}\n                  'f:apiVersion': {}\n                  'f:securityProfile':\n                    .: {}\n                    'f:settings': {}\n                  'f:osDisk':\n                    .: {}\n                    'f:diskSettings': {}\n                    'f:diskSizeGB': {}\n                    'f:managedDisk':\n                      .: {}\n                      'f:securityProfile':\n                        .: {}\n                        'f:diskEncryptionSet': {}\n                      'f:storageAccountType': {}\n                    'f:osType': {}\n                  'f:networkResourceGroup': {}\n                  'f:diagnostics': {}\n                  'f:credentialsSecret':\n                    .: {}\n                    'f:name': {}\n                    'f:namespace': {}\n                  'f:publicLoadBalancer': {}\n                  'f:userDataSecret':\n                    .: {}\n                    'f:name': {}\n                  'f:vnet': {}\n                  'f:resourceGroup': {}\n      manager: cluster-bootstrap\n      operation: Update\n      time: '2024-12-06T14:24:56Z'\n    - apiVersion: machine.openshift.io/v1beta1\n      fieldsType: FieldsV1\n      fieldsV1:\n        'f:status': {}\n      manager: cluster-bootstrap\n      operation: Update\n      subresource: status\n      time: '2024-12-06T14:24:57Z'\n    - apiVersion: machine.openshift.io/v1beta1\n      fieldsType: FieldsV1\n      fieldsV1:\n        'f:metadata':\n          'f:annotations':\n            .: {}\n            'f:capacity.cluster-autoscaler.kubernetes.io/labels': {}\n            'f:machine.openshift.io/GPU': {}\n            'f:machine.openshift.io/memoryMb': {}\n            'f:machine.openshift.io/vCPU': {}\n      manager: machine-controller-manager\n      operation: Update\n      time: '2024-12-06T14:37:14Z'\n    - apiVersion: machine.openshift.io/v1beta1\n      fieldsType: FieldsV1\n      fieldsV1:\n        'f:status':\n          'f:availableReplicas': {}\n          'f:fullyLabeledReplicas': {}\n          'f:observedGeneration': {}\n          'f:readyReplicas': {}\n          'f:replicas': {}\n      manager: machineset-controller\n      operation: Update\n      subresource: status\n      time: '2024-12-06T15:54:53Z'\n  namespace: openshift-machine-api\n  labels:\n    machine.openshift.io/cluster-api-cluster: fk-openshift-shtmp\n    machine.openshift.io/cluster-api-machine-role: worker\n    machine.openshift.io/cluster-api-machine-type: worker\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      machine.openshift.io/cluster-api-cluster: fk-openshift-shtmp\n      machine.openshift.io/cluster-api-machineset: fk-openshift-shtmp-worker-germanywestcentral1\n  template:\n    metadata:\n      labels:\n        machine.openshift.io/cluster-api-cluster: fk-openshift-shtmp\n        machine.openshift.io/cluster-api-machine-role: worker\n        machine.openshift.io/cluster-api-machine-type: worker\n        machine.openshift.io/cluster-api-machineset: fk-openshift-shtmp-worker-germanywestcentral1\n    spec:\n      lifecycleHooks: {}\n      metadata: {}\n      providerSpec:\n        value:\n          osDisk:\n            diskSettings: {}\n            diskSizeGB: 300\n            managedDisk:\n              securityProfile:\n                diskEncryptionSet: {}\n              storageAccountType: Premium_LRS\n            osType: Linux\n          networkResourceGroup: fk-rg\n          publicLoadBalancer: fk-openshift-shtmp\n          userDataSecret:\n            name: worker-user-data\n          vnet: fk-openshift\n          securityProfile:\n            settings: {}\n          credentialsSecret:\n            name: azure-cloud-credentials\n            namespace: openshift-machine-api\n          diagnostics: {}\n          zone: '1'\n          metadata:\n            creationTimestamp: null\n          publicIP: false\n          resourceGroup: fk-openshift-rg\n          kind: AzureMachineProviderSpec\n          location: germanywestcentral\n          vmSize: Standard_D32s_v3\n          image:\n            offer: ''\n            publisher: ''\n            resourceID: /resourceGroups/fk-openshift-rg/providers/Microsoft.Compute/galleries/gallery_fk_openshift_shtmp/images/fk-openshift-shtmp-gen2/versions/latest\n            sku: ''\n            version: ''\n          acceleratedNetworking: true\n          managedIdentity: fk-openshift-shtmp-identity\n          subnet: control-plane\n          apiVersion: machine.openshift.io/v1beta1\nstatus:\n  availableReplicas: 3\n  fullyLabeledReplicas: 3\n  observedGeneration: 3\n  readyReplicas: 3\n  replicas: 3\n</code></pre></p>"},{"location":"10-use-deployer/5-post-run/azure-self-managed-add-gpu/#transform-to-gpu-machineset","title":"Transform to GPU <code>MachineSet</code>","text":"<p>To create a GPU <code>MachineSet</code> in the same region, copy the yaml into your favourite text editor and remove all the unnecessary properties. Below is an example of the resulting yaml, highlighting the items that have changed to define the GPU node(s). In this example there is only 1 GPU node of type <code>Standard_NC96ads_A100_v4</code>. Only the highlighted properties should be changed.</p> <pre><code>apiVersion: machine.openshift.io/v1beta1\nkind: MachineSet\nmetadata:\n  name: fk-openshift-shtmp-gpu-germanywestcentral1\n  namespace: openshift-machine-api\n  labels:\n    machine.openshift.io/cluster-api-cluster: fk-openshift-shtmp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      machine.openshift.io/cluster-api-cluster: fk-openshift-shtmp\n      machine.openshift.io/cluster-api-machineset: fk-openshift-shtmp-gpu-germanywestcentral1\n  template:\n    metadata:\n      labels:\n        machine.openshift.io/cluster-api-cluster: fk-openshift-shtmp\n        machine.openshift.io/cluster-api-machine-role: worker\n        machine.openshift.io/cluster-api-machine-type: worker\n        machine.openshift.io/cluster-api-machineset: fk-openshift-shtmp-gpu-germanywestcentral1\n    spec:\n      providerSpec:\n        value:\n          osDisk:\n            diskSettings: {}\n            diskSizeGB: 300\n            managedDisk:\n              securityProfile:\n                diskEncryptionSet: {}\n              storageAccountType: Premium_LRS\n            osType: Linux\n          networkResourceGroup: fk-rg\n          publicLoadBalancer: fk-openshift-shtmp\n          userDataSecret:\n            name: worker-user-data\n          vnet: fk-openshift\n          securityProfile:\n            settings: {}\n          credentialsSecret:\n            name: azure-cloud-credentials\n            namespace: openshift-machine-api\n          zone: '1'\n          publicIP: false\n          resourceGroup: fk-openshift-rg\n          kind: AzureMachineProviderSpec\n          location: germanywestcentral\n          vmSize: Standard_NC96ads_A100_v4\n          image:\n            offer: ''\n            publisher: ''\n            resourceID: /resourceGroups/fk-openshift-rg/providers/Microsoft.Compute/galleries/gallery_fk_openshift_shtmp/images/fk-openshift-shtmp-gen2/versions/latest\n            sku: ''\n            version: ''\n          acceleratedNetworking: true\n          managedIdentity: fk-openshift-shtmp-identity\n          subnet: control-plane\n          apiVersion: machine.openshift.io/v1beta1\n</code></pre>"},{"location":"10-use-deployer/5-post-run/azure-self-managed-add-gpu/#create-the-gpu-machineset","title":"Create the GPU <code>MachineSet</code>","text":"<p>Once ready, do the following to create the <code>MachineSet</code>:</p> <ul> <li>Go to the OpenShift console</li> <li>Click the \"+\" sign at the top of the page</li> <li>Paste the updated yaml into the the window</li> </ul> <p>The <code>MachineSet</code> will create <code>Machine</code> CRs in the <code>openshift-machines</code> project. If the instance type is available in the selected Azure region and there is enough capacity, the Azure VM(s) will be created and after a 5-10 minutes, it/they will appear as nodes in the OpenShift cluster.</p>"},{"location":"10-use-deployer/5-post-run/keycloak-add-users/","title":"Adding users to Keycloak to be used in Software Hub (Cloud Pak for Data/watsonx)","text":"<p>When <code>redhat-sso</code> is present in your configuration yaml files, the deployer will install Keycloak and create the defined groups and Software Hub. Below are the steps to add users to Keycloak and associate them with the pre-defined groups.</p> <p>Example configuration: <pre><code>openshift_redhat_sso:\n- openshift_cluster_name: \"{{ env_id }}\"\n  keycloak_name: ibm-keycloak\n  groups:\n  - name: kc-cp4d-admins\n    state: present\n  - name: kc-cp4d-data-engineers\n    state: present\n  - name: kc-cp4d-data-scientists\n    state: present\n  - name: kc-cp4d-monitors\n    state: present\n\nzen_role:\n- name: cp4d-monitoring-role\n  description: Cloud Pak for Data monitoring role\n  state: installed\n  permissions:\n  - monitor_platform\n\nzen_access_control:\n- project: cpd\n  openshift_cluster_name: \"{{ env_id }}\"\n  keycloak_name: ibm-keycloak\n  user_groups:\n  - name: cp4d-admins\n    description: Cloud Pak for Data Administrators\n    roles:\n    - Administrator\n    keycloak_groups:\n    - kc-cp4d-admins\n  - name: cp4d-data-engineers\n    description: Cloud Pak for Data Data Engineers\n    roles:\n    - User\n    keycloak_groups:\n    - kc-cp4d-data-engineers\n  - name: cp4d-data-scientists\n    description: Cloud Pak for Data Data Scientists\n    roles:\n    - User\n    keycloak_groups:\n    - kc-cp4d-data-scientists\n  - name: cp4d-monitors\n    description: Cloud Pak for Data Monitoring\n    roles:\n    - cp4d-monitoring-role\n    keycloak_groups:\n    - kc-cp4d-monitors\n</code></pre></p>"},{"location":"10-use-deployer/5-post-run/keycloak-add-users/#find-the-keycloak-credentials","title":"Find the Keycloak credentials","text":"<p>Once the deployer finishes you will file a message like the below in the log: <pre><code>--- Keycloak instance ibm-keycloak ---\nKeycloak URL: https://keycloak-ibm-keycloak.apps.abcdef123456.ocp.techzone.ibm.com\nKeycloak User: admin\nKeycloak admin password: &lt;admin password&gt;\n</code></pre></p>"},{"location":"10-use-deployer/5-post-run/keycloak-add-users/#login-to-keycloak-using-the-keycloak-url","title":"Login to Keycloak using the Keycloak URL","text":"<ul> <li>Open a browser and past the Keycloak URL</li> <li>Click on Administration Console</li> <li>Enter the <code>admin</code> user and the password</li> </ul>"},{"location":"10-use-deployer/5-post-run/keycloak-add-users/#check-that-the-keycloak-groups-have-been-created","title":"Check that the Keycloak groups have been created","text":"<ul> <li>Click on the Groups link in the left menu</li> <li>Check that the Keycloak groups defined in the <code>openshift_redhat_sso</code> resoucrce are there</li> </ul>"},{"location":"10-use-deployer/5-post-run/keycloak-add-users/#create-an-administrator-user","title":"Create an administrator user","text":"<ul> <li>Click on the Users link in the left menu</li> <li>Click on Add User at the top right</li> <li>Fill out the user's mandatory details</li> <li>Username: As a best practice, type the user's unique e-mail address</li> <li>Email: Same as the user name, or the e-mail address</li> <li>First name: The user's first name</li> <li>Last name: The user's last name</li> <li>Tick the Email verified checkbox to make sure the user does not have to confirm their e-mail address</li> <li>For gropus, start typing the keycloak group name, for example <code>kc</code></li> <li>Select the <code>kc-cp4d-admins</code> user group</li> <li>Save the user</li> </ul>"},{"location":"10-use-deployer/5-post-run/keycloak-add-users/#set-the-password-for-the-adminstrator-user","title":"Set the password for the adminstrator user","text":"<ul> <li>Click the Credentials tab for the user that has just been created</li> <li>Type the new password and repeat it in the Password confirmation propety</li> <li>Uncheck Temporary if you don't want the password to be changed at first login</li> <li>Click the Set password button</li> <li>When the pop-up box appears, click on Set password</li> </ul>"},{"location":"10-use-deployer/5-post-run/keycloak-add-users/#logout-from-keycloak","title":"Logout from Keycloak","text":"<p>Warning</p> <p>It is important that you now logout from Keycloak. If you stay logged in and then try to log in to Software Hub using the <code>ibm-keycloak</code> IdP, login will fail because user <code>admin</code> does not have any permissions in the product.</p>"},{"location":"10-use-deployer/5-post-run/keycloak-add-users/#log-in-to-the-product","title":"Log in to the product","text":"<ul> <li>Open the login page for the product. Like for Keycloak, this can be found in the deployer log</li> <li>Select <code>ibm-keycloak</code> for the login with drop-down</li> <li>Click Continue</li> </ul> <ul> <li>Log in with the user and password you created in Keycloak</li> </ul>"},{"location":"10-use-deployer/5-post-run/keycloak-add-users/#add-additional-users","title":"Add additional users","text":"<p>You can add additional users to Keycloak and select the appropriate group(s).</p>"},{"location":"10-use-deployer/5-post-run/post-run/","title":"Post-run changes","text":"<p>If you want to change the deployed configuration, you can just update the configuration files and re-run the deployer. Make sure that you use the same input configuration and status directories and also the <code>env_id</code> if you specified one, otherwise deployment may fail.</p> <p>Below are a couple of examples of post-run changes you may want to do.</p>"},{"location":"10-use-deployer/5-post-run/post-run/#configure-idp-users","title":"Configure IdP users","text":"<p>When Keycloak (Red Hat SSO) has been configured, you cannot login without first creating a user. Refer to Add users to Keycloak for the steps to configure users.</p>"},{"location":"10-use-deployer/5-post-run/post-run/#change-cloud-pak-for-data-administrator-password","title":"Change Cloud Pak for Data administrator password","text":"<p>When initially installed, the Cloud Pak Deployer will generate a strong password for the Cloud Pak for Data <code>admin</code> user (or <code>cpadmin</code> if you have selected to use Foundational Services IAM). If you want to change the password afterwards, you can do this from the Cloud Pak for Data user interface, but this means that the deployer will no longer be able to make changes to the Cloud Pak for Data configuration.</p> <p>If you have updated the admin password from the UI, please make sure you also update the secret in the vault.</p> <p>First, list the secrets in the vault: <pre><code>cp-deploy.sh vault list\n</code></pre></p> <p>This will show something similar to the following: <pre><code>Secret list for group sample:\n- ibm_cp_entitlement_key\n- sample-provision-ssh-key\n- sample-provision-ssh-pub-key\n- sample-terraform-tfstate\n- cp4d_admin_zen_sample_sample\n</code></pre></p> <p>Then, update the password: <pre><code>cp-deploy.sh vault set -vs cp4d_admin_zen_sample_sample -vsv \"my Really Sec3re Passw0rd\"\n</code></pre></p> <p>Finally, run the deployer again. It will make the necessary changes to the OpenShift secret and check that the <code>admin</code> (or <code>cpadmin</code>) user can log in. In this case you can speed up the process via the <code>--skip-infra</code> flag. <pre><code>cp-deploy.sh env apply --skip-infra [--accept-all-liceneses]\n</code></pre></p>"},{"location":"10-use-deployer/5-post-run/post-run/#add-gpu-nodes-to-the-cluster","title":"Add GPU nodes to the cluster","text":"<p>watsonx.ai requires GPUs to run and tune the foundation models. Deployer currently does not provision these GPU nodes, but you can add them manually from the OpenShift console.</p>"},{"location":"10-use-deployer/5-post-run/post-run/#gpu-nodes-on-aws","title":"GPU nodes on AWS","text":"<p>For adding GPU nodes on AWS infrastructure, refer to Add GPUs to self-managed OpenShift on AWS.</p>"},{"location":"10-use-deployer/5-post-run/post-run/#gpu-nodes-on-azure","title":"GPU nodes on Azure","text":"<p>For adding GPU nodes on Azure infrastructure, refer to Add GPUs to self-managed OpenShift on Azure.</p>"},{"location":"10-use-deployer/7-command/command/","title":"Open a command line within the Cloud Pak Deployer container","text":"<p>Sometimes you may need to access the OpenShift cluster using the OpenShift client. For convenience we have made the <code>oc</code> command available in the Cloud Pak Deployer and you can start exploring the current OpenShift cluster immediately without having to install the client on your own workstation.</p>"},{"location":"10-use-deployer/7-command/command/#prepare-for-the-command-line","title":"Prepare for the command line","text":""},{"location":"10-use-deployer/7-command/command/#set-environment-variables","title":"Set environment variables","text":"<p>Make sure you have set the CONFIG_DIR and STATUS_DIR environment variables to the same values when you ran the <code>env apply</code> command. This will ensure that the <code>oc</code> command will access the OpenShift cluster(s) of that configuration.</p>"},{"location":"10-use-deployer/7-command/command/#set-path-and-alias-for-the-deployer","title":"Set path and alias for the deployer","text":"<pre><code>source ./set-env.sh\n</code></pre>"},{"location":"10-use-deployer/7-command/command/#optional-prepare-openshift-cluster","title":"Optional: prepare OpenShift cluster","text":"<p>If you have not run the deployer yet and do not intend to install any Cloud Paks, but you do want to access the OpenShift cluster from the command line to check or prepare items, run the deployer with the <code>--skip-cp-install</code> flag.</p> <pre><code>cp-deploy.sh env apply --skip-cp-install\n</code></pre> <p>Deployer will check the configuration, download clients, attempt to login to OpenShift and prepare the OpenShift cluster with the global pull secret and (for Cloud Pak for Data) node settings. After that the deployer will finish without installing any Cloud Pak.</p>"},{"location":"10-use-deployer/7-command/command/#run-the-cloud-pak-deployer-command-line","title":"Run the Cloud Pak Deployer command line","text":"<pre><code>cp-deploy.sh env cmd \n</code></pre> <p>You should see something like this: <pre><code>-------------------------------------------------------------------------------\nEntering Cloud Pak Deployer command line in a container.\nUse the \"exit\" command to leave the container and return to the hosting server.\n-------------------------------------------------------------------------------\nInstalling OpenShift client\nCurrent OpenShift context: cpd\n</code></pre></p> <p>Now, you can check the OpenShift cluster version: <pre><code>oc get clusterversion\n</code></pre></p> <pre><code>NAME      VERSION   AVAILABLE   PROGRESSING   SINCE   STATUS\nversion   4.8.14    True        False         2d3h    Cluster version is 4.8.14\n</code></pre> <p>Or, display the list of OpenShift projects: <pre><code>oc get projects | grep -v openshift-\n</code></pre></p> <pre><code>NAME                                               DISPLAY NAME   STATUS\ncalico-system                                                     Active\ndefault                                                           Active\nibm-cert-store                                                    Active\nibm-odf-validation-webhook                                        Active\nibm-system                                                        Active\nkube-node-lease                                                   Active\nkube-public                                                       Active\nkube-system                                                       Active\nopenshift                                                         Active\nservices                                                          Active\ntigera-operator                                                   Active\ncpd                                                            Active\n</code></pre>"},{"location":"10-use-deployer/7-command/command/#exit-the-command-line","title":"Exit the command line","text":"<p>Once finished, exit out of the container. <pre><code>exit\n</code></pre></p>"},{"location":"10-use-deployer/9-destroy/destroy/","title":"Destroy the created resources","text":"<p>If you have previously used the Cloud Pak Deployer to create assets, you can destroy the assets with the same command.</p> <p>Info</p> <p>Currently, destroy is only implemented for OpenShift clusters on IBM Cloud ROKS, AWS and Azure, and for Cloud Pak for Data on an existing OpenShift cluster.</p>"},{"location":"10-use-deployer/9-destroy/destroy/#prepare-for-destroy","title":"Prepare for destroy","text":""},{"location":"10-use-deployer/9-destroy/destroy/#prepare-for-destroy-on-existing-openshift","title":"Prepare for destroy on existing OpenShift","text":""},{"location":"10-use-deployer/9-destroy/destroy/#set-environment-variables-for-existing-openshift","title":"Set environment variables for existing OpenShift","text":"<p>Optional: set environment variables for deployer config and status directories. If not specified, respectively <code>$HOME/cpd-config</code> and <code>$HOME/cpd-status</code> will be used. <pre><code>export STATUS_DIR=$HOME/cpd-status\nexport CONFIG_DIR=$HOME/cpd-config\n</code></pre></p> <ul> <li><code>STATUS_DIR</code>: The directory where the Cloud Pak Deployer keeps all status information and log files. Please note that if you have chosen to use a File Vault, the directory specified must be the one you used when you created the environment</li> <li><code>CONFIG_DIR</code>: Directory that holds the configuration. This must be the same directory you used when you created the environment</li> </ul>"},{"location":"10-use-deployer/9-destroy/destroy/#prepare-for-destroy-on-ibm-cloud","title":"Prepare for destroy on IBM Cloud","text":""},{"location":"10-use-deployer/9-destroy/destroy/#set-environment-variables-for-ibm-cloud","title":"Set environment variables for IBM Cloud","text":"<pre><code>export IBM_CLOUD_API_KEY=your_api_key\n</code></pre> <p>Optional: set environment variables for deployer config and status directories. If not specified, respectively <code>$HOME/cpd-config</code> and <code>$HOME/cpd-status</code> will be used. <pre><code>export STATUS_DIR=$HOME/cpd-status\nexport CONFIG_DIR=$HOME/cpd-config\n</code></pre></p> <ul> <li><code>IBM_CLOUD_API_KEY</code>: This is the API key you generated using your IBM Cloud account, this is a 40+ character string</li> <li><code>STATUS_DIR</code>: The directory where the Cloud Pak Deployer keeps all status information and log files. Please note that if you have chosen to use a File Vault, the directory specified must be the one you used when you created the environment</li> <li><code>CONFIG_DIR</code>: Directory that holds the configuration. This must be the same directory you used when you created the environment</li> </ul>"},{"location":"10-use-deployer/9-destroy/destroy/#prepare-for-destroy-on-aws","title":"Prepare for destroy on AWS","text":""},{"location":"10-use-deployer/9-destroy/destroy/#set-environment-variables-for-aws","title":"Set environment variables for AWS","text":"<p>We assume that the vault already holds the mandatory secrets for AWS Access Key, Secret Access Key and ROSA login token.</p> <pre><code>export STATUS_DIR=$HOME/cpd-status\nexport CONFIG_DIR=$HOME/cpd-config\n</code></pre> <ul> <li><code>STATUS_DIR</code>: The directory where the Cloud Pak Deployer keeps all status information and log files. Please note that if you have chosen to use a File Vault, the directory specified must be the one you used when you created the environment</li> <li><code>CONFIG_DIR</code>: Directory that holds the configuration. This must be the same directory you used when you created the environment</li> </ul>"},{"location":"10-use-deployer/9-destroy/destroy/#prepare-for-destroy-on-azure","title":"Prepare for destroy on Azure","text":""},{"location":"10-use-deployer/9-destroy/destroy/#set-environment-variables-for-azure","title":"Set environment variables for Azure","text":"<p>We assume that the vault already holds the mandatory secrets for Azure - Service principal id and its password, tenant id and ARO login token.</p> <pre><code>export STATUS_DIR=$HOME/cpd-status\nexport CONFIG_DIR=$HOME/cpd-config\n</code></pre> <ul> <li><code>STATUS_DIR</code>: The directory where the Cloud Pak Deployer keeps all status information and log files. Please note that if you have chosen to use a File Vault, the directory specified must be the one you used when you created the environment</li> <li><code>CONFIG_DIR</code>: Directory that holds the configuration. This must be the same directory you used when you created the environment</li> </ul>"},{"location":"10-use-deployer/9-destroy/destroy/#set-path-and-alias-for-the-deployer","title":"Set path and alias for the deployer","text":"<pre><code>source ./set-env.sh\n</code></pre>"},{"location":"10-use-deployer/9-destroy/destroy/#run-the-cloud-pak-deployer-from-the-command-line-to-uninstall-the-software","title":"Run the Cloud Pak Deployer from the command line to uninstall the software","text":"<pre><code>cp-deploy.sh env destroy --confirm-destroy\n</code></pre> <p>Please ensure you specify the same extra (dynamic) variables that you used when you ran the <code>env apply</code> command.</p> <p>When running the command, the container will start as a daemon and the command will tail-follow the logs. You can press Ctrl-C at any time to interrupt the logging but the container will continue to run in the background.</p> <p>You can return to view the logs as follows:</p> <pre><code>cp-deploy.sh env logs\n</code></pre> <p>If you need to interrupt the process, use CTRL-C to stop the logging output and then use:</p> <pre><code>cp-deploy.sh env kill\n</code></pre>"},{"location":"10-use-deployer/9-destroy/destroy/#run-the-cloud-pak-deployer-from-the-openshift-console-to-uninstall-the-software","title":"Run the Cloud Pak Deployer from the OpenShift console to uninstall the software","text":"<p>If you have deployed on an existing OpenShift cluster using the console, you can uninstall the software from the <code>cloud-pak-deployer-debug-xxxxx</code> pod.</p> <ul> <li>In the OpenShift console, open <code>Workloads</code> in the left menu, then select <code>Pods</code></li> <li>From the top, select the <code>cloud-pak-deployer</code> project from the drop-down</li> <li>Find the <code>cloud-pak-deployer-debug-xxxxx</code> pod and open it</li> <li>Select the <code>Terminal</code> tab to open a command line</li> <li>Run <code>cp-deploy.sh env destroy --confirm-destroy</code> from the command line</li> </ul> <p>The above steps will delete Cloud Pak for Data and watsonx from the cluster, based on the configuration that is kept in <code>cloud-pak-deployer</code> configuration map.</p>"},{"location":"10-use-deployer/9-destroy/destroy/#run-a-script-from-the-openshift-console-to-uninstall-the-software","title":"Run a script from the OpenShift console to uninstall the software","text":"<p>If you have deployed on an existing OpenShift cluster using the console, you can uninstall the software from the <code>cloud-pak-deployer-debug-xxxxx</code> pod.</p> <ul> <li>In the OpenShift console, open <code>Workloads</code> in the left menu, then select <code>Pods</code></li> <li>From the top, select the <code>cloud-pak-deployer</code> project from the drop-down</li> <li>Find the <code>cloud-pak-deployer-debug-xxxxx</code> pod and open it</li> <li>Select the <code>Terminal</code> tab to open a command line</li> <li>Run the <code>cd /cloud-pak-deployer/scripts/cp4d</code> command</li> <li>Run the <code>./cp4d-delete-instance.sh &lt;namespace&gt;</code> command</li> </ul> <p>The above steps will delete Cloud Pak for Data and watsonx from the select namespace and will also delete the operators. After that it will also delete the license manager, knative eventing, IBM certificate manager and any obsolete CRDs from the cluster.</p>"},{"location":"30-reference/timings/","title":"Timings for the deployment","text":""},{"location":"30-reference/timings/#duration-of-the-overall-deployment-process","title":"Duration of the overall deployment process","text":"Phase Step Time in minutes Comments 10 - Validation 3 20 - Prepare Generators 3 30 - Provision infrastructure Create VPC 1 Create VSI without storage 5 Create VSI with storage 10 Create VPC ROKS cluster 45 Install ROKS ODF add-on and create storage classes 45 40 - Configure infrastructure Install NFS on VSIs 10 Create NFS storage classes 5 Create private container registry namespace 5 50 - Install Cloud Pak Prepare OpenShift for Cloud Pak for Data install 60 During this step, the compute nodes may be replaced and also the Kubernetes services may be restarted. Mirror Cloud Pak for Data images to private registry (only done when using private registry) 30-600 If the entitled registry is used, this step will be skipped. When using a private registry, if images have already been mirrored, the duration will be much shorter, approximately 10 minutes. Install Cloud Pak for Data control plane 20 Create Cloud Pak for Data subscriptions for cartridges 15 Install cartridges 20-300 The amount of time really depends on the cartridges being installed. In the table below you will find an estimate of the installation time for each cartridge. Cartridges will be installed in parallel through the operators. 60 - Configure Cloud Pak Configure Cloud Pak for Data LDAP 5 Provision instances for cartridges 30-60 For cartridges that have instances defined. Creation of the instances will run in parallel where possible. Configure cartridge and instance permissions based on LDAP config 10 70 - Deploy assets No activities yet 0 80 - Smoke tests Show Cloud Pak for Data cluster details 1"},{"location":"30-reference/timings/#cloud-pak-for-data-cartridge-deployment","title":"Cloud Pak for Data cartridge deployment","text":"Cartridge Full name Installation time Instance provisioning time Dependencies cpd_platform Cloud Pak for Data control plane 20 N/A ccs Common Core Services 75 N/A db2aas Db2 as a Service 30 N/A iis Information Server 60 N/A ccs, db2aas ca Cognos Analytics 20 45 ccs planning-analytics Planning Analytics 15 N/A watson_assistant Watson Assistant 70 N/A watson-discovery Watson Discovery 100 N/A ] watson-ks Watson Knowledge Studio 20 N/A watson-speech Watson Speech to Text and Text to Speech 20 N/A wkc Watson Knowledge Catalog 90 N/A ccs, db2aas, iis wml Watson Machine Learning 45 N/A ccs ws Watson Studio 30 N/A ccs <p>Examples:</p> <ul> <li>Cloud Pak for Data installation with just Cognos Analytics will take 20 (control plane) + 75 (ccs) + 20 (ca) + 45 (ca instance) = ~160 minutes</li> <li>Cloud Pak for Data installation with Cognos Analytics and Watson Studio will take 20 (control plane) + 75 (ccs) + 45 (ws+ca) + 45 (ca instance) = ~185 minutes</li> <li>Cloud Pak for Data installation with just Watson Knowledge Catalog will take 20 (control plane) + 75 (ccs) + 30 (db2aas) + 60 (iis) + 90 (wkc) = ~275 minutes</li> <li>Cloud Pak for Data installation with Watson Knowledge Catalog and Watson Studio will take the same time because WS will finish 30 minutes after installing CCS, while WKC will take a lot longer to complete</li> </ul>"},{"location":"30-reference/configuration/cloud-pak/","title":"Cloud Paks","text":"<p>Defines the Cloud Pak(s) which is/are layed out on the OpenShift cluster, typically in one or more OpenShift projects. The Cloud Pak definition represents the instance users connect to and which is responsible for managing the functional capabilities installed within the application.</p>"},{"location":"30-reference/configuration/cloud-pak/#cloud-pak-configuration","title":"Cloud Pak configuration","text":"<ul> <li>Cloud Pak for Data</li> <li>Cloud Pak for Integration</li> <li>Cloud Pak for Watson AIOps</li> <li>Cloud Pak for Business Automation</li> </ul>"},{"location":"30-reference/configuration/cloud-pak/#cp4d","title":"<code>cp4d</code>","text":"<p>Defines the Cloud Pak for Data instances to be configured on the OpenShift cluster(s).</p> <pre><code>cp4d:\n- project: cpd\n  operators_project: cpd-operators\n  openshift_cluster_name: sample\n  cp4d_version: latest\n  use_fs_iam: False\n  change_node_settings: True\n  db2u_limited_privileges: False\n  ibm_cert_manager: False\n  accept_licenses: False\n  openshift_storage_name: nfs-storage\n  cp4d_entitlement: \n  - cpd-enterprise\n  # - cpd-standard\n  # - cognos-analytics\n  - data-product-hub\n  # - datastage\n  # - ikc-premium\n  # - ikc-standard\n  # - openpages\n  # - planning-analytics\n  # - product-master\n  # - speech-to-text\n  # - text-to-speech\n  - watson-assistant\n  # - watson-discovery\n  # - watsonx-ai\n  # - watsonx-code-assistant-ansible\n  # - watsonx-code-assistant-z\n  # - watsonx-data\n  # - watsonx-gov-mm\n  # - watsonx-gov-rc\n  # - watsonx-orchestrate  \n  cp4d_production_license: True\n  state: installed\n  \n  cartridges:\n  - name: cpfs\n  - name: cpd_platform\n</code></pre>"},{"location":"30-reference/configuration/cloud-pak/#properties","title":"Properties","text":"Property Description Mandatory Allowed values project Name of the OpenShift project of the Cloud Pak for Data instance Yes operators_project Name of the OpenShift project that holds the operators. If not specified, the name will be <code>project</code>-operators No Valid OpenShift project name openshift_cluster_name Name of the OpenShift cluster Yes, inferred from openshift Existing <code>openshift</code> cluster cp4d_version Cloud Pak for Data version to install, this will determine the version for all cartridges. When specifying <code>latest</code>, the version will be determined from the olm-utils image Yes latest, x.y.z sequential_install Deprecated property No True (default), False use_fs_iam If set to <code>True</code> the deployer will enable Foundational Services IAM for authentication No False (default), True use_cp_alt_repo When set to <code>False</code>, deployer will use use the alternative repo specified in <code>cp_alt_repo</code> resource No True (default), False change_node_settings Controls whether the node settings using the machine configs will be applied onto the OpenShift cluster. No True, False db2u_limited_privileges Depicts whether Db2U containers run with limited privileges. If they do (<code>True</code>), Deployer will create KubeletConfig and Tuned OpenShift resources as per the documentation. No False (default), True ibm_cert_manager Specify if the IBM certificate manager must be installed instead of the Red Hat certificate Manager No False (default), True accept_licenses Set to 'True' to accept Cloud Pak licenses. Alternatively the <code>--accept-all-licenses</code> can be used for the <code>cp-deploy.sh</code> command No True, False (default) cp4d_entitlement Set to <code>cpd-enterprise</code>, <code>cpd-standard</code>, <code>watsonx-data</code>, <code>watsonx-ai</code>, <code>watsonx-gov-mm</code>, <code>watsonx-gov-rc</code>, dependent on the deployed license, multiple entitlements can be specified No For valid values, refer to product documentation cp4d_production_license Whether the Cloud Pak for Data is a production license No True (default), False state Indicated whether Cloud Pak for Data must be installed or removed No installed (default), removed image_registry_name When using private registry, specify name of <code>image_registry</code> No openshift_storage_name References an <code>openshift_storage</code> element in the OpenShift cluster that was defined for this Cloud Pak for Data instance. The name must exist under `openshift.[openshift_cluster_name].openshift_storage. No, inferred from openshift-&gt;openshift_storage cartridges List of cartridges to install for this Cloud Pak for Data instance. See Cloud Pak for Data cartridges for more details Yes"},{"location":"30-reference/configuration/cloud-pak/#cp4i","title":"<code>cp4i</code>","text":"<p>Defines the Cloud Pak for Integration installation to be configured on the OpenShift cluster(s).</p> <pre><code>cp4i:\n- project: cp4i\n  openshift_cluster_name: {{ env_id }}\n  openshift_storage_name: nfs-rook-ceph\n  cp4i_version: 2021.4.1\n  accept_licenses: False\n  use_top_level_operator: False\n  top_level_operator_channel: v1.5\n  top_level_operator_case_version: 2.5.0\n  operators_in_all_namespaces: True\n \n  instances:\n  - name: integration-navigator\n    type: platform-navigator\n    license: L-RJON-C7QG3S\n    channel: v5.2\n    case_version: 1.5.0\n</code></pre>"},{"location":"30-reference/configuration/cloud-pak/#openshift-projects","title":"OpenShift projects","text":"<p>The immediate content of the cp4i object is actually a list of OpenShift projects (namespaces). There can be more than one project and instances can be created in separate projects.</p> <pre><code>cp4i:\n- project: cp4i\n  ...\n\n- project: cp4i-ace\n  ...\n\n- project: cp4i-apic\n  ...\n</code></pre>"},{"location":"30-reference/configuration/cloud-pak/#operator-channels-case-versions-license-ids","title":"Operator channels, CASE versions, license IDs","text":"<p>Before you run the Cloud Pak Deployer be sure that the correct operator channels are defined for the selected instance types. Some products require a license ID, please check the documentation of each product for the correct license. If you decide to use CASE files instead of the IBM Operator Catalog (more on that below) make sure that you selected the correct CASE versions - please refer: https://github.com/IBM/cloud-pak/tree/master/repo/case</p>"},{"location":"30-reference/configuration/cloud-pak/#cp4i-main-properties","title":"CP4I main properties","text":"<p>The following properties are defined on the project level:</p> Property Description Mandatory Allowed values project The name of the OpenShift project that will be created and used for the installation of the defined instances. Yes openshift_cluster_name Dynamically defined form the <code>env_id</code> parameter during the execution. Yes, inferred from openshift Existing <code>openshift</code> cluster openshift_storage_name Reference to the storage definition that exists in the <code>openshift</code> object (please see above). The definition must include the class name of the file storage type and the class name of the block storage type. No, inferred from openshift-&gt;openshift_storage cp4i_version The version of the Cloud Pak for Integration (e.g. 2021.4.1) Yes use_case_files The property defines if the CASE files are used for installation. If it is True then the operator catalogs are created from the CASE files. If it is False, the IBM Operator Catalog from the entitled registry is used. No True, False (default) accept_licenses Set to <code>True</code> to accept Cloud Pak licenses. Alternatively the <code>--accept-all-licenses</code> can be used for the <code>cp-deploy.sh</code> command Yes True, False use_top_level_operator If it is <code>True</code> then the CP4I top-level operator that installs all other operators is used. Otherwise, only the operators for the selected instance types are installed. No True, False (default) top_level_operator_channel Needed if the <code>use_top_level_operator</code> is <code>True</code> otherwise, it is ignored. Specifies the channel of the top-level operator. No top_level_operator_case_version Needed if the <code>use_top_level_operator</code> is <code>True</code> otherwise, it is ignored. Specifies the CASE package version of the top-level operator. No operators_in_all_namespaces It defines whether the operators are visible in all namespaces or just in the specific namespace where they are needed. No True, False (default) instances List of the instances that are going to be created (please see below). Yes <p>Warning</p> <p>Despite the properties use_case_files, use_top_level_operator and operators_in_all_namespaces are defined as optional, they are actually crucial for the way of execution of the installation process. If any of them is omitted, it is assumed that the default False value is used. If none of them exists, it means that all are False. In this case, it means that the IBM Operator Catalog is used and only the needed operators for specified instance types are installed in the specific namespace.</p>"},{"location":"30-reference/configuration/cloud-pak/#properties-of-the-individual-instances","title":"Properties of the individual instances","text":"<p>The instance property contains one or more instances definitions. Each instance must have a unique name. There can be more the one instance of the same type.</p>"},{"location":"30-reference/configuration/cloud-pak/#naming-convention-for-instance-types","title":"Naming convention for instance types","text":"<p>For each instance definition, an instance type must be specified. We selected the type names that are as much as possible similar to the naming convention used in the Platform Navigator use interface. The following table shows all existing types:</p> Instance type Description/Product name platform-navigator Platform Navigator api-management IBM API Connect automation-assets Automation assets a.k.a Asset repo enterprise-gateway IBM Data Power event-endpoint-management Event endpoint manager - managing asynchronous APIs event-streams IBM Event Streams - Kafka high-speed-transfer-server Aspera HSTS integration-dashboard IBM App Connect Integration Dashboard integration-design IBM App Connect Designer integration-tracing Operations Dashboard messaging IBM MQ"},{"location":"30-reference/configuration/cloud-pak/#platform-navigator","title":"Platform navigator","text":"<p>The Platform Navigator is defined as one of the instance types. There is typically only one instance of it. The exception would be an installation in two or more completely separate namespaces (see the CP4I documentation). Special attention is paid to the installation of the Navigator. The Cloud Pak Deployer will install the Navigator instance first, before any other instance, and it will wait until the instance is ready (this could take up to 45 minutes).  </p> <p>When the installation is completed, you will find the admin user password in the status/cloud-paks/cp4i--cp4i-PN-access.txt file. Of course, you can obtain the password also from the platform-auth-idp-credentials secret in ibm-common-services namespace. Property Description Sample value for 2021.4.1 name Unique name within the cluster using only lowercase alphanumerics and \"-\" type It must be platform-navigator license License ID L-RJON-C7QG3S channel Subscription channel v5.2 case_version CASE version 1.5.0"},{"location":"30-reference/configuration/cloud-pak/#api-management-ibm-api-connect","title":"API management (IBM API Connect)","text":"Property Description Sample value for 2021.4.1 name Unique name within the cluster using only lowercase alphanumerics and \"-\" type It must be api-management license License ID L-RJON-C7BJ42 version Version of API Connect 10.0.4.0 channel Subscription channel v2.4 case_version CASE version 3.0.5"},{"location":"30-reference/configuration/cloud-pak/#automation-assets-asset-repo","title":"Automation assets (Asset repo)","text":"Property Description Sample value for 2021.4.1 name Unique name within the cluster using only lowercase alphanumerics and \"-\" type It must be automation-assets license License ID L-PNAA-C68928 version Version of Asset repo 2021.4.1-2 channel Subscription channel v1.4 case_version CASE version 1.4.2"},{"location":"30-reference/configuration/cloud-pak/#enterprise-gateway-ibm-data-power","title":"Enterprise gateway (IBM Data Power)","text":"Property Description Sample value for 2021.4.1 name Unique name within the cluster using only lowercase alphanumerics and \"-\" type It must be enterprise-gateway admin_password_secret The name of the secret where admin password is stored. The default name is used if you leave it empty. license License ID L-RJON-BYDR3Q version Version of Data Power 10.0-cd channel Subscription channel v1.5 case_version CASE version 1.5.0"},{"location":"30-reference/configuration/cloud-pak/#event-endpoint-management","title":"Event endpoint management","text":"Property Description Sample value for 2021.4.1 name Unique name within the cluster using only lowercase alphanumerics and \"-\" type It must be event-endpoint-management license License ID L-RJON-C7BJ42 version Version of Event endpoint manager 10.0.4.0 channel Subscription channel v2.4 case_version CASE version 3.0.5"},{"location":"30-reference/configuration/cloud-pak/#event-streams","title":"Event streams","text":"Property Description Sample value for 2021.4.1 name Unique name within the cluster using only lowercase alphanumerics and \"-\" type It must be event-streams version Version of Event streams 10.5.0 channel Subscription channel v2.5 case_version CASE version 1.5.2"},{"location":"30-reference/configuration/cloud-pak/#high-speed-transfer-server-aspera-hsts","title":"High speed transfer server (Aspera HSTS)","text":"Property Description Sample value for 2021.4.1 name Unique name within the cluster using only lowercase alphanumerics and \"-\" type It must be high-speed-transfer-server aspera_key A license key for the Aspera software redis_version Version of the Redis database 5.0.9 version Version of Aspera HSTS 4.0.0 channel Subscription channel v1.4 case_version CASE version 1.4.0"},{"location":"30-reference/configuration/cloud-pak/#integration-dashboard-ibm-app-connect-dashboard","title":"Integration dashboard (IBM App Connect Dashboard)","text":"Property Description Sample value for 2021.4.1 name Unique name within the cluster using only lowercase alphanumerics and \"-\" type It must be integration-dashboard license License ID L-APEH-C79J9U version Version of IBM App Connect 12.0 channel Subscription channel v3.1 case_version CASE version 3.1.0"},{"location":"30-reference/configuration/cloud-pak/#integration-design-ibm-app-connect-designer","title":"Integration design (IBM App Connect Designer)","text":"Property Description Sample value for 2021.4.1 name Unique name within the cluster using only lowercase alphanumerics and \"-\" type It must be integration-design license License ID L-KSBM-C87FU2 version Version of IBM App Connect 12.0 channel Subscription channel v3.1 case_version CASE version 3.1.0"},{"location":"30-reference/configuration/cloud-pak/#integration-tracing-operation-dashborad","title":"Integration tracing (Operation dashborad)","text":"Property Description Sample value for 2021.4.1 name Unique name within the cluster using only lowercase alphanumerics and \"-\" type It must be integration-tracing version Version of Integration tracing 2021.4.1-2 channel Subscription channel v2.5 case_version CASE version 2.5.2"},{"location":"30-reference/configuration/cloud-pak/#messaging-ibm-mq","title":"Messaging (IBM MQ)","text":"Property Description Sample value for 2021.4.1 name Unique name within the cluster using only lowercase alphanumerics and \"-\" type It must be messaging queue_manager_name The name of the initial queue. Default is QUICKSTART license License ID L-RJON-C7QG3S version Version of IBM MQ 9.2.4.0-r1 channel Subscription channel v1.7 case_version CASE version 1.7.0"},{"location":"30-reference/configuration/cloud-pak/#cp4waiops","title":"<code>cp4waiops</code>","text":"<p>Defines the Cloud Pak for Watson AIOps installation to be configured on the OpenShift cluster(s). The following instances can be installed by the deployer:</p> <ul> <li>AI Manager</li> <li>Event Manager</li> <li>Turbonomic</li> <li>Instana</li> <li>Infrastructure management</li> <li>ELK stack (ElasticSearch, Logstash, Kibana)</li> </ul> <p>Aside from the base install, the deployer can also install ready-to-use demos for each of the instances</p> <pre><code>cp4waiops:\n- project: cp4waiops\n  openshift_cluster_name: \"{{ env_id }}\"\n  openshift_storage_name: auto-storage\n  accept_licenses: False\n \n  instances:\n  - name: cp4waiops-aimanager\n    kind: AIManager\n    install: true\n  ...\n</code></pre>"},{"location":"30-reference/configuration/cloud-pak/#aiops-main-properties","title":"AIOPS main properties","text":"<p>The following properties are defined on the project level:</p> Property Description Mandatory Allowed values project The name of the OpenShift project that will be created and used for the installation of the defined instances. Yes openshift_cluster_name Dynamically defined form the <code>env_id</code> parameter during the execution. No, only if mutiple OpenShift clusters defined Existing <code>openshift</code> cluster openshift_storage_name Reference to the storage definition that exists in the <code>openshift</code> object (please see above). No, inferred from openshift-&gt;openshift_storage accept_licenses Set to <code>True</code> to accept Cloud Pak licenses. Alternatively the <code>--accept-all-licenses</code> can be used for the <code>cp-deploy.sh</code> command Yes True, False"},{"location":"30-reference/configuration/cloud-pak/#service-instances","title":"Service instances","text":"<p>The project that is specified at the <code>cp4waiops</code> level defines the OpenShift project into which the instances of each of the services will be installed. Below is a list of instance \"kinds\" that can be installed. For every \"service instance\" there can also be a \"demo content\" entry to prepare the demo content for the capability.</p>"},{"location":"30-reference/configuration/cloud-pak/#ai-manager","title":"AI Manager","text":"<pre><code>  instances:\n  - name: cp4waiops-aimanager\n    kind: AIManager\n    install: true\n\n    waiops_size: small\n    custom_size_file: none\n    waiops_name: ibm-cp-watson-aiops\n    subscription_channel: v3.6\n    freeze_catalog: false\n</code></pre> Property Description Mandatory Allowed values name Unique name within the cluster using only lowercase alphanumerics and \"-\" Yes kind Service kind to install Yes AIManager install Must the service be installed? Yes true, false waiops_size Size of the install Yes small, tall, custom custom_size_file Name of the file holding the custom sizes if <code>waiops_size</code> is <code>custom</code> No waiops_name Name of the CP4WAIOPS instance Yes subscription_channel Subscription channel of the operator Yes freeze_catalog Freeze the version of the catalog source? Yes false, true case_install Must AI manager be installed via case files? No false, true case_github_url GitHub URL to download case file Yes if <code>case_install</code> is <code>true</code> case_name Name of the case file Yes if <code>case_install</code> is <code>true</code> case_version Version of the case file to download Yes if <code>case_install</code> is <code>true</code> case_inventory_setup Case file operation to run for this service Yes if <code>case_install</code> is <code>true</code> cpwaiopsSetup"},{"location":"30-reference/configuration/cloud-pak/#ai-manager---demo-content","title":"AI Manager - Demo Content","text":"<pre><code>  instances:\n  - name: cp4waiops-aimanager-demo-content\n    kind: AIManagerDemoContent\n    install: true\n    ...\n</code></pre> Property Description Mandatory Allowed values name Unique name within the cluster using only lowercase alphanumerics and \"-\" Yes kind Service kind to install Yes AIManagerDemoContent install Must the content be installed? Yes true, false <p>See sample config for remainder of properties.</p>"},{"location":"30-reference/configuration/cloud-pak/#event-manager","title":"Event Manager","text":"<pre><code>  instances:\n  - name: cp4waiops-eventmanager\n    kind: EventManager\n    install: true\n    subscription_channel: v1.11\n    starting_csv: noi.v1.7.0\n    noi_version: 1.6.6\n</code></pre> Property Description Mandatory Allowed values name Unique name within the cluster using only lowercase alphanumerics and \"-\" Yes kind Service kind to install Yes EventManager install Must the service be installed? Yes true, false subscription_channel Subscription channel of the operator Yes starting_csv Starting Cluster Server Version Yes noi_version Version of noi Yes"},{"location":"30-reference/configuration/cloud-pak/#event-manager-demo-content","title":"Event Manager Demo Content","text":"<pre><code>  instances:\n  - name: cp4waiops-eventmanager\n    kind: EventManagerDemoContent\n    install: true\n</code></pre> Property Description Mandatory Allowed values name Unique name within the cluster using only lowercase alphanumerics and \"-\" Yes kind Service kind to install Yes EventManagerDemoContent install Must the content be installed? Yes true, false"},{"location":"30-reference/configuration/cloud-pak/#infrastructure-management","title":"Infrastructure Management","text":"<pre><code>  instances:\n  - name: cp4waiops-infrastructure-management\n    kind: InfrastructureManagement\n    install: false\n    subscription_channel: v3.5\n</code></pre> Property Description Mandatory Allowed values name Unique name within the cluster using only lowercase alphanumerics and \"-\" Yes kind Service kind to install Yes InfrastructureManagement install Must the service be installed? Yes true, false subscription_channel Subscription channel of the operator Yes"},{"location":"30-reference/configuration/cloud-pak/#elk-stack","title":"ELK stack","text":"<p>ElasticSearch, Logstash and Kibana stack.</p> <pre><code>  instances:\n  - name: cp4waiops-elk\n    kind: ELK\n    install: false\n</code></pre> Property Description Mandatory Allowed values name Unique name within the cluster using only lowercase alphanumerics and \"-\" Yes kind Service kind to install Yes ELK install Must the service be installed? Yes true, false"},{"location":"30-reference/configuration/cloud-pak/#instana","title":"Instana","text":"<pre><code>  instances:\n  - name: cp4waiops-instana\n    kind: Instana\n    install: true\n    version: 241-0\n\n    sales_key: 'NONE'\n    agent_key: 'NONE'\n\n    instana_admin_user: \"admin@instana.local\"\n    #instana_admin_pass: 'CHANGEME'\n    \n    install_agent: true\n\n    integrate_aimanager: true\n    #integrate_turbonomic: true\n</code></pre> Property Description Mandatory Allowed values name Unique name within the cluster using only lowercase alphanumerics and \"-\" Yes kind Service kind to install Yes Instana install Must the service be installed? Yes true, false version Version of Instana to install No sales_key License key to be configured No agent_key License key for agent to be configured No instana_admin_user Instana admin user to be configured Yes instana_admin_pass Instana admin user password to be set (if different from global password) No install_agent Must the Instana agent be installed? Yes true, false integrate_aimanager Must Instana be integrated with AI Manager? Yes true, false integrate_turbonomic Must Instana be integrated with Turbonomic? No true, false"},{"location":"30-reference/configuration/cloud-pak/#turbonomic","title":"Turbonomic","text":"<pre><code>  instances:\n  - name: cp4waiops-turbonomic\n    kind: Turbonomic\n    install: true\n    turbo_version: 8.7.0\n</code></pre> Property Description Mandatory Allowed values name Unique name within the cluster using only lowercase alphanumerics and \"-\" Yes kind Service kind to install Yes Turbonomic install Must the service be installed? Yes true, false turbo_version Version of Turbonomic to install Yes"},{"location":"30-reference/configuration/cloud-pak/#turbonomic-demo-content","title":"Turbonomic Demo Content","text":"<pre><code>  instances:\n  - name: cp4waiops-turbonomic-demo-content\n    kind: TurbonomicDemoContent\n    install: true\n    #turbo_admin_password: CHANGEME\n    create_user: false\n    demo_user: demo\n    #turbo_demo_password: CHANGEME\n</code></pre> Property Description Mandatory Allowed values name Unique name within the cluster using only lowercase alphanumerics and \"-\" Yes kind Service kind to install Yes TurbonomicDemoContent install Must the content be installed? Yes true, false turbo_admin_pass Turbonomic admin user password to be set (if different from global password) No create_user Must the demo user be created? No false, true demo_user Name of the demo user No turbo_demo_password Demo user password if different from global password No <p>See sample config for remainder of properties.</p>"},{"location":"30-reference/configuration/cloud-pak/#cp4ba","title":"<code>cp4ba</code>","text":"<p>Defines the Cloud Pak for Business Automation installation to be configured on the OpenShift cluster(s). See Cloud Pak for Business Automation for additional details.  </p> <pre><code>---\ncp4ba:\n- project: cp4ba\n  collateral_project: cp4ba-collateral\n  openshift_cluster_name: \"{{ env_id }}\"\n  openshift_storage_name: auto-storage\n  accept_licenses: false\n  state: installed\n  cpfs_profile_size: small # Profile size which affect replicas and resources of Pods of CPFS as per https://www.ibm.com/docs/en/cpfs?topic=operator-hardware-requirements-recommendations-foundational-services\n\n  # Section for Cloud Pak for Business Automation itself\n  cp4ba:\n    # Set to false if you don't want to install (or remove) CP4BA\n    enabled: true # Currently always true\n    profile_size: small # Profile size which affect replicas and resources of Pods as per https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=pcmppd-system-requirements\n    patterns:\n      foundation: # Foundation pattern, always true - https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__foundation\n        optional_components:\n          bas: true # Business Automation Studio (BAS) \n          bai: true # Business Automation Insights (BAI)\n          ae: true # Application Engine (AE)\n      decisions: # Operational Decision Manager (ODM) - https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__odm\n        enabled: true\n        optional_components:\n          decision_center: true # Decision Center (ODM)\n          decision_runner: true # Decision Runner (ODM)\n          decision_server_runtime: true # Decision Server (ODM)\n        # Additional customization for Operational Decision Management\n        # Contents of the following will be merged into ODM part of CP4BA CR yaml file. Arrays are overwritten.\n        cr_custom:\n          spec:\n            odm_configuration:\n              decisionCenter:\n                # Enable support for decision models\n                disabledDecisionModel: false\n      decisions_ads: # Automation Decision Services (ADS) - https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__ads\n        enabled: true\n        optional_components:\n          ads_designer: true # Designer (ADS)\n          ads_runtime: true # Runtime (ADS)\n        gen_ai: # https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=services-configuring-generative-ai-secret\n          apiKey: watsonx_ai_api_key\n          mlUrl: https://us-south.ml.cloud.ibm.com\n          projectId: project_id          \n      content: # FileNet Content Manager (FNCM) - https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__ecm\n        enabled: true\n        optional_components:\n          cmis: true # Content Management Interoperability Services (FNCM - CMIS)\n          css: true # Content Search Services (FNCM - CSS)\n          es: true # External Share (FNCM - ES)\n          tm: true # Task Manager (FNCM - TM)\n          ier: true # IBM Enterprise Records (FNCM - IER)\n          icc4sap: false # IBM Content Collector for SAP (FNCM - ICC4SAP) - Currently not implemented\n      application: # Business Automation Application (BAA) - https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__baa\n        enabled: true\n        optional_components:\n          app_designer: true # App Designer (BAA)\n          ae_data_persistence: true # App Engine data persistence (BAA)\n      document_processing: # Automation Document Processing (ADP) - https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__adp\n        enabled: true\n        optional_components: \n          document_processing_designer: true # Designer (ADP)\n        # Additional customization for Automation Document Processing\n        # Contents of the following will be merged into ADP part of CP4BA CR yaml file. Arrays are overwritten.\n        cr_custom:\n          spec:\n            ca_configuration:\n              ## NB: All config parameters for ADP are described here ==&gt; https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=parameters-automation-document-processing\n              ocrextraction:\n                # [Tech Preview] OCR Engine 2 (IOCR) for ADP - Starts the Watson Document Understanding (WDU) pods to process documents.\n                use_iocr: auto # Allowed values: auto, all, none. Refer to doc for option details: https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=parameters-automation-document-processing#:~:text=ocrextraction.use_iocr\n                deep_learning_object_detection: # When enabled, ca_configuration.deeplearning parameters will be used (ignored otherwise), and deep-learning pods will be deployed to enhance object detection.\n                  # If disabled, all training will automatically be done in \"fast-training\" mode and should finish in less than 10 min.\n                  # Warn: If you enable this option and don't select the \"fast training\" mode in ADP before starting training, training could take hours (or more if you don't have GPUs).\n                  #       See \"Important\" note here for usage recommandation on using \"fast/deeplarning\" training: https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/23.0.2?topic=project-creating-data-extraction-model#:~:text=Training%20takes%20time\n                  enabled: true\n              deeplearning: # Only used if deep_learning_object_detection is enabled. Configure usage of GPU-enabled Nodes.\n                gpu_enabled: false # Use GPUs for deeplearning training instead of CPUs.\n                nodelabel_key: nvidia.com/gpu.present\n                nodelabel_value: \"true\"\n                replica_count: 1 # Controls the number of deep learning pod replicas. NB: The number of GPUs available on your cluster should be \u2265 to replica_count.\n      workflow: # Business Automation Workflow (BAW) - https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__baw\n        enabled: true\n        optional_components:\n          baw_authoring: true # Workflow Authoring (BAW) - always keep true if workflow pattern is chosen. BAW Runtime is not implemented.\n          kafka: true # Will install a kafka cluster and enable kafka service for workflow authoring.\n          workflow_assistant: true # Will enable Authoring assistant for workflow authoring.\n          workplace_assistant: true # Will enable Workplace assistant.\n        gen_ai: # https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/25.0.0?topic=customizing-enabling-generative-ai\n          apiKey: watsonx_ai_api_key\n          mlUrl: https://us-south.ml.cloud.ibm.com\n          projectId: project_id\n          defaultFoundationModel: meta-llama/llama-3-3-70b-instruct\n  \n  # Section for IBM Process mining\n  pm:\n    # Set to false if you don't want to install (or remove) Process Mining\n    enabled: true\n    # Additional customization for Process Mining\n    # Contents of the following will be merged into PM CR yaml file. Arrays are overwritten.\n    cr_custom:\n      spec:\n        processmining:\n          storage:\n            # Disables redis to spare resources as per https://www.ibm.com/docs/en/process-mining/latest?topic=configurations-custom-resource-definition\n            redis:\n              install: false  \n\n  # Section for IBM Robotic Process Automation\n  rpa:\n    # Set to false if you don't want to install (or remove) RPA\n    enabled: true\n    # Additional customization for Robotic Process Automation\n    # Contents of the following will be merged into RPA CR yaml file. Arrays are overwritten.\n    cr_custom:\n      spec:\n        # Configures the NLP provider component of IBM RPA. You can disable it by specifying 0. https://www.ibm.com/docs/en/rpa/latest?topic=platform-configuring-rpa-custom-resources#basic-setup\n        sizeMapping:\n          watson-nlp:\n            replicas: 1\n\n  # Set to false if you don't want to install (or remove) CloudBeaver (PostgreSQL, DB2, MSSQL UI)\n  cloudbeaver_enabled: true\n\n  # Set to false if you don't want to install (or remove) Roundcube\n  roundcube_enabled: true\n\n  # Set to false if you don't want to install (or remove) Cerebro\n  cerebro_enabled: true\n\n  # Set to false if you don't want to install (or remove) AKHQ\n  akhq_enabled: true\n\n  # Set to false if you don't want to install (or remove) phpLDAPAdmin\n  phpldapadmin_enabled: true\n\n  # Set to false if you don't want to install (or remove) OpenSearch Dashboards\n  opensearch_dashboards_enabled: true  \n</code></pre>"},{"location":"30-reference/configuration/cloud-pak/#cp4ba-main-properties","title":"CP4BA main properties","text":"<p>The following properties are defined on the project level.</p> Property Description Mandatory Allowed values project The name of the OpenShift project that will be created and used for the installation of the defined instances. Yes Valid OCP project name collateral_project The name of the OpenShift project that will be created and used for the installation of all collateral (prerequisites and extras). Yes Valid OCP project name openshift_cluster_name Dynamically defined form the <code>env_id</code> parameter during the execution. No, only if multiple OpenShift clusters defined Existing <code>openshift</code> cluster openshift_storage_name Reference to the storage definition that exists in the <code>openshift</code> object (please see above). No, inferred from openshift-&gt;openshift_storage accept_licenses Set to <code>true</code> to accept Cloud Pak licenses. Alternatively the <code>--accept-all-licenses</code> can be used for the <code>cp-deploy.sh</code> command Yes true, false state Set to <code>installed</code> to install <code>enabled</code> capabilities, set to <code>removed</code> to remove <code>enabled</code> capabilities. Yes installed, removed cpfs_profile_size Profile size which affect replicas and resources of Pods of CPFS as per https://www.ibm.com/docs/en/cpfs?topic=operator-hardware-requirements-recommendations-foundational-services Yes starterset, small, medium, large"},{"location":"30-reference/configuration/cloud-pak/#cloud-pak-for-business-automation-properties","title":"Cloud Pak for Business Automation properties","text":"<p>Used to configure CP4BA. Placed in <code>cp4ba</code> key on the project level.</p> Property Description Mandatory Allowed values enabled Set to <code>true</code> to enable CP4BA. Currently always <code>true</code>. Yes true profile_size Profile size which affect replicas and resources of Pods as per https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=pcmppd-system-requirements Yes small, medium, large patterns Section where CP4BA patterns are configured. Please make sure to select all that is needed as a dependencies. Dependencies can be determined from documentation at https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments Yes Object - see details below"},{"location":"30-reference/configuration/cloud-pak/#foundation-pattern-properties","title":"Foundation pattern properties","text":"<p>Always configure in CP4BA. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__foundation Placed in <code>cp4ba.patterns.foundation</code> key.</p> Property Description Mandatory Allowed values optional_components Sub object for definition of optional components for pattern. Yes Object - specific to each pattern optional_components.bas Set to <code>true</code> to enable Business Automation Studio Yes true, false optional_components.bai Set to <code>true</code> to enable Business Automation Insights Yes true, false optional_components.ae Set to <code>true</code> to enable Application Engine Yes true, false"},{"location":"30-reference/configuration/cloud-pak/#decisions-pattern-properties","title":"Decisions pattern properties","text":"<p>Used to configure Operation Decision Manager. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__odm Placed in <code>cp4ba.patterns.decisions</code> key.</p> Property Description Mandatory Allowed values enabled Set to <code>true</code> to enable <code>decisions</code> pattern. Yes true, false optional_components Sub object for definition of optional components for pattern. Yes Object - specific to each pattern optional_components.decision_center Set to <code>true</code> to enable Decision Center Yes true, false optional_components.decision_runner Set to <code>true</code> to enable Decision Runner Yes true, false optional_components.decision_server_runtime Set to <code>true</code> to enable Decision Server Yes true, false cr_custom Additional customization for Operational Decision Management. Contents will be merged into ODM part of CP4BA CR yaml file. Arrays are overwritten. No Object"},{"location":"30-reference/configuration/cloud-pak/#decisions-ads-pattern-properties","title":"Decisions ADS pattern properties","text":"<p>Used to configure Automation Decision Services. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__ads Placed in <code>cp4ba.patterns.decisions_ads</code> key.</p> Property Description Mandatory Allowed values enabled Set to <code>true</code> to enable <code>decisions_ads</code> pattern. Yes true, false optional_components Sub object for definition of optional components for pattern. Yes Object - specific to each pattern optional_components.ads_designer Set to <code>true</code> to enable Designer Yes true, false optional_components.ads_runtime Set to <code>true</code> to enable Runtime Yes true, false gen_ai Sub object for definition of GenAI connection. More on https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/24.0.0?topic=services-configuring-generative-ai-secret false Object gen_ai.apiKey Set to real value of your Watsonx.AI platform false Your real value gen_ai.mlUrl Set to real value of your Watsonx.AI platform false Your real value, (default) https://us-south.ml.cloud.ibm.com gen_ai.projectId Set to real value of your Watsonx.AI platform false Your real value"},{"location":"30-reference/configuration/cloud-pak/#content-pattern-properties","title":"Content pattern properties","text":"<p>Used to configure FileNet Content Manager. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__ecm Placed in <code>cp4ba.patterns.content</code> key.</p> Property Description Mandatory Allowed values enabled Set to <code>true</code> to enable <code>content</code> pattern. Yes true, false optional_components Sub object for definition of optional components for pattern. Yes Object - specific to each pattern optional_components.cmis Set to <code>true</code> to enable CMIS Yes true, false optional_components.css Set to <code>true</code> to enable Content Search Services Yes true, false optional_components.tm Set to <code>true</code> to enable Task Manager Yes true, false optional_components.ier Set to <code>true</code> to enable IBM Enterprise Records Yes true, false optional_components.icc4sap Set to <code>true</code> to enable IBM Content Collector for SAP. Currently not functional. Always false. Yes false"},{"location":"30-reference/configuration/cloud-pak/#application-pattern-properties","title":"Application pattern properties","text":"<p>Used to configure Business Automation Application. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__baa Placed in <code>cp4ba.patterns.application</code> key.</p> Property Description Mandatory Allowed values enabled Set to <code>true</code> to enable <code>application</code> pattern. Yes true, false optional_components Sub object for definition of optional components for pattern. Yes Object - specific to each pattern optional_components.app_designer Set to <code>true</code> to enable Application Designer Yes true, false optional_components.ae_data_persistence Set to <code>true</code> to enable App Engine data persistence Yes true, false"},{"location":"30-reference/configuration/cloud-pak/#document-processing-pattern-properties","title":"Document Processing pattern properties","text":"<p>Used to configure Automation Document Processing. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__baa Placed in <code>cp4ba.patterns.document_processing</code> key.</p> Property Description Mandatory Allowed values enabled Set to <code>true</code> to enable <code>document_processing</code> pattern. Yes true, false optional_components Sub object for definition of optional components for pattern. Yes Object - specific to each pattern optional_components.document_processing_designer Set to <code>true</code> to enable Designer Yes true cr_custom Additional customization for Automation Document Processing. Contents will be merged into ADP part of CP4BA CR yaml file. Arrays are overwritten. No Object"},{"location":"30-reference/configuration/cloud-pak/#workflow-pattern-properties","title":"Workflow pattern properties","text":"<p>Used to configure Business Automation Workflow. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capabilities-production-deployments#concept_c2l_1ks_fnb__baw Placed in <code>cp4ba.patterns.workflow</code> key.</p> Property Description Mandatory Allowed values enabled Set to <code>true</code> to enable <code>workflow</code> pattern. Yes true, false optional_components Sub object for definition of optional components for pattern. Yes Object - specific to each pattern optional_components.baw_authoring Set to <code>true</code> to enable Workflow Authoring. Currently always <code>true</code>. Yes true optional_components.kafka Set to <code>true</code> to enable kafka service for workflow authoring. Yes true, false optional_components.workflow_assistant Set to <code>true</code> to enable Authoring assistant for workflow authoring. Yes true, false optional_components.workplace_assistant Set to <code>true</code> to enable Workplace assistant. Yes true, false gen_ai Sub object for definition of GenAI connection. For Gen AI Service Flow, Authoring Assistant and Workplace Assistant false Object gen_ai.apiKey Set to real value of your Watsonx.AI platform false Your real value gen_ai.mlUrl Set to real value of your Watsonx.AI platform false Your real value, (default) https://us-south.ml.cloud.ibm.com gen_ai.projectId Set to real value of your Watsonx.AI platform false Your real value gen_ai.defaultFoundationModel Set to desired ID of foundation model (Only for Gen AI Service flow) false Your real value, (default) meta-llama/llama-3-3-70b-instruct"},{"location":"30-reference/configuration/cloud-pak/#process-mining-properties","title":"Process Mining properties","text":"<p>Used to configure IBM Process Mining. Placed in <code>pm</code> key on the project level.</p> Property Description Mandatory Allowed values enabled Set to <code>true</code> to enable <code>process mining</code>. Yes true, false cr_custom Additional customization for Process Mining. Contents will be merged into PM CR yaml file. Arrays are overwritten. No Object"},{"location":"30-reference/configuration/cloud-pak/#robotic-process-automation-properties","title":"Robotic Process Automation properties","text":"<p>Used to configure IBM Robotic Process Automation. Placed in <code>rpa</code> key on the project level.</p> Property Description Mandatory Allowed values enabled Set to <code>true</code> to enable <code>rpa</code>. Yes true, false cr_custom Additional customization for Process Mining. Contents will be merged into RPA CR yaml file. Arrays are overwritten. No Object"},{"location":"30-reference/configuration/cloud-pak/#business-automation-manager-open-editions-properties","title":"Business Automation Manager Open Editions properties","text":"<p>Used to configure Business Automation Manager Open Editions. Placed in <code>bamoe</code> key on the project level.</p> Property Description Mandatory Allowed values enabled Set to <code>true</code> to enable <code>bamoe</code>. Yes true, false"},{"location":"30-reference/configuration/cloud-pak/#other-properties","title":"Other properties","text":"<p>Used to configure extra UIs. The following properties are defined on the project level.  </p> Property Description Mandatory Allowed values cloudbeaver_enabled Set to <code>true</code> to enable CloudBeaver (PostgreSQL, DB2, MSSQL UI). Yes true, false roundcube_enabled Set to <code>true</code> to enable Roundcube. Client for mail. Yes true, false cerebro_enabled Set to <code>true</code> to enable Cerebro. Client for ElasticSearch in CP4BA. Yes true, false akhq_enabled Set to <code>true</code> to enable AKHQ. Client for Kafka in CP4BA. Yes true, false phpldapadmin_enabled Set to <code>true</code> to enable phpLDApAdmin. Client for OpenLDAP. Yes true, false opensearch_dashboards_enabled Set to <code>true</code> to enable OpenSearch Dashboards. Client for OpenSearch. Yes true, false"},{"location":"30-reference/configuration/cp4ba/","title":"Cloud Pak for Business Automation","text":"<p>Contains CP4BA version 25.0.1. Contains IPM version 2.1.0. Contains RPA version 30.0.1. Contains BAMOE version 9.3.1.  </p> <ul> <li>Disclaimer \u270b</li> <li>Documentation base \ud83d\udcdd</li> <li>Benefits \ud83d\ude80</li> <li>General information \ud83d\udce2</li> <li>What is in the package \ud83d\udce6</li> <li>Environments used for installation \ud83d\udcbb</li> <li>Automated post-deployment tasks \u2705</li> <li>Usage &amp; operations \ud83d\udcc7</li> <li>Optional post deployment steps \ud83d\udccb</li> <li>Licensing \ud83d\udcd1</li> </ul>"},{"location":"30-reference/configuration/cp4ba/#disclaimer-","title":"Disclaimer \u270b","text":"<p>This is not an official IBM documentation. Absolutely no warranties, no support, no responsibility for anything. Use it on your own risk and always follow the official IBM documentations. It is always your responsibility to make sure you are license compliant when using this repository to install IBM Cloud Pak for Business Automation.</p> <p>Please do not hesitate to create an issue here if needed. Your feedback is appreciated.</p> <p>Not for production use (neither dev nor test or prod environments). Suitable for Demo and PoC environments - but with Production deployment. </p> <p>!Important - Keep in mind that this deployment contains capabilities (the ones which are not bundled with CP4BA) which are not eligible to run on Worker Nodes covered by CP4BA OCP Restricted licenses. More info on https://www.ibm.com/docs/en/cloud-paks/1.0?topic=clusters-restricted-openshift-entitlement.</p>"},{"location":"30-reference/configuration/cp4ba/#documentation-base-","title":"Documentation base \ud83d\udcdd","text":"<p>Deploying CP4BA is based on official documentation which is located at https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation.</p> <p>Deployment of other parts is also based on respective official documentations.</p> <ul> <li>IBM Robotic Process Automation (RPA) https://www.ibm.com/docs/en/rpa</li> <li>IBM Process Mining https://www.ibm.com/docs/en/process-mining/</li> <li>IBM Business Automation Manager Open Editions https://www.ibm.com/docs/en/ibamoe</li> <li>IBM Cloud Pak Foundational Services (CPFS) https://www.ibm.com/docs/en/cloud-paks/foundational-services</li> </ul>"},{"location":"30-reference/configuration/cp4ba/#benefits-","title":"Benefits \ud83d\ude80","text":"<ul> <li>Automatic deployment of the whole platform where you don't need to take care about almost any prerequisites</li> <li>OCP Ingress certificate is used for Routes so there is only one certificate you need to trust in you local machine to trust all URLs of the whole platform</li> <li>Trusted certificate in browser also enable you to save passwords</li> <li>Wherever possible a common admin user cpadmin with adjustable password is used, so you don't need to remember multiple credentials when you want to access the platform (convenience also comes with responsibility - so you don't want to expose your platform to whole world)</li> <li>The whole platform is running on containers, so you don't need to manually prepare anything on traditional VMs and take care of them including required prerequisites</li> <li>Many otherwise manual post-deployment steps have been automated</li> <li>Pre integrated and automatically connected extras are deployed in the platform for easier access/management/troubleshooting</li> <li>You have a working Production deployment which you can use as a reference for further custom deployments</li> </ul>"},{"location":"30-reference/configuration/cp4ba/#general-information-","title":"General information \ud83d\udce2","text":"<p>What is not included:</p> <ul> <li>ICCs - not covered.</li> <li>FNCM External share - Not supported with ZEN</li> <li>Asset Repository - it is more part of CP4I.</li> <li>Workflow Server and Workstream Services - this is a dev deployment. BAW Authoring and (BAW + IAWS) are mutually exclusive in single project.</li> <li>ADP Runtime deployment - this is a dev deployment.</li> </ul> <p>You can review the code here and here to directly view how certain components are enabled based on parameter settings.</p>"},{"location":"30-reference/configuration/cp4ba/#what-is-in-the-package-","title":"What is in the package \ud83d\udce6","text":"<p>Only fully configured CP4BA, RPA, IPM and BAMOE including all Extras and Prerequisites is validated. As validation activities have NOT been executed using partial components, please report any issues you may encounter.</p> <p>More details about each section from the picture follows below it.</p> <p></p>"},{"location":"30-reference/configuration/cp4ba/#extras-section","title":"Extras section","text":"<p>Contains extra software which makes working with the platform even easier.</p> <ul> <li>phpLDAPadmin - Web UI for OpenLDAP directory making it easier to admin and troubleshoot the LDAP.</li> <li>Installed if enabled in configuration (default).</li> <li>Gitea - Contains Git server with web UI and is used for ADS and ADP for project sharing and publishing. Organizations for ADS and APD are automatically created. Gitea is connected to OpenLDAP for authentication and authorization.</li> <li>Installed if ADS or Document Processing is enabled.</li> <li>Nexus - Repository manager which contains pushed ADS java libraries needed for custom development and also for publishing custom ADS jars. Nexus is connected to OpenLDAP for authentication and authorization.</li> <li>Installed if ADS is enabled.</li> <li>Roundcube - Web UI for included Mail server to be able to browse incoming emails.</li> <li>Installed if enabled in configuration (default) and if Mail server (below) is installed.</li> <li>Cerebro - Web UI elastic search browser automatically connected to ES instance deployed with CP4BA.</li> <li>Installed if enabled in configuration (default) and if BAI or BAW is enabled.</li> <li>AKHQ - Web UI kafka browser automatically connected to Kafka instance deployed with CP4BA.</li> <li>Installed if enabled in configuration (default) and if BAI or BAW with Kafka feature is enabled.</li> <li>OpenSearch Dashboards - Web UI OpenSearch dashboard tool automatically connected to OpenSearch instance deployed with CP4BA.</li> <li>Installed if enabled in configuration (default) and if BAI or BAW is enabled.</li> <li>Mail server - For various mail integrations e.g. from BAN, BAW and RPA.</li> <li>Installed if CP4BA or RPA is enabled.</li> <li>CloudBeaver - Web UI for Postgresql and MSSQL databases making it easier to admin and troubleshoot the DBs.</li> <li>Installed if enabled in configuration (default) and if PostgreSQL or MSSQL (from Pre-requisites) is installed.</li> </ul>"},{"location":"30-reference/configuration/cp4ba/#business-automation-section","title":"Business Automation section","text":""},{"location":"30-reference/configuration/cp4ba/#business-automation-capabilities","title":"Business Automation capabilities","text":"<p>CP4BA capabilities are in purple color.</p> <p>More info for CP4BA capabilities is available in official docs at https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest.</p> <p>More specifically in overview of patterns at https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/latest?topic=deployment-capability-patterns-production-deployments.</p> <p>Pink color is used for CPFS dedicated capabilities.</p> <p>More info for CPFS dedicated capabilities is available in official docs at https://www.ibm.com/docs/en/cloud-paks/foundational-services/latest.</p> <p>Magenta color is used for additional capabilities.</p> <p>More info for Process Mining is available in official docs at https://www.ibm.com/docs/en/process-mining/latest.</p> <p>More info for RPA is available in official docs at https://www.ibm.com/docs/en/rpa/latest.</p> <p>More info for BAMOE is available in official docs at https://www.ibm.com/docs/en/ibamoe.</p> <p>Assets are currently not deployed.</p>"},{"location":"30-reference/configuration/cp4ba/#cpfs-cloud-pak-foundational-services-section","title":"CPFS (Cloud Pak Foundational Services) section","text":"<p>Contains services which are reused by Cloud Paks.</p> <p>More info available in official docs at https://www.ibm.com/docs/en/cpfs.</p> <ul> <li>License service - Tracks license usage.</li> <li>Certificate Manager - Provides certificate handling.</li> </ul>"},{"location":"30-reference/configuration/cp4ba/#pre-requisites-section","title":"Pre-requisites section","text":"<p>Contains prerequisites for the whole platform.</p> <ul> <li>PostgreSQL - Database storage for the majority of capabilities.</li> <li>Installed if CP4BA or IPM is enabled.</li> <li>OpenLDAP - Directory solution for users and groups definition.</li> <li>Always installed.</li> <li>MSSQL server - Database storage for RPA server. </li> <li>nstalled if RPA is enbled.</li> </ul>"},{"location":"30-reference/configuration/cp4ba/#environments-used-for-installation-","title":"Environments used for installation \ud83d\udcbb","text":"<p>With proper sizing of the cluster and provided RWX File and RWO Block Storage Class, CP4BA deployed with Deployer should be working on any OpenShift 4.18 with Worker Nodes which in total have free 96 CPU, 384GB Memory for requests.</p>"},{"location":"30-reference/configuration/cp4ba/#automated-post-deployment-tasks-","title":"Automated post-deployment tasks \u2705","text":"<p>For your convenience the following post-deployment setup tasks have been automated:</p> <ul> <li>CPFS - OpenSearch cpadmin user added as admin.</li> <li>Zen - Users and Groups added.</li> <li>Zen - Administrative group is given all available privileges from all pillars.</li> <li>Zen - Regular groups are given developer privileges from all pillars.</li> <li>Zen - Service account created in CPFS IM and Zen and Zen API key is generated for convenient and stable usage.</li> <li>Zen - OCP Ingress certificate is used for better SSL trusting.</li> <li>Workforce Insights - Connection setup. You just need to create WFI dashboard. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/22.0.1?topic=secrets-creating-custom-bpc-workforce-secret</li> <li>ADS - Nexus connection setup and all ADS plugins loaded.</li> <li>ADS - Organization in Git created. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/22.0.1?topic=gst-task-2-connecting-git-repository-sharing-decision-service</li> <li>ADS - Automatic Git project connection. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/22.0.1?topic=services-connecting-remote-repository-automatically</li> <li>ADS - Roles assigned to users and groups. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/24.0.0?topic=services-managing-user-permissions</li> <li>ADS - Precreation of GenAI Secret. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/24.0.0?topic=services-configuring-generative-ai-secret</li> <li>ODM - Service user credentials automatically assigned to servers.</li> <li>ODM - Roles assigned to users and groups. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/24.0.0?topic=access-managing-user-permissions</li> <li>ADP - Organization in Git created. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/22.0.1?topic=processing-setting-up-remote-git-organization</li> <li>ADP - Default project data loaded. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/22.0.1?topic=processing-loading-default-sample-data</li> <li>ADP - Git connection and CDD repo creation done. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/24.0.1?topic=processing-setting-up-remote-git-organization</li> <li>ADP - More project DBs created (6 in total - accommodates 3 ADP projects). https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/24.0.0?topic=processing-creating-additional-project-databases</li> <li>IER - Task Manager pod has TM_JOB_URL parameter set.</li> <li>IER - Task manager set up with CPE JARs required by IER.</li> <li>Task manager - Enabled in Navigator.</li> <li>FNCM - Enabled search result highlighting for Simple Search for FNCM (OS1), IER (FPOS, ROS), BAW (BAWTOS, BAWDOS, BAWDOCS), AE (AEOS) and ADP (DEVOS1) objectstores.</li> <li>FNCM - Set default storage policy for Document class and its subclasses to FileSystem based ASA instead of DB.</li> <li>BAW - tw_admins enhanced with LDAP admin groups.</li> <li>BAW - tw_authors enhanced with LDAP user and admin groups.</li> <li>BAW - Created FileNet Subscription for ECM Content event in BAWTOS Object Store. https://www.ibm.com/docs/en/baw/24.x?topic=events-using-event-handler-filenet-content-manager</li> <li>BAW - Enable Case History on FNCM Domain level as a prerequisite for Timeline Visualizer. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/24.0.1?topic=widgets-timeline-visualizer</li> <li>BAW - Added stub configurations for watsonx.ai integration. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/24.0.1?topic=customizing-enabling-generative-ai</li> <li>BAW - Enabled Process Admin audit log. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/24.0.1?topic=customizing-enabling-audit-log</li> <li>BAW - Added stub configurations for Git integration. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/24.0.1?topic=integration-integrating-github</li> <li>BAW - Added stub configuration for Workplace and Authoring assistants.https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/25.0.1?topic=customizing-configuring-authoring-assistant</li> <li>BAW - Added stub configuration for watsonx Orchestrate integration. https://www.ibm.com/docs/en/cloud-paks/cp-biz-automation/25.0.1?topic=services-integrating-ai-agents</li> <li>BAI - extra flink task manager added for custom event processing.</li> <li>RPA - Bot Developer permission added to administrative user.</li> <li>IPM - Task mining related permissions added to admin user.</li> <li>IPM - Task mining admin user enabled for TM agent usage.</li> <li>IPM - Prescriptive mining permissions assigned to admin user.</li> </ul>"},{"location":"30-reference/configuration/cp4ba/#usage--operations-","title":"Usage &amp; operations \ud83d\udcc7","text":"<p>Endpoints, access info and other useful information is available in CP4BA Project (by default cp4ba) in ConfigMap 000-usage after installation.</p>"},{"location":"30-reference/configuration/cp4ba/#optional-post-deployment-steps-","title":"Optional post deployment steps \ud83d\udccb","text":"<p>CP4BA Review and perform post deploy manual steps for CP4BA as specified in CP4BA Project (by default cp4ba) in ConfigMap 000-cp4ba-postdeploy in postdeploy.md file. It is best to copy the contents and open it in nice MarkDown editor like VSCode.</p> <p>RPA Review and perform post deploy manual steps for RPA as specified in CP4BA Project (by default cp4ba) in ConfigMap 000-rpa-postdeploy in postdeploy.md file. It is best to copy the contents and open it in nice MarkDown editor like VSCode.</p> <p>Process Mining Review and perform post deploy manual steps for IPM as specified in CP4BA Project (by default cp4ba) in ConfigMap 000-pm-postdeploy in postdeploy.md file. It is best to copy the contents and open it in nice MarkDown editor like VSCode.</p>"},{"location":"30-reference/configuration/cp4ba/#licensing-","title":"Licensing \ud83d\udcd1","text":"<p>Referrence to licenses of used tools apart from those from IBM</p> <ul> <li>phpLDAPadmin<ul> <li>MIT License https://github.com/osixia/docker-phpLDAPadmin/blob/stable/LICENSE (OSI approved https://opensource.org/license/mit)</li> </ul> </li> <li>Gitea<ul> <li>MIT License https://github.com/go-gitea/gitea/blob/main/LICENSE (OSI approved https://opensource.org/license/mit)</li> </ul> </li> <li>Nexus<ul> <li>Own EULA at https://www.sonatype.com/dnt/usage/community-edition-eula https://help.sonatype.com/en/ce-onboarding.html Source at https://help.sonatype.com/en/ce-onboarding.html#what-is-sonatype-nexus-repository-community-edition- states that \"Sonatype Nexus Repository Community Edition is the perfect solution to help individual developers and small teams manage their components effectively\u2014for free!\". Accepted during the deployment using REST API.</li> </ul> </li> <li>Roundcube<ul> <li>GNU General Public License v3.0 https://github.com/roundcube/roundcubemail/blob/master/LICENSE (OSI approved https://opensource.org/license/gpl-3-0)</li> <li>Also using NGINX - BSD 2-Clause \"Simplified\" License https://github.com/nginx/nginx/blob/master/LICENSE (OSI approved https://opensource.org/license/bsd-2-clause)</li> </ul> </li> <li>Cerebro<ul> <li>MIT License https://github.com/lmenezes/cerebro/blob/main/LICENSE (OSI approved https://opensource.org/license/mit)</li> </ul> </li> <li>AKHQ<ul> <li>Apache License 2.0 https://github.com/tchiotludo/akhq/blob/dev/LICENSE (OSI Approved https://opensource.org/license/apache-2-0)</li> </ul> </li> <li>OpenSearch Dashboards<ul> <li>Apache License 2.0 https://github.com/opensearch-project/OpenSearch-Dashboards/blob/main/LICENSE.txt (OSI Approved https://opensource.org/license/apache-2-0)</li> </ul> </li> <li>Mail server<ul> <li>MIT License https://github.com/docker-mailserver/docker-mailserver/blob/master/LICENSE (OSI approved https://opensource.org/license/mit)</li> </ul> </li> <li>CloudBeaver<ul> <li>Apache License 2.0 https://github.com/dbeaver/cloudbeaver/blob/devel/LICENSE (OSI Approved https://opensource.org/license/apache-2-0)</li> </ul> </li> <li>PostgreSQL<ul> <li>The PostgreSQL License https://www.postgresql.org/about/licence/ (OSI approved https://opensource.org/license/postgresql)</li> </ul> </li> <li>OpenLDAP<ul> <li>OpenLDAP itself - OpenLDAP Public License https://git.openldap.org/openldap/openldap/-/blob/master/LICENSE?ref_type=heads (OSI approved https://opensource.org/license/oldap-2-8)</li> <li>Bitnami package - https://github.com/bitnami/containers/tree/main/bitnami/openldap#license</li> </ul> </li> <li>MSSQL server<ul> <li>Uses Developer Edition, description at https://learn.microsoft.com/en-us/sql/sql-server/editions-and-components-of-sql-server-2022?view=sql-server-ver16#sql-server-editions</li> </ul> </li> </ul>"},{"location":"30-reference/configuration/cp4d-access-control/","title":"Cloud Pak for Data access control","text":"<p>Cloud Pak for Data can connect to an external identity provider (IdP) for user authentication. This function is delegated to Foundational Services IAM. Additional to user authentication, the IdP's groups can be mapped to Cloud Pak for Data user groups for access control.</p>"},{"location":"30-reference/configuration/cp4d-access-control/#roles---zen_role","title":"Roles - <code>zen_role</code>","text":"<p>Cloud Pak Deployer can be used to define user-defined roles in Cloud Pak for Data. Roles identify the permissions that a user or user group has on the platform.</p> <pre><code>zen_role:\n- name: monitor-role\n  description: User-defined role for monitoring the platform\n  state: installed\n  permissions:\n  - monitor_platform\n</code></pre>"},{"location":"30-reference/configuration/cp4d-access-control/#property-explanation","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the Cloud Pak for Data role Yes description Description of the Cloud Pak for Data role Yes state Indicates if the role must be installed or removed same OpenShift cluster No installed (default), removed permissions[] List of permissions to grant to the role Yes <p>To find the permissions that are allows, you can use the following REST API (GET) after authenticating to the platform: https://$CP4D_URL/icp4d-api/v1/permissions.</p>"},{"location":"30-reference/configuration/cp4d-access-control/#access-control---zen_access_control","title":"Access Control - <code>zen_access_control</code>","text":"<p>The <code>zen_access_control</code> object controls the creation of Zen user groups that map identify provider (IdP) groups and define the roles of the user group. A <code>user_groups</code> entry must contain at least 1 <code>roles</code> and must reference the associated IdP grouop(s).</p>"},{"location":"30-reference/configuration/cp4d-access-control/#example-with-red-hat-sso-keycloak-authentication","title":"Example with Red Hat SSO (Keycloak) authentication","text":"<p>The below configuration references a keycloak (Red Hat SSO) configuration. For the Red hat SSO configuration, refer to openshift_redhat_sso.</p> <pre><code>zen_access_control:\n- project: cpd\n  openshift_cluster_name: \"{{ env_id }}\"\n  keycloak_name: ibm-keycloak\n  user_groups:\n  - name: cp4d-admins\n    description: Cloud Pak for Data Administrators\n    roles:\n    - Administrator\n    keycloak_groups:\n    - kc-cp4d-admins\n  - name: cp4d-data-engineers\n    description: Cloud Pak for Data Data Engineers\n    roles:\n    - User\n    keycloak_groups:\n    - kc-cp4d-data-engineers\n  - name: cp4d-data-scientists\n    description: Cloud Pak for Data Data Scientists\n    roles:\n    - User\n    keycloak_groups:\n    - kc-cp4d-data-scientists\n  - name: cp4d-monitor\n    description: Cloud Pak for Data monitoring\n    roles:\n    - monitor-role\n    keycloak_groups:\n    - kc-cp4d-monitor\n</code></pre>"},{"location":"30-reference/configuration/cp4d-access-control/#example-with-ldap-authentication","title":"Example with LDAP authentication","text":"<p>The below configuration references an LDAP configuration. For the LDAP configuration, refer to ldap.</p> <pre><code>zen_access_control:\n- project: cpd\n  openshift_cluster_name: \"{{ env_id }}\"\n  ldap_names:\n  - cp4d-ldap\n  user_groups:\n  - name: cp4d-admins\n    description: Cloud Pak for Data Administrators\n    roles:\n    - Administrator\n    ldap_groups:\n    - cn=cp4d-admins,ou=Groups,dc=cp,dc=internal\n  - name: cp4d-data-engineers\n    description: Cloud Pak for Data Data Engineers\n    roles:\n    - User\n    ldap_groups:\n    - cn=cp4d-data-engineers,ou=Groups,dc=cp,dc=internal\n  - name: cp4d-data-scientists\n    description: Cloud Pak for Data Data Scientists\n    roles:\n    - User\n    ldap_groups:\n    - cn=cp4d-data-scientists,ou=Groups,dc=cp,dc=internal\n  - name: cp4d-monitor\n    description: Cloud Pak for Data monitoring\n    roles:\n    - monitor-role\n    ldap_groups:\n    - cn=cp4d-monitor,ou=Groups,dc=cp,dc=internal\n</code></pre>"},{"location":"30-reference/configuration/cp4d-access-control/#example-with-openldap-authentication","title":"Example with OpenLDAP authentication","text":"<p>The below configuration references an Demo OpenLDAP configuration. For the Demo OpenLDAP configuration, refer to demo_openldap.</p> <pre><code>zen_access_control:\n- project: cpd\n  openshift_cluster_name: \"{{ env_id }}\"\n  demo_openldap_names:\n  - ibm-openldap\n  user_groups:\n  - name: cp4d-admins\n    description: Cloud Pak for Data Administrators\n    roles:\n    - Administrator\n    ldap_groups:\n    - cn=cp4d-admins,ou=Groups,dc=cp,dc=internal\n  - name: cp4d-data-engineers\n    description: Cloud Pak for Data Data Engineers\n    roles:\n    - User\n    ldap_groups:\n    - cn=cp4d-data-engineers,ou=Groups,dc=cp,dc=internal\n  - name: cp4d-data-scientists\n    description: Cloud Pak for Data Data Scientists\n    roles:\n    - User\n    ldap_groups:\n    - cn=cp4d-data-scientists,ou=Groups,dc=cp,dc=internal\n  - name: cp4d-monitor\n    description: Cloud Pak for Data monitoring\n    roles:\n    - monitor-role\n    ldap_groups:\n    - cn=cp4d-monitor,ou=Groups,dc=cp,dc=internal\n</code></pre> <p></p>"},{"location":"30-reference/configuration/cp4d-access-control/#property-explanation_1","title":"Property explanation","text":"Property Description Mandatory Allowed values project <code>project</code> of the <code>cp4d</code> instance Yes openshift_cluster_name Reference to the <code>openshift</code> name Yes keycloak_name Name of the Red Hat SSO (Keycloak) instance on the same OpenShift cluster No demo_openldap_names[] Names of the demo OpenLDAP instances defined in the <code>demo_openldap</code> resource No user_groups[] Cloud Pak for Data user groups to be configured Yes .name Name of the CP4D user group Yes .description Description of the CP4D user group No .roles[] List of CP4D roles to assign to the user grouop Yes .keycloak_groups[] List of Red Hat SSO (Keycloak) groups to assign to the CP4D user group Yes if IdP is Keycloak .ldap_groups[] List of OpenLDAP groups to assign to the CP4D user group Yes if IdP is OpenLDAP <p><code>role</code> values: The following roles are defined by default in Cloud Pak for Data: - Administrator - User</p> <p>Further roles can be defined in the <code>zen</code> object and can be referenced by the <code>user_groups.roles[]</code> property.</p> <p>During the creation of User Group(s) the following validations are performed: - The provided role(s) are available in Cloud Pak for Data</p>"},{"location":"30-reference/configuration/cp4d-access-control/#provisioned-instance-authorization---cp4d_instance_configuration","title":"Provisioned instance authorization - <code>cp4d_instance_configuration</code>","text":"<p>When using Cloud Pak for Data LDAP connectivity and User Groups, the User Groups can be assigned to authorize the users of the LDAP groups access to the proviosioned instance(s).</p> <p>Currently supported instance authorization: - Cognos Analytics (ca)</p>"},{"location":"30-reference/configuration/cp4d-access-control/#cognos-analytics-instance-authorization","title":"Cognos Analytics instance authorization","text":"<pre><code>cp4d_instance_configuration:\n- project: zen-sample                # Mandatory\n  openshift_cluster_name: sample     # Mandatory\n  cartridges:\n  - name: cognos_analytics\n    manage_access:                                  # Optional, requires LDAP connectivity\n    - ca_role: Analytics Viewer                     # Mandatory, one the CA Access roles\n      cp4d_user_group: CA_Analytics_Viewer          # Mandatory, the CP4D User Group Name\n    - ca_role: Analytics Administrators             # Mandatory, one the CA Access roles\n      cp4d_user_group: CA_Analytics_Administrators  # Mandatory, the CP4D User Group Name\n</code></pre> <p>A Cognos Analytics (ca) instance can have multiple <code>manage_access</code> entries. Each entry consists of 1 <code>ca_role</code> and 1 <code>cp4d_user_group</code> element.  The <code>ca_role</code> must be one of the following possible values: - Analytics Administrators - Analytics Explorers - Analytics Users  - Analytics Viewer</p> <p>During the configuration of the instance authorization the following validations are performend: - LDAP configuration is completed - The provided <code>ca_role</code> is valid - The provided <code>cp4d_user_group</code> exists</p>"},{"location":"30-reference/configuration/cp4d-assets/","title":"Cloud Pak for Data Asset configuration","text":"<p>The Cloud Pak Deployer can implement demo assets and accelerators as part of the deployment process to standardize standing up fully-featured demo environments, or to test patches or new versions of the Cloud Pak using pre-defined assets.</p>"},{"location":"30-reference/configuration/cp4d-assets/#node-changes-for-roks-and-satellite-clusters","title":"Node changes for ROKS and Satellite clusters","text":"<p>If you put a script named <code>apply-custom-node-settings.sh</code> in the <code>CONFIG_DIR/assets</code> directory, it will be run as part of applying the node settings. This way you can override the existing node settings applied by the deployer or update the compute nodes with new settings. For more information regarding the <code>apply-custom-node-settings.sh</code> script, go to Prepare OpenShift cluster on IBM Cloud and IBM Cloud Satellite.</p>"},{"location":"30-reference/configuration/cp4d-assets/#cp4d_asset","title":"<code>cp4d_asset</code>","text":"<p>A <code>cp4d_asset</code> entry defines one or more assets to be deployed for a specific Cloud Pak for Data instance (OpenShift project). In the configuration, a directory relative to the configuration directory (<code>CONFIG_DIR</code>) is specified. For example, if the directory where the configuration is stored is <code>$HOME/cpd-config/sample</code> and you specify <code>assets</code> as the asset directory, all assets under <code>/cpd-config/sample/assets</code> are processed.</p> <p>You can create one or more subdirectories under the specified location, each holding an asset to be deployed. The deployer finds all <code>cp4d-asset.sh</code> scripts and <code>cp4d-asset.yaml</code> Ansible task files and runs them.</p> <p>The following runtime attributes will be set prior to running the shell script or the Ansible task: * If the Cloud Pak for Data resource is installed, <code>cpd-cli</code> is configured for the current Cloud Pak for Data instance and the current context is set to the CP4D admin user of the instance. This means you can run all <code>cpd-cli</code> commands without first having to login to Cloud Pak for Data. You can also manually create and set the contect by running the <code>$STATUS_DIR/cp4d/cpd-cli-&lt;project&gt;-&lt;cp4d-admin-user&gt;</code> script. * The current working directory is set to the directory holding the <code>cp4d-asset.sh</code> script. * When running the <code>cp4d-asset.sh</code> shell script, the following environment variables are available:     - <code>CP4D_URL</code>: Cloud Pak for Data URL     - <code>CP4D_ADMIN_PASSWORD</code>: Cloud Pak for Data admin password     - <code>CP4D_OCP_PROJECT</code>: OpenShift project that holds the Cloud Pak for Data instance     - <code>KUBECONFIG</code>: OpenShift configuration file that allows you to run <code>oc</code> commands for the cluster</p> <pre><code>cp4d_asset:\n- name: sample-asset\n  project: cpd\n  asset_location: cp4d-assets\n</code></pre>"},{"location":"30-reference/configuration/cp4d-assets/#property-explanation","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the asset to be deployed. You can specify as many assets as wanted Yes project Name of OpenShift project of the matching <code>cp4d</code> entry. The cp4d project must exist. Yes asset_location Directory holding the asset(s). This is a directory relative to the config directory (CONFIG_DIR) that was passed to the deployer Yes"},{"location":"30-reference/configuration/cp4d-assets/#asset-example","title":"Asset example","text":"<p>Below is an example asset that implements the Customer Attrition industry accelerator, which can be found here: https://github.com/IBM/Industry-Accelerators/blob/master/CPD%204.0.1.0/utilities-customer-attrition-prediction-industry-accelerator.tar.gz</p> <p>To implement:</p> <ul> <li>Download the zip file to the <code>cp4d-assets</code> directory in the specified configuration directory</li> <li>Create the <code>cp4d-asset.sh</code> shell script (example below)</li> <li>Add a <code>cp4d_asset</code> entry to the Cloud Pak for Data config file in the <code>config</code> directory (or in any other file with extention <code>.yaml</code>)</li> </ul> <p><code>cp4d-asset.sh</code> shell script: <pre><code>#!/bin/bash\nSCRIPT_DIR=$( cd \"$( dirname \"${BASH_SOURCE[0]}\" )\" &gt;/dev/null 2&gt;&amp;1 &amp;&amp; pwd )\n\n# Function to retrieve project by name\nfunction retrieve_project {\n    project_name=$1\n\n    # First check if project already exists\n    project_id=$(cpd-cli project list \\\n        --output json | \\\n        jq -r --arg project_name $project_name \\\n        'if .total_results==0 then \"\" else .resources[] | select(.entity.name == $project_name) | .metadata.guid end')\n\n    echo $project_id\n}\n\n# Function to create a project\nfunction create_project {\n    project_name=$1\n\n    retrieve_project $project_name\n\n    if [ \"$project_id\" != \"\" ];then\n        echo \"Project $project_name already exists\"\n        return\n    else\n        echo \"Creating project $project_name\"\n        storage_id=$(uuidgen)\n        storage=$(jq --arg storage_id $storage_id '. | .guid=$storage_id | .type=\"assetfiles\"' &lt;&lt;&lt; '{}')\n        cpd-cli project create --name $project_name --storage \"$storage\"\n    fi\n\n    # Find project_id to return\n    project_id=$(cpd-cli project list \\\n        --output json | \\\n        jq -r --arg project_name $project_name \\\n        'if .total_results==0 then \"\" else .resources[] | select(.entity.name == $project_name) | .metadata.guid end')\n}\n\n# Function to import a project\nfunction import_project {\n    project_id=$1\n    zip_file=$2\n    import_id=$(cpd-cli asset import start \\\n        --project-id $project_id --import-file $zip_file \\\n        --output json --jmes-query \"metadata.id\" --raw-output)\n    \n    cpd-cli asset import get --project-id $project_id --import-id $import_id --output json\n\n}\n\n# Function to run jobs\nfunction run_jobs {\n    project_id=$1\n    for job in $(cpd-cli job list --project-id $project_id \\\n        --output json | jq -r '.results[] | .metadata.asset_id');do\n        cpd-cli job run create --project-id $project_id --job-id $job --job-run \"{}\"\n    done\n}\n\n#\n# Start of the asset code\n#\n\n# Unpack the utilities-customer-attrition-prediction-industry-accelerator directory\nrm -rf /tmp/utilities-customer-attrition-prediction-industry-accelerator\ntar xzf utilities-customer-attrition-prediction-industry-accelerator.tar.gz -C /tmp\nasset_dir=/tmp/customer-attrition-prediction-industry-accelerator\n\n# Change to the asset directory\npushd ${asset_dir} &gt; /dev/null\n\n# Log on to Cloud Pak for Data with the admin user\ncp4d_token=$(curl -s -k -H 'Content-Type: application/json' -X POST $CP4D_URL/icp4d-api/v1/authorize -d '{\"username\": \"admin\", \"password\": \"'$CP4D_ADMIN_PASSWORD'\"}' | jq -r .token)\n\n# Import categories\ncurl -s -k -H 'accept: application/json' -H \"Authorization: Bearer ${cp4d_token}\" -H \"content-type: multipart/form-data\" -X POST $CP4D_URL/v3/governance_artifact_types/category/import?merge_option=all -F \"file=@./utilities-customer-attrition-prediction-glossary-categories.csv;type=text/csv\"\n\n# Import glossary terms\ncurl -s -k -H 'accept: application/json' -H \"Authorization: Bearer ${cp4d_token}\" -H \"content-type: multipart/form-data\" -X POST $CP4D_URL/v3/governance_artifact_types/glossary_term/import?merge_option=all -F \"file=@./utilities-customer-attrition-prediction-glossary-terms.csv;type=text/csv\"\n\n# Check if customer-attrition project already exists. If so, do nothing\nproject_id=$(retrieve_project \"customer-attrition\")\n\n# If project does not exist, import it and run jobs\nif [ \"$project_id\" == \"\" ];then\n    create_project \"customer-attrition\"\n    import_project $project_id \\\n        /tmp/utilities-customer-attrition-prediction-industry-accelerator/utilities-customer-attrition-prediction-analytics-project.zip\n    run_jobs $project_id\nelse\n    echo \"Skipping deployment of CP4D asset, project customer-attrition already exists\"\nfi\n\n# Return to original directory\npopd &gt; /dev/null\n\nexit 0\n</code></pre></p>"},{"location":"30-reference/configuration/cp4d-cartridges/","title":"Cloud Pak for Data cartridges","text":"<p>Defines the services (cartridges) which must be installed into the Cloud Pak for Data instances. The cartridges will be configured with the storage class defined at the Cloud Pak for Data object level. For each cartridge you can specify whether it must be installed or removed by specifying the state. If a cartridge is installed and the state is changed to <code>removed</code>, the cartridge and all of its instances are removed by the deployer when it is run.</p> <p>An example Cloud Pak for Data object with cartridges is below: <pre><code>cp4d:\n- project: cpd-instance\n  cp4d_version: 4.8.3\n\n  cartridges:\n  - name: cpfs\n\n  - name: cpd_platform\n\n  - name: db2oltp\n    size: small\n    instances:\n    - name: db2-instance\n      metadata_size_gb: 20\n      data_size_gb: 20\n      backup_size_gb: 20\n      transactionlog_size_gb: 20\n    state: installed\n\n  - name: wkc\n    size: small\n    state: removed\n\n  - name: wml\n    size: small\n    state: installed\n\n  - name: ws\n    state: installed\n</code></pre></p> <p>When run, the deployer installs the Db2 OLTP (<code>db2oltp</code>), Watson Machine Learning (<code>wml</code>) and Watson Studio (<code>ws</code>) cartridges. If the Watson Knowledge Catalog (<code>wkc</code>) is installed in the <code>cpd-instance</code> OpenShift project, it is removed.</p> <p>After the deployer installs Db2 OLTP, a new Db2 instance is created with the specified attributes.</p>"},{"location":"30-reference/configuration/cp4d-cartridges/#cloud-pak-for-data-cartridges_1","title":"Cloud Pak for Data cartridges","text":""},{"location":"30-reference/configuration/cp4d-cartridges/#cp4dcartridges","title":"<code>cp4d.cartridges</code>","text":"<p>This is a list of cartridges that will be installed in the Cloud Pak for Data instance. Every cartridge is identified by its name.</p> <p>Some cartridges may require additional information to correctly install or to create an instance for the cartridge. Below you will find a list of all tested Cloud Pak for Data cartridges and their specific properties.</p>"},{"location":"30-reference/configuration/cp4d-cartridges/#properties-for-all-cartridges","title":"Properties for all cartridges","text":"Property Description Mandatory Allowed values name Name of the cartridge Yes state Whether the cartridge must be <code>installed</code> or <code>removed</code>. If not specified, the cartridge will be installed No installed, removed installation_options Record of properties that will be applied to the <code>spec</code> of the OpenShift Custom Resource No"},{"location":"30-reference/configuration/cp4d-cartridges/#cartridge-cpfs-or-cp-foundation","title":"Cartridge <code>cpfs</code> or <code>cp-foundation</code>","text":"<p>Defines the Cloud Pak Foundational Services (fka Common Services) which are required for all Cloud Pak for Data installations. Cloud Pak for Data Foundational Services provide functionalities around identity and access management (IAM) and other common services.</p> <p>This cartridge is mandatory for every Cloud Pak for Data and watsonx instance.</p>"},{"location":"30-reference/configuration/cp4d-cartridges/#additional-properties-for-cartridge-cp-foundation","title":"Additional properties for cartridge <code>cp-foundation</code>","text":"Property Description Mandatory Allowed values scale Scale configuration of Foundational Services. If the property is not provided, the scale will not be changed No level_1, level_2, level_3, level_4, level_5 license_service Properties that will be applied to the IBM license service .threads_per_core Specify the threads per core (hyperthreading) that the license service must use to calculate usage No 1 (default), numeric value"},{"location":"30-reference/configuration/cp4d-cartridges/#cartridge-cpd_platform-or-lite","title":"Cartridge <code>cpd_platform</code> or <code>lite</code>","text":"<p>Defines the Cloud Pak for Data platform operator (fka \"lite\") which installs the base services needed to operate Cloud Pak for Data, such as the Zen metastore, Zen watchdog and the user interface.</p> <p>This cartridge is mandatory for every Cloud Pak for Data instance.</p>"},{"location":"30-reference/configuration/cp4d-cartridges/#cartridge-wkc","title":"Cartridge <code>wkc</code>","text":"<p>Manages the Watson Knowledge Catalog installation for the Cloud Pak for Data instance.</p>"},{"location":"30-reference/configuration/cp4d-cartridges/#additional-properties-for-cartridge-wkc","title":"Additional properties for cartridge <code>wkc</code>","text":"Property Description Mandatory Allowed values size Scale configuration of the cartridge No small (default), medium, large installation_options.install_wkc_core_only Install only the core of WKC? No True, False (default) installation_options.enableKnowledgeGraph Enable the knowledge graph for business lineage? No True, False (default) installation_options.enableDataQuality Enable data quality for WKC? No True, False (default) installation_options.enableMANTA Enable MANTA? No True, False (default)"},{"location":"30-reference/configuration/cp4d-connections/","title":"Cloud Pak for Data platform connections","text":""},{"location":"30-reference/configuration/cp4d-connections/#cloud-pak-for-data-platform-connection---cp4d_conection","title":"Cloud Pak for Data platform connection - <code>cp4d_conection</code>","text":"<p>The <code>cp4d_connection</code> object can be used to create Global Platform connections.</p> <pre><code>cp4d_connection:\n- name: connection_name                                 # Name of the connection, must be unique\n  type: database                                        # Type, currently supported: [database]\n  cp4d_instance: cpd                                    # CP4D instance on which the connection must be created\n  openshift_cluster_name: cluster_name                  # OpenShift cluster name on which the cp4d_instance is deployed\n  database_type: db2                                    # Type of connection\n  database_hostname: hostname                           # Hostname of the connection\n  database_port: 30556                                  # Port of the connection\n  database_name: bludb                                  # Database name of the connection\n  database_port_ssl: true                               # enable ssl flag\n  database_credentials_username: 77066f69               # Username of the datasource\n  database_credentials_password_secret: db-credentials  # Vault lookup name to contain the password\n  database_ssl_certificate_secret: db-ssl-cert          # Vault lookup name to contain the SSL certificate\n</code></pre>"},{"location":"30-reference/configuration/cp4d-connections/#cloud-pak-for-data-backup-and-restore-platform-connections---cp4d_backup_restore_connections","title":"Cloud Pak for Data backup and restore platform connections - <code>cp4d_backup_restore_connections</code>","text":"<p>The <code>cp4d_backup_restore_connections</code> can be used to backup all current configured Global Platform connections, which are either created by the Cloud Pak Deployer or added manually. The backup is stored in the <code>status</code>/cp4d/exports folder as a json file. </p> <p>A backup file can be used to restore global platform connections. A flag can be used to indicate whether if a Global Platform connection with the same name already exists, the restore is skipped.</p> <p>Using the Cloud Pak Deployer cp4d_backup_restore_connections capability implements the following: - Connect to the IBM Cloud Pak for Data instance specified using <code>cp4d_instance</code> and <code>openshift_cluster_name</code> - If <code>connections_backup_file</code> is specified export all Global Platform connections to the specified file in the <code>status</code>/cp4d/export/connections folder - If <code>connections_restore_file</code> is specified, load the file and restore the Global Platform connections - The <code>connections_restore_overwrite</code> (true/false) indicates whether if a Global Platform Connection with the same already exists, it will be replaced.</p> <pre><code>cp4d_backup_restore_connections:\n- cp4d_instance: cpd\n  openshift_cluster_name: {{ env_id }}\n  connections_backup_file: {{ env_id }}_cpd_connections.json\n  connections_restore_file: {{ env_id }}_cpd_connection.json\n  connections_restore_overwrite: false\n</code></pre>"},{"location":"30-reference/configuration/cp4d-instances/","title":"Cloud Pak for Data instances","text":""},{"location":"30-reference/configuration/cp4d-instances/#manage-cloud-pak-for-data-instances","title":"Manage cloud Pak for Data instances","text":"<p>Some cartridges have the ability to create one or more instances to run an isolated installation of the cartridge. If instances have been configured for the cartridge, the deployer can manage creating and deleting the instances.</p> <p>The following Cloud Pak for Data cartridges are currently supported for managing instances:</p> <ul> <li>Analytics engine powered by Apache Spark (<code>analytics-engine</code>)</li> <li>DataStage (<code>datastage-ent-plus</code>) </li> <li>Db2 OLTP (<code>db2</code>)</li> <li>Data Virtualization (<code>dv</code>)</li> <li>Cognos Analytics (<code>ca</code>)</li> <li>EDB Postgres (<code>edb_cp4d</code>)</li> <li>OpenPabes (<code>openpages</code>)</li> </ul>"},{"location":"30-reference/configuration/cp4d-instances/#analytics-engine-powered-by-apache-spark-instances","title":"Analytics engine powered by Apache Spark Instances","text":"<p>Analytics Engine instances can be defined by adding the <code>instances</code> section to the <code>cartridges</code> entry of cartridge <code>analytics-engine</code>. The following example shows the configuration to define an instance.</p> <pre><code>cp4d:\n- project: cpd-instance\n  openshift_cluster_name: \"{{ env_id }}\"\n...\n  cartridges:\n  - name: analytics-engine\n    size: small\n    state: installed\n    instances:\n    - name: analyticsengine-instance\n      storage_size_gb: 50\n</code></pre> Property Description Mandatory Allowed Values name Name of the instance Yes storage_size_db Size of the storage allocated to the instance Yes numeric value"},{"location":"30-reference/configuration/cp4d-instances/#datastage-instances","title":"DataStage instances","text":"<p>DataStage instances can be defined by adding the <code>instances</code> section to the <code>cartridges</code> entry of cartridge <code>datastage-ent-plus</code>. The following example shows the configuration to define an instance.</p> <p>DataStage, upon deployment, always creates a default instance called <code>ds-px-default</code>. This instance cannot be configured in the <code>instances</code> section. </p> <pre><code>cp4d:\n- project: cpd-instance\n  openshift_cluster_name: \"{{ env_id }}\"\n...\n  cartridges:\n  - name: datastage-ent-plus\n    state: installed\n\n    instances:\n    - name: ds-instance\n      # Optional settings\n      description: \"datastage ds-instance\"\n      size: medium\n      storage_class: efs-nfs-client\n      storage_size_gb: 60\n      # Optional Custom Scale options\n      scale_px_runtime:\n        replicas: 2\n        cpu_request: 500m\n        cpu_limit: 2\n        memory_request: 2Gi\n        memory_limit: 4Gi\n      scale_px_compute:\n        replicas: 2\n        cpu_request: 1\n        cpu_limit: 3\n        memory_request: 4Gi\n        memory_limit: 12Gi   \n</code></pre> Property Description Mandatory Allowed Values name Name of the instance Yes description Description of the instance No size Size of the DataStage instance No small (default), medium, large storage_class Override the default storage class No storage_size_gb Storage size allocated to the DataStage instance No numeric <p>Optionally, the default px_runtime and px_compute instances of the DataStage instance can be tweaked. Both <code>scale_px_runtime</code> and <code>scale_px_compute</code> must be specified when used, and all properties must be specified.</p> Property Description Mandatory replicas Number of replicas Yes cpu_request CPU Request value Yes memory_request Memory Request value Yes cpu_limit CPU limit value Yes memory_limit Memory limit value Yes"},{"location":"30-reference/configuration/cp4d-instances/#db2-oltp-instances","title":"Db2 OLTP Instances","text":"<p>DB2 OLTP instances can be defined by adding the <code>instances</code> section to the <code>cartridges</code> entry of cartridge <code>db2</code>. The following example shows the configuration to define an instance.</p> <pre><code>cp4d:\n- project: cpd-instance\n  openshift_cluster_name: \"{{ env_id }}\"\n...\n  cartridges:\n  - name: db2\n    size: small\n    state: installed\n    instances:\n    - name: db2 instance\n      metadata_size_gb: 20\n      data_size_gb: 20\n      backup_size_gb: 20  \n      transactionlog_size_gb: 20\n    \n</code></pre> Property Description Mandatory Allowed Values name Name of the instance Yes metadata_size_gb Size of the metadata store Yes numeric value data_size_gb Size of the data store Yes numeric value backup_size_gb Size of the backup store Yes numeric value transactionlog_size_gb Size of the transactionlog store Yes numeric value"},{"location":"30-reference/configuration/cp4d-instances/#data-virtualization-instances","title":"Data Virtualization Instances","text":"<p>Data Virtualization instances can be defined by adding the <code>instances</code> section to the <code>cartridges</code> entry of cartridge <code>dv</code>. The following example shows the configuration to define an instance.</p> <pre><code>cp4d:\n- project: cpd-instance\n  openshift_cluster_name: \"{{ env_id }}\"\n...\n  cartridges:\n  - name: dv\n    size: small\n    state: installed\n    instances:\n    - name: data-virtualization\n</code></pre> Property Description Mandatory Allowed Values name Name of the instance Yes"},{"location":"30-reference/configuration/cp4d-instances/#cognos-analytics-instance","title":"Cognos Analytics Instance","text":"<p>A Cognos Analytics instance can be defined by adding the <code>instances</code> section to the <code>cartridges</code> entry of cartridge <code>ca</code>. The following example shows the configuration to define an instance.</p> <pre><code>cp4d:\n- project: cpd-instance\n  openshift_cluster_name: \"{{ env_id }}\"\n...\n  cartridges:\n  - name: ca\n    size: small\n    state: installed\n    instances:\n    - name: ca-instance\n      metastore_ref: ca-metastore\n</code></pre> Property Description Mandatory name Name of the instance Yes metastore_ref Name of the DB2 instance used for the Cognos Repository database Yes <p>The Cognos Content Repository database can use an IBM Cloud Pak for Data DB2 OLTP instance. The Cloud Pak Deployer will first determine whether an existing DB2 OLTP existing with the name specified <code>metastore_ref</code>. If this is the case, this DB2 OLTP instance will be used and the database is prepared using the Cognos DB2 script prior to provisioning the Cognos instance.</p>"},{"location":"30-reference/configuration/cp4d-instances/#edb-postgres-for-cloud-pak-for-data-instances","title":"EDB Postgres for Cloud Pak for Data instances","text":"<p>EnterpriseDB instances can be defined by adding the <code>instances</code> section to the <code>cartridges</code> entry of cartridge <code>dv</code>. The following example shows the configuration to define an instance.</p> <pre><code>cp4d:\n- project: cpd-instance\n  openshift_cluster_name: \"{{ env_id }}\"\n...\n  cartridges:\n\n  # Please note that for EDB Postgress, a secret edb-postgres-license-key must be created in the vault\n  # before deploying\n  - name: edb_cp4d\n    size: small\n    state: installed\n    instances:\n    - name: instance1\n      version: \"13.5\"\n      #Optional Parameters\n      type: Standard\n      members: 1\n      size_gb: 50\n      resource_request_cpu: 1000m\n      resource_request_memory: 4Gi\n      resource_limit_cpu: 1000m\n      resource_limit_memory: 4Gi\n</code></pre> Property Description Mandatory Allowed Values name Name of the instance Yes version Version of the EDB PostGres instance Yes 12.11, 13.5 type Enterprise or Standard version No Standard (default), Enterprise members Number of members of the instance No number, 1 (default) size_gb Storage Size allocated to the instance No number, 50 (default) resource_request_cpu Request CPU of the instance No 1000m (default) resource_request_memory Request Memory of the instance No 4Gi (default) resource_limit_cpu Limit CPU of the instance No 1000m (default) resource_limit_memory Limit Memory of the instance No 4Gi (default)"},{"location":"30-reference/configuration/cp4d-instances/#openpages-instance","title":"OpenPages Instance","text":"<p>An OpenPages instance can be defined by adding the <code>instances</code> section to the <code>cartridges</code> entry of cartridge <code>openpages</code>. The following example shows the configuration to define an instance.</p> <pre><code>cp4d:\n- project: cpd-instance\n  openshift_cluster_name: \"{{ env_id }}\"\n...\n  cartridges:\n  - name: openpages\n    state: installed\n    instances:\n    - name: openpages-instance\n      size: xsmall\n</code></pre> Property Description Mandatory name Name of the instance Yes size The size of the OpenPages instances, default is xsmall No"},{"location":"30-reference/configuration/cp4d-saml/","title":"Cloud Pak for Data SAML configuration","text":"<p>You can configure Single Sign-on (SSO) by specifying a SAML server for the Cloud Pak for Data instance, which will take care of authenticating users. SAML configuration can be used in combination with the Cloud Pak for Data LDAP configuration, in which case LDAP complements the identity with access management (groups) for users.</p>"},{"location":"30-reference/configuration/cp4d-saml/#saml-configuration---cp4d_saml_config","title":"SAML configuration - <code>cp4d_saml_config</code>","text":"<p>An <code>cp4d_saml_config</code> entry holds connection information, certificates and field configuration that is needed in the exchange between Cloud Pak for Data user management and the identity provider (idP). The entry must created for every Cloud Pak for Data project that requires SAML authentication.</p> <p>When a <code>cp4d_saml_config</code> entry exists for a certain <code>cp4d</code> project, the user management pods are updated with a <code>samlConfig.json</code> file and then restarted. If an entry is removed later, the file is removed and the pods restarted again. When no changes are needed, the file in the pod is left untouched and no restart takes place.</p> <p>For more information regarding the Cloud Pak for Data SAML configuration, check the single sign-on documentation: https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=client-configuring-sso</p> <pre><code>cp4d_saml_config:\n- project: cpd\n  entrypoint: \"https://prepiam.ice.ibmcloud.com/saml/sps/saml20ip/saml20/login\"\n  field_to_authenticate: email\n  sp_cert_secret: {{ env_id }}-cpd-sp-cert\n  idp_cert_secret: {{ env_id }}-cpd-idp-cert\n  issuer: \"cp4d\"\n  identifier_format: \"\"\n  callback_url: \"\"\n</code></pre> <p>The above configuration uses the IBM preproduction IAM server to delegate authentication to and authentication is done via the user's e-mail address. An issuer must be configured in the identity provider (idP) and the idP's certificate must be kept in the vault so Cloud Pak for Data can confirm its identity.</p>"},{"location":"30-reference/configuration/cp4d-saml/#property-explanation","title":"Property explanation","text":"Property Description Mandatory Allowed values project Name of OpenShift project of the matching <code>cp4d</code> entry. The cp4d project must exist. Yes entrypoint URL of the identity provider (idP) login page Yes field_to_authenticate Name of the parameter to authenticate with the idP Yes sp_cert_secret Vault secret that holds the private certificate to authenticate to the idP. If not specified, requests will not be signed. No idp_cert_secret Vault secret that holds the public certificate of the idP. This confirms the identity of the idP Yes issuer The name you chose to register the Cloud Pak for Data instance with your idP Yes identifier_format Format of the requests from Cloud Pak for Data to the idP. If not specified, <code>urn:oasis:names:tc:SAML:1.1:nameid-format:emailAddress</code> is used No callback_url Specify the callback URL if you want to override the default of cp4d_url<code>/auth/login/sso/callback</code> No <p>The <code>callbackUrl</code> field in the <code>samlConfig.json</code> file is automatically populated by the deployer if it is not specified by the <code>cp4d_saml_config</code> entry. It then consists of the Cloud Pak for Data base URL appended with <code>/auth/login/sso/callback</code>.</p> <p>Before running the deployer with SAML configuration, ensure that the secret configured for <code>idp_cert_secret</code> exists in the vault. Check Vault configuration for instructions on adding secrets to the vault.</p>"},{"location":"30-reference/configuration/cpd-global-config/","title":"Global configuration for Cloud Pak Deployer","text":""},{"location":"30-reference/configuration/cpd-global-config/#global_config","title":"global_config","text":"<p>Cloud Pak Deployer can use properties set in the global configuration (<code>global_config</code>) during the deployment process and also as substitution variables in the configuration, such as <code>{{ env_id}}</code> and <code>{{ ibm_cloud_region }}</code>. </p> <p>The following <code>global_config</code> variables are automatically copied into a \"simple\" form so they can be referenced in the configuration file(s) and also overridden using the command line.</p> Variable name Description <code>environment_name</code> Name used to group secrets, typically you will specify <code>sample</code> <code>cloud_platform</code> Cloud platform applicable to configuration, such as <code>ibm-cloud</code>, <code>aws</code>, <code>azure</code> <code>env_id</code> Environment ID used in various other configuration objects <code>ibm_cloud_region</code> When Cloud Platform is <code>ibm-cloud</code>, the region into which the ROKS cluster is deployed <code>aws_region</code> When Cloud Platform is <code>aws</code>, the region into which the ROSA/self-managed OpenShift cluster is deployed <code>azure_location</code> When Cloud Platform is <code>azure</code>, the region into which the ARO OpenShift cluster is deployed <code>universal_admin_user</code> User name to be used for admin user (currently not used) <code>universal_password</code> Password to be used for all (admin) users it not specified in the vault <code>confirm_destroy</code> Is destroying of clusters, services/cartridges and instances allowed? } <code>optimize_deploy</code> Optimize deployment by skipping components already installed with the correct version? <p>For all other variables, you can refer to the qualified form, for example: <code>\"{{ global_config.division }}\"</code></p> <p>Sample global configuration: <pre><code>global_config:\n  environment_name: sample\n  cloud_platform: ibm-cloud\n  env_id: pluto-01\n  ibm_cloud_region: eu-de\n  universal_password: very_secure_Passw0rd$\n  confirm_destroy: False\n  optimize_deploy: True\n</code></pre></p> <p>If you run the <code>cp-deploy.sh</code> command and specify <code>-e env_id=jupiter-03</code>, this will override the value in the <code>global_config</code> object. The same applies to the other variables.</p>"},{"location":"30-reference/configuration/cpd-objects/","title":"Configuration objects","text":"<p>All objects used by the Cloud Pak Deployer are defined in a yaml format in files in the <code>config</code> directory. You can create a single yaml file holding all objects, or group objects in individual yaml files. At deployment time, all yaml files in the <code>config</code> directory are merged.</p> <p>To make it easier to navigate the different object types, they have been groups in different tabs. You can also use the index below to find the definitions.</p>"},{"location":"30-reference/configuration/cpd-objects/#configuration","title":"Configuration","text":"<ul> <li>Global configuration</li> <li>Vault configuration</li> </ul>"},{"location":"30-reference/configuration/cpd-objects/#infrastructure","title":"Infrastructure","text":"<ul> <li>Infrastructure objects</li> <li>Provider</li> <li>Resource groups</li> <li>Virtual Private Clouds (VPCs)</li> <li>Security groups</li> <li>Security rules</li> <li>Address prefixes</li> <li>Subnets</li> <li>Floating ips</li> <li>Virtual Server Instances (VSIs)</li> <li>NFS Servers</li> <li>SSH keys</li> <li>Transit Gateways</li> </ul>"},{"location":"30-reference/configuration/cpd-objects/#openshift-object-types","title":"OpenShift object types","text":"<ul> <li>Existing OpenShift</li> <li>OpenShift on IBM Cloud</li> <li>OpenShift on AWS - ROSA</li> <li>OpenShift on AWS - self-managed</li> <li>OpenShift on Microsoft Azure (ARO)</li> <li>OpenShift on vSphere</li> </ul>"},{"location":"30-reference/configuration/cpd-objects/#cloud-paks-and-related-object-types","title":"Cloud Paks and related object types","text":"<ul> <li>Cloud Pak for Data - cp4d</li> <li>Cloud Pak for Integration - cp4d</li> <li>Cloud Pak for Watson AIOps - cp4d</li> <li>Private registry</li> </ul>"},{"location":"30-reference/configuration/cpd-objects/#cloud-pak-for-data-cartridges-object-types","title":"Cloud Pak for Data Cartridges object types","text":"<ul> <li>Cloud Pak for Data Control Plane - cpd_platform</li> <li>Cloud Pak for Data Cognos Analytics - ca</li> <li>Cloud Pak for Data Db2 OLTP - db2oltp</li> <li>Cloud Pak for Data Watson Studio - ws</li> <li>Cloud Pak for Data Watson Machine Learning - wml</li> </ul>"},{"location":"30-reference/configuration/demo-openldap/","title":"OpenLDAP configuration (for demonstration purposes only)","text":"<p>You can install an OpenLDAP service on your OpenShift cluster for demonstration and testing purposes. This way you can experiment with LDAP identity providers in Foundational Services if you don't (yet) have access to an enterprise-ready LDAP service in the organization's infrastructure services.</p> <p>Note Installing an OpenLDAP server must only be done if you have unrestricted OpenShift Container Platform entitlements. When using the Cloud Pak entitlements for Red Hat OpenShift, installing third-party applications like Bitnami OpenLDAP is not allowed.</p>"},{"location":"30-reference/configuration/demo-openldap/#demonstration-openldap-configuration---demo_openldap","title":"Demonstration OpenLDAP configuration - <code>demo_openldap</code>","text":"<p>A <code>demo_openldap</code> resource in the configuration indicates that the Bitname OpenLDAP service is installed on the specified OpenShift cluster. The default OpenShift poject for the OpenLDAP service is <code>openldap</code>. You can install several instances on the same OpenShift cluster if necessary, each with its own name and <code>openldap_project</code> project.</p> <p>Sample configuration <pre><code>demo_openldap:\n- name: cp4d-openldap\n  openshift_cluster_name: \"{{ env_id }}\"\n  openldap_project: openldap\n  ldap_config:\n    ldap_tls: True\n    bind_admin_user: cn=admin,dc=cp,dc=internal\n    base_dn: dc=cp,dc=internal\n    base_dc: cp\n    base_domain: cp.internal\n    user_ou: Users\n    user_id_attribute: uid\n    user_base_dn: ou=Users,dc=cp,dc=internal\n    user_object_class: inetOrgPerson\n    group_ou: Groups\n    group_id_attribute: cn\n    group_base_dn: ou=Groups,dc=cp,dc=internal\n    group_object_class: groupOfUniqueNames\n    group_member_attribute: uniqueMember\n  users:\n  - uid: ttoussaint\n    givenName: Tara\n    sn: Toussaint\n    mail: ttoussaint@cp.internal\n  - uid: rramones\n    givenName: Rosa\n    sn: Ramones\n    mail: rramones@cp.internal\n    # password: specific_password_for_the_user\n  - uid: ssharpe\n    givenName: Shelly\n    sn: Sharpe\n    mail: ssharpe@cp.internal\n    # password: specific_password_for_the_user\n  - uid: pprimo\n    givenName: Paco\n    sn: Primo\n    mail: pprimo@cp.internal\n    # password: specific_password_for_the_user\n  - uid: rroller\n    givenName: Rico\n    sn: Roller\n    mail: rroller@cp.internal\n    # password: specific_password_for_the_user\n  groups:\n  - cn: cp4d-admins\n    members:\n    - uid=ttoussaint,ou=Users,dc=cp,dc=internal\n  - cn: cp4d-data-engineers\n    members:\n    - uid=rramones,ou=Users,dc=cp,dc=internal\n    - uid=ssharpe,ou=Users,dc=cp,dc=internal\n  - cn: cp4d-data-scientists\n    members:\n    - uid=pprimo,ou=Users,dc=cp,dc=internal\n    - uid=ssharpe,ou=Users,dc=cp,dc=internal\n    - uid=rroller,ou=Users,dc=cp,dc=internal\n  state: installed\n</code></pre></p> <p>The above configuration installs the OpenLDAP service in OpenShift project <code>openldap</code> and configures it for domain <code>cp.internal</code>. Subsequently, an LDIF file with the Organization Units, Groups and Users is generated and then the OpenLDAP service is started. </p> <p>The OpenLDAP name is referenced in the Cloud Pak for Data Access Control resource and this is also where the mapping from LDAP groups to Cloud Pak for Data groups takes place. </p>"},{"location":"30-reference/configuration/demo-openldap/#property-explanation","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the OpenLDAP server, for reference by <code>zen_access_control</code> Yes openshift_cluster_name Name of OpenShift cluster into which the OpenLDAP service is installed Yes. if more than 1 <code>openshift</code> resource in the configuration openldap_project OpenShift project into which the OpenLDAP server is installed No, default is <code>openldap</code> ldap_config LDAP configuration Yes .ldap_tls Set to True if the LDAPS protocol just be used to communicate with the LDAP server No False (default), True .bind_admin_user Distinguished name of the user to bind (login) to the LDAP server Yes .base_dn Base domain name, specify through <code>dc</code> components Yes .base_dc First <code>dc</code> component in the <code>base_dn</code> Yes .base_domain Base domain of the LDAP root, specified as <code>cp.internal</code> Yes .user_ou Organizational Unit of users, typically <code>Users</code> Yes .user_id_attribute Attribute used to identify user, typically <code>uid</code> Yes .user_base_dn Base domain name of users, typically <code>user_ou</code>, followed by <code>base_dn</code> Yes .user_object_class Object class of the users, typically <code>inetOrgPerson</code> Yes .group_ou Organizational Unit of groups, typically <code>Groups</code> Yes .group_id_attribute Attribte used to idenfity group, typically <code>cn</code> Yes .group_base_dn Base domain name of groups, typically <code>group_ou</code>, followed by <code>base_dn</code> Yes .group_object_class Object class of the gruops, typically <code>groupOfUniqueNames</code> Yes .group_member_attribute Attribute used for a member (user) of a group, typically <code>uniqueMember</code> Yes users[] List of users to be added to the LDAP configuration Yes .uid User identifier that is used to login to the platform Yes .givenName First name of the user Yes .sn Surname of the user Yes .mail e-mail address of the user Yes .password Password to be assigned to the user. If not specified, the universal password is used No groups[] List of groups to be added to the LDAP configuration Yes .cn Group identifier, together with the <code>group_ou</code> and <code>base_dn</code>, this will define the group to map to the Cloud Pak group(s) Yes .members[] List of user distinguished names to be added as members to the group Yes state Indicates whether or nog OpenLDAP must be installed Yes <code>installed</code>, <code>removed</code>"},{"location":"30-reference/configuration/dns/","title":"Upstream DNS servers for OpenShift","text":"<p>When deploying OpenShift in a private network, one may want to reach additional private network services by their host name. Examples could be a database server, Hadoop cluster or an LDAP server.  OpenShift provides a DNS operator which deploys and manages CoreDNS which takes care of name resolution for pods running inside the container platform, also known as DNS forwarding.</p> <p>If the services that need to be reachable our registered on public DNS servers, you typically do not have to configure upstream DNS servers.</p> <p>The upstream DNS used for a particular OpenShift cluster is configured like this: <pre><code>openshift:\n- name: sample\n...\n  upstream_dns:\n  - name: sample-dns\n    zones:\n    - example.com\n    dns_servers:\n    - 172.31.2.73:53\n</code></pre></p> <p>The zones which have been defined for each of the upstream_dns configurations control which DNS server(s) will be used for name resolution. For example, if <code>example.com</code> is given as the zone and an upstream DNS server of <code>172.31.2.73:53</code>, any host name matching <code>*.example.com</code> will be resolved using DNS server <code>172.31.2.73</code> and port <code>53</code>.</p> <p>If you want to remove the upstream DNS that was previously configured, you can change the deployer configuration as below and run the deployer. Removing the <code>upstream_dns</code> element altogether will not make changes to the OpenShift DNS operator.</p> <pre><code>  upstream_dns: []\n</code></pre> <p>See https://docs.openshift.com/container-platform/4.8/networking/dns-operator.html for more information about the operator that is configured by specifying upstream DNS servers.</p>"},{"location":"30-reference/configuration/dns/#property-explanation","title":"Property explanation","text":"Property Description Mandatory Allowed values upstream_dns[] List of alternative upstream DNS servers(s) for OpenShift No name Name of the upstream DNS entry Yes zones Specification of one or more zone for which the DNS server is applicable Yes dns_servers One or more DNS servers (host:port) that will resolve host names in the specified zone Yes"},{"location":"30-reference/configuration/ibm-storage/","title":"Configuring IBM Storage for OpenShift","text":"<p>You can configure IBM Storage Fusion for OpenShift using Cloud Pak Deployer.</p> <p>IBM Storage for a particular OpenShift cluster is configured like this: <pre><code>---\nibm_storage:\n- openshift_cluster_name: \"{{ env_id }}\"\n  backup_restore:\n    install: True\n</code></pre></p>"},{"location":"30-reference/configuration/ibm-storage/#property-explanation","title":"Property explanation","text":"Property Description Mandatory Allowed values ibm_storage[] List IBM Storage definitions, one for each OpenShift cluster No openshift_cluster_name Name of the OpenShift cluster. If not specified and there is only one OpenShift cluster, it will be selected automatically No backup_restore Specification of the Backup &amp; Restore component No .install Select whether or not to install the Backup &amp; Restore component No False (default), True"},{"location":"30-reference/configuration/infrastructure/","title":"Infrastructure","text":"<p>For some of the cloud platforms, you must explicitly specify the infrastructure layer on which the OpenShift cluster(s) will be provisioned, or you can override the defaults. </p> <p>For IBM Cloud, you can configure the VPC, subnets, NFS server(s), other Virtual Server Instance(s) and a number of other objects. When provisioning OpenShift on vSphere, you can configure data center, data store, network and virtual machine definitions. For Azure ARO you configure a single object with information about the virtual network (vnet) to be used and the node server profiles. When deploying OpenShift on AWS you can specify an EFS server if you want to use elastic storage.</p> <p>This page lists all the objects you can configure for each of the supported cloud providers. - IBM Cloud - Microsoft Azure - Amazon AWS - vSphere</p>"},{"location":"30-reference/configuration/infrastructure/#ibm-cloud","title":"IBM Cloud","text":"<p>For IBM Cloud, the following object types are supported:</p> <ul> <li>provider</li> <li>resource_group</li> <li>ssh_keys</li> <li>address_prefix</li> <li>subnet</li> <li>network_acl</li> <li>security_group</li> <li>vsi</li> <li>transit_gateway</li> <li>nfs_server</li> <li>serviceid</li> <li>cos</li> </ul>"},{"location":"30-reference/configuration/infrastructure/#ibm-cloud-provider","title":"IBM Cloud <code>provider</code>","text":"<p>Defines the provider that Terraform will use for managing the IBM Cloud assets.</p> <pre><code>provider:\n- name: ibm\n  region: eu-de\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the provider cluster No ibm region Region to connect to Yes Any IBM Cloud region"},{"location":"30-reference/configuration/infrastructure/#ibm-cloud-resource_group","title":"IBM Cloud <code>resource_group</code>","text":"<p>The resource group is for cloud asset grouping purposes. You can define multiple resource groups in your IBM cloud account to group the provisioned assets. If you do not need to group your assets, choose <code>default</code>.</p> <pre><code>resource_group:\n- name: default\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_1","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the existing resource group Yes"},{"location":"30-reference/configuration/infrastructure/#ibm-cloud-ssh_keys","title":"IBM Cloud <code>ssh_keys</code>","text":"<p>SSH keys to connect to VSIs. If you have Virtual Server Instances in your VPC, you will need an SSH key to connect to them. SSH keys defined here will be looked up in the vault and created if they don't exist already.</p> <pre><code>ssh_keys:\n- name: vsi-access\n  managed: True\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_2","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the SSH key in IBM Cloud Yes managed Determines if the SSH key will be created if it doesn't exist No True (default), False"},{"location":"30-reference/configuration/infrastructure/#ibm-cloud-security_rule","title":"IBM Cloud <code>security_rule</code>","text":"<p>Defines the services (or ports) which are allowed within the context of a VPC and/or VSI.</p> <pre><code>security_rule:\n- name: https\n  tcp: {port_min: 443, port_max: 443}\n- name: ssh\n  tcp: {port_min: 22, port_max: 22}\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_3","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the security rule Yes tcp Range of tcp ports (<code>port_min</code> and <code>port_max</code>) to allow No 1-65535 udp Range of udp ports (<code>port_min</code> and <code>port_max</code>) to allow No 1-65535 icmp ICMP Type and Code for IPv4 (<code>code</code> and <code>type</code>) to allow No 1-255 for code, 1-254 for type"},{"location":"30-reference/configuration/infrastructure/#ibm-cloud-vpc","title":"IBM Cloud <code>vpc</code>","text":"<p>Defines the virtual private cloud which groups the provisioned objects (including VSIs and OpenShift cluster).</p> <pre><code>vpc:\n- name: sample\n  allow_inbound: ['ssh', 'https']\n  classic_access: false\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_4","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the Virtual Private Cloud Yes managed Controls whether the VPC is managed. The default is <code>True</code>. Only set to <code>False</code> if the VPC is not managed but only referenced by other objects such as transit gateways. No True (default), False allow_inbound Security rules which are allowed for inbound traffic No Existing <code>security_rule</code> classic_access Connect VPC to IBM Cloud classic infratructure resources No false (default), true"},{"location":"30-reference/configuration/infrastructure/#ibm-cloud-address_prefix","title":"IBM Cloud <code>address_prefix</code>","text":"<p>Defines the zones used within the VPC, along with the subnet the addresses will be issued for.</p> <pre><code>- name: sample-zone-1\n  vpc: sample\n  zone: eu-de-1\n  cidr: 10.27.0.0/26\n- name: sample-zone-2\n  vpc: sample\n  zone: eu-de-2\n  cidr: 10.27.0.64/26\n- name: sample-zone-3\n  vpc: sample\n  zone: eu-de-3\n  cidr: 10.27.0.128/26\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_5","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the zone Yes zone Zone in the IBM Cloud Yes cidr Address range that IPs in this zone will fall into Yes vpc Virtual Private Cloud this address prefix belongs to Yes, inferred from vpc Existing <code>vpc</code>"},{"location":"30-reference/configuration/infrastructure/#ibm-cloud-subnet","title":"IBM Cloud <code>subnet</code>","text":"<p>Defines the subnet that Virtual Server Instances and ROKS compute nodes will be attached to.</p> <pre><code>subnet:\n- name: sample-subnet-zone-1\n  address_prefix: sample-zone-1\n  ipv4_cidr_block: 10.27.0.0/26\n  zone: eu-de-1\n  vpc: sample\n  network_acl: sample-acl\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_6","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the subnet Yes zone Zone this subnet belongs to Yes, inferred from address_prefix-&gt;zone ipv4_cidr_block Address range that IPs in this subnet will fall into Yes, inferred from address_prefix-&gt;cidr Range of subrange of zone address_prefix Zone of the address prefix definition Yes, inferred from address_prefix Existing <code>address_prefix</code> vpc Virtual Private Cloud this subnet prefix belongs to Yes, inferred from address_prefix-&gt;vpc Existing <code>vpc</code> network_acl Reference to the network access control list protecting this subnet No"},{"location":"30-reference/configuration/infrastructure/#ibm-cloud-network_acl","title":"IBM Cloud <code>network_acl</code>","text":"<p>Defines the network access control list to be associated with subnets to allow or deny traffic from or to external connections. The rules are processed in sequence per direction. Rules that appear higher in the list will be processed first.</p> <pre><code>network_acl:\n- name: \"{{ env_id }}-acl\"\n  vpc_name: \"{{ env_id }}\"\n  rules:\n  - name: inbound-ssh\n    action: allow               # Can be allow or deny\n    source: \"0.0.0.0/0\"\n    destination: \"0.0.0.0/0\"\n    direction: inbound\n    tcp:\n      source_port_min: 1        # optional\n      source_port_max: 65535    # optional\n      dest_port_min: 22         # optional\n      dest_port_max: 22         # optional\n  - name: output-udp\n    action: deny                # Can be allow or deny\n    source: \"0.0.0.0/0\"\n    destination: \"0.0.0.0/0\"\n    direction: outbound\n    udp:\n      source_port_min: 1        # optional\n      source_port_max: 65535    # optional\n      dest_port_min: 1000       # optional\n      dest_port_max: 2000       # optional\n  - name: output-icmp\n    action: allow               # Can be allow or deny\n    source: \"0.0.0.0/0\"\n    destination: \"0.0.0.0/0\"\n    direction: outbound\n    icmp:\n      code: 1\n      type: 1\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_7","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the network access control liet Yes vpc_name Virtual Private Cloud this network ACL belongs to Yes rules Rules to be applied, every rule is an entry in the list Yes rules.name Unique name of the rule Yes rules.action Defines whether the traffic is allowed or denied Yes allow, deny rules.source Source address range that defines the rule Yes rules.destination Destination address range that defines the rule Yes rules.direction Inbound or outbound direction of the traffic Yes inbound, outbound rules.tcp Rule for TCP traffic No rules.tcp.source_port_min Low value of the source port range No, default=1 1-65535 rules.tcp.source_port_max High value of the source port range No, default=65535 1-65535 rules.tcp.dest_port_min Low value of the destination port range No, default=1 1-65535 rules.tcp.dest_port_max High value of the destination port range No, default=65535 1-65535 rules.udp Rule for UDP traffic No rules.udp.source_port_min Low value of the source port range No, default=1 1-65535 rules.udp.source_port_max High value of the source port range No, default=65535 1-65535 rules.udp.dest_port_min Low value of the destination port range No, default=1 1-65535 rules.udp.dest_port_max High value of the destination port range No, default=65535 1-65535 rules.icmp Rule for ICMP traffic No rules.icmp.code ICMP traffic code No, default=all 0-255 rules.icmp.type ICMP traffic type No, default=all 0-254"},{"location":"30-reference/configuration/infrastructure/#ibm-cloud-vsi","title":"IBM Cloud <code>vsi</code>","text":"<p>Defines a Virtual Server Instance within the VPC.</p> <pre><code>vsi:\n- name: sample-bastion\n  infrastructure:\n    type: vpc\n    keys:\n    - \"vsi-access\"\n    image: ibm-redhat-8-3-minimal-amd64-3\n    subnet: sample-subnet-zone-1\n    primary_ipv4_address: 10.27.0.4\n    public_ip: True\n    vpc_name: sample\n    zone: eu-de-3\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_8","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the Virtual Server Instance Yes infrastructure Infrastructure attributes Yes infrastructure.type Infrastructure type Yes vpc infrastructure.allow_ip_spoofing Decide if IP spoofing is allowed for the interface or not No False (default), True infrastructure.keys List of SSH keys to attach to the VSI Yes, inferred from ssh_keys Existing <code>ssh_keys</code> infrastructure.image Operating system image to be used Yes Existing image in IBM Cloud infrastructure.profile Server profile to be used, for example cx2-2x4 Yes Existing profile in IBM Cloud infrastructure.subnet Subnet the VSI will be connected to Yes, inferred from sunset Existing <code>subnet</code> infrastructure.primary_ipv4_address IP v4 address that will be assigned to the VSI No If specified, address in the <code>subnet</code> range infrastructure.public_ip Must a public IP address be attached to this VSI? No False (default), True infrastructure.vpc_name Virtual Private Cloud this VSI belongs to Yes, inferred from vpc Existing <code>vpc</code> infrastructure.zone Zone the VSI will be plaed into Yes, inferred from subnet-&gt;zone"},{"location":"30-reference/configuration/infrastructure/#ibm-cloud-transit_gateway","title":"IBM Cloud <code>transit_gateway</code>","text":"<p>Connects one or more VPCs to each other.</p> <pre><code>transit_gateway:\n- name: sample-tgw\n  location: eu-de\n  connections:\n  - vpc: other-vpc\n  - vpc: sample\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_9","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the transit gateway Yes location IBM Cloud location of the transit gateway Yes connections Defines which VPCs must be included in the transit gateway Yes connection.vpc Defines the VPC to include. Every VPC must exist in the configuration, even if not managed by this configuration. When referencing an existing VPC, make sure that there is a <code>vpc</code> object of that name with <code>managed</code> set to <code>False</code>. Yes Existing <code>vpc</code>"},{"location":"30-reference/configuration/infrastructure/#ibm-cloud-nfs_server","title":"IBM Cloud <code>nfs_server</code>","text":"<p>Defines a Virtual Server Instance within the VPC that will be used as an NFS server.</p> <pre><code>nfs_server:\n- name: sample-nfs\n  infrastructure:\n    type: vpc\n    vpc_name: sample\n    subnet: sample-subnet-zone-1\n    zone: eu-de-1\n    primary_ipv4_address: 10.27.0.5\n    image: ibm-redhat-8-3-minimal-amd64-3\n    profile: cx2-2x4\n    bastion_host: sample-bastion\n    storage_folder: /data/nfs\n    storage_profile: 10iops-tier\n    keys:\n      - \"sample-nfs-provision\"\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_10","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the Virtual Server Instance Yes infrastructure Infrastructure attributes Yes infrastructure.image Operating system image to be used Yes Existing image in IBM Cloud infrastructure.profile Server profile to be used, for example cx2-2x4 Yes Existing profile in IBM Cloud infrastructure.type Type of infrastructure for NFS servers to Yes vpc infrastructure.vpc_name Virtual Private Cloud this VSI belongs to Yes, inferred from vpc Existing <code>vpc</code> infrastructure.subnet Subnet the VSI will be connected to Yes, inferred from subnet Existing <code>subnet</code> infrastructure.zone Zone the VSI will be plaed into Yes, inferred from subnet-&gt;zone infrastructure.primary_ipv4_address IP v4 address that will be assigned to the VSI No If specified, address in the <code>subnet</code> range infrastructure.bastion_host Specify the VSI of the bastion to reach this NFS server No infrastructure.storage_profile Storage profile that will be used Yes 3iops-tier, 5iops-tier, 10iops-tier infrastructure.volume_size_gb Size of the NFS server data volume Yes infrastructure.storage_folder Folder that holds the data, this will be mounted from the NFS storage class Yes infrastructure.keys List of SSH keys to attach to the NFS server VSI Yes, inferred from ssh_keys Existing <code>ssh_keys</code> infrastructure.allow_ip_spoofing Decide if IP spoofing is allowed for the interface or not No False (default), True"},{"location":"30-reference/configuration/infrastructure/#ibm-cloud-cos","title":"IBM Cloud <code>cos</code>","text":"<p>Defines a IBM Cloud Cloud Object Storage instance and allows to create buckets.</p> <pre><code>cos:\n- name: {{ env_id }}-cos\n  plan: standard\n  location: global\n  serviceids:\n  - name: {{ env_id }}-cos-serviceid\n    roles: [\"Manager\", \"Viewer\", \"Administrator\"]\n  buckets:\n  - name: bucketone6c9d6840\n    cross_region_location: eu\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_11","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the serviceid Yes plan short description of the serviceid Yes location collection of servicekeys that should be created for the parent serviceid Yes serviceids Collection of references to defined seriveids No serviceids.name Name of the serviceid Yes serviceids.roles An array of strings to define which role should be granted to the serviceid Yes buckets Collection of buckets that should be created inside the cos instance No buckets[].name Name of the bucket No buckets[].storage_class Storage class of the bucket No standard (default), vault, cold, flex, smart buckets[].endpoint_type Endpoint type of the bucket No public (default), private buckets[].cross_region_location If you use this parameter, do not set single_site_location or region_location at the same time. Yes (one of) us, eu, ap buckets[].region_location If you set this parameter, do not set single_site_location or cross_region_location at the same time. Yes (one of) au-syd, eu-de, eu-gb, jp-tok, us-east, us-south, ca-tor, jp-osa, br-sao buckets[].single_site_location If you set this parameter, do not set region_location or cross_region_location at the same time. Yes (one of) ams03, che01, hkg02, mel01, mex01, mil01, mon01, osl01, par01, sjc04, sao01, seo01, sng01, and tor01"},{"location":"30-reference/configuration/infrastructure/#serviceid","title":"<code>serviceid</code>","text":"<p>Defines a iam_service_id that can be granted several role based accesss right via attaching iam_policies to it.</p> <pre><code>serviceid:\n- name: sample-serviceid\n  description: to access ibmcloud services from external\n  servicekeys:\n  - name: primarykey\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_12","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the serviceid Yes description short description of the serviceid No servicekeys collection of servicekeys that should be created for the parent serviceid No servicekeys.name Name of the servicekey Yes"},{"location":"30-reference/configuration/infrastructure/#microsoft-azure","title":"Microsoft Azure","text":"<p>For Microsoft Azure, the following object type is supported:</p> <ul> <li>azure</li> </ul>"},{"location":"30-reference/configuration/infrastructure/#azure","title":"Azure","text":"<p>Defines an infrastructure configuration onto which OpenShift will be provisioned.</p> <pre><code>azure:\n- name: sample\n  resource_group:\n    name: sample\n    location: westeurope\n  vnet:\n    name: vnet\n    address_space: 10.0.0.0/22\n  control_plane:\n    subnet:\n      name: control-plane-subnet\n      address_prefixes: 10.0.0.0/23\n  compute:\n    subnet:\n      name: compute-subnet\n      address_prefixes: 10.0.2.0/23\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#properties-explanation","title":"Properties explanation","text":"Property Description Mandatory Allowed values name Name of the <code>azure</code> definition object, will be referenced by <code>openshift</code> Yes resource_group Resource group attributes Yes resource_group.name Name of the resource group (will be provisioned) Yes unique value, it must not exist resource_group.location Azure location Yes to pick a different location, run: <code>az account list-locations -o table</code> vnet Virtual network attributes Yes vnet.name Name of the virtual network Yes vnet.address_space Address space of the virtual network Yes control_plane Control plane (master) nodes attributes Yes control_plane.subnet Control plane nodes subnet attributes Yes control_plane.subnet.name Name of the control plane nodes subnet Yes control_plane.subnet.address_prefixes Address prefixes of the control plane nodes subnet (divided by a <code>,</code> comma, if relevant) Yes control_plane.vm Control plane nodes virtual machine attributes Yes control_plane.vm.size Virtual machine size (aka flavour) of the control plane nodes Yes <code>Standard_D8s_v3</code>, <code>Standard_D16s_v3</code>, <code>Standard_D32s_v3</code> compute Compute (worker) nodes attributes Yes compute.subnet Compute nodes subnet attributes Yes compute.subnet.name Name of the compute nodes subnet Yes compute.subnet.address_prefixes Address prefixes of the compute nodes subnet (divided by a <code>,</code> comma, if relevant) Yes compute.vm Compute nodes virtual machine attributes Yes compute.vm.size Virtual machine size (aka flavour) of the compute nodes Yes See the full list of supported virtual machine sizes compute.vm.disk_size_gb Disk size in GBs of the compute nodes virtual machine Yes minimum value is 128 compute.vm.count Number of compute nodes virtual machines Yes minimum value is 3"},{"location":"30-reference/configuration/infrastructure/#amazon","title":"Amazon","text":"<p>For Amazon AWS, the following object types are supported:</p> <ul> <li>nfs_server</li> </ul>"},{"location":"30-reference/configuration/infrastructure/#aws-efs-server-nfs_server","title":"AWS EFS Server <code>nfs_server</code>","text":"<p>Defines a new Elastic File Storage (EFS) service that is connected to the OpenShift cluster within the same VPC. The file storage will be used as the back-end for the <code>efs-nfs-client</code> OpenShift storage class.</p> <pre><code>nfs_server:\n- name: sample-elastic\n  infrastructure:\n    aws_region: eu-west-1\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_13","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the EFS File System service to be created Yes infrastructure Infrastructure attributes Yes infrastructure.aws_region AWS region where the storage will be provisioned Yes"},{"location":"30-reference/configuration/infrastructure/#vsphere","title":"vSphere","text":"<p>For vSphere, the following object types are supported:</p> <ul> <li>vsphere</li> <li>vm_definition</li> <li>nfs_server</li> </ul>"},{"location":"30-reference/configuration/infrastructure/#vsphere-vsphere","title":"vSphere <code>vsphere</code>","text":"<p>Defines the vSphere vCenter onto which OpenShift will be provisioned.</p> <pre><code>vsphere:\n- name: sample\n  vcenter: 10.99.92.13\n  datacenter: Datacenter1\n  datastore: Datastore1\n  cluster: Cluster1\n  network: \"VM Network\"\n  folder: /Datacenter1/vm/sample\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_14","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the vSphere definition, will be referenced by <code>openshift</code> Yes vcenter Host or IP address of the vSphere Center Yes datacenter vSphere Data Center to be used for the virtual machines Yes datastore vSphere Datastore to be used for the virtual machines Yes cluster vSphere cluster to be used for the virtual machines Yes resource_pool vSphere resource pool No network vSphere network to be used for the virtual machines Yes folder Fully qualified folder name into which the OpenShift cluster will be placed; the folder must exist Yes"},{"location":"30-reference/configuration/infrastructure/#vsphere-vm_definition","title":"vSphere <code>vm_definition</code>","text":"<p>Defines the virtual machine properties to be used for the control-plane nodes and compute nodes.</p> <pre><code>vm_definition:\n- name: control-plane\n  vcpu: 8\n  memory_mb: 32768\n  boot_disk_size_gb: 100\n- name: compute\n  vcpu: 16\n  memory_mb: 65536\n  boot_disk_size_gb: 200\n  # Optional overrides for vsphere properties\n  # datastore: Datastore1\n  # network: \"VM Network\"\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_15","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the VM definition, will be referenced by <code>openshift</code> Yes vcpu Number of virtual CPUs to be assigned to the VMs Yes memory_mb Amount of memory in MiB of the virtual machines Yes boot_disk_size_gb Size of the virtual machine boot disk in GiB Yes datastore vSphere Datastore to be used for the virtual machines, overrides <code>vsphere.datastore</code> No network vSphere network to be used for the virtual machines, overrides <code>vsphere.network</code> No"},{"location":"30-reference/configuration/infrastructure/#vsphere-nfs_server","title":"vSphere <code>nfs_server</code>","text":"<p>Defines an existing NFS server that will be used for the OpenShift NFS storage class.</p> <pre><code>nfs_server:\n- name: sample-nfs\n  infrastructure:\n    host_ip: 10.99.92.31\n    storage_folder: /data/nfs\n</code></pre>"},{"location":"30-reference/configuration/infrastructure/#property-explanation_16","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the NFS server Yes infrastructure Infrastructure attributes Yes infrastructure.host_ip Host or IP address of the NFS server Yes infrastructure.storage_folder Folder that holds the data, this will be mounted from the NFS storage class Yes"},{"location":"30-reference/configuration/ldap/","title":"LDAP configuration","text":"<p>You can reference an LDAP service that is available in the organization's infrastructure services.</p>"},{"location":"30-reference/configuration/ldap/#ldap-configuration---ldap","title":"LDAP configuration - <code>ldap</code>","text":"<p>Sample configuration for LDAP <pre><code>ldap:\n- name: cp4d-ldap\n  ldap_url: ldap://openldap.cp4d-openldap.svc:389\n  ldap_base_dn: dc=cp,dc=internal\n  ldap_bind_dn: cn=admin,dc=cp,dc=internal\n  ldap_bind_password_vault_secret: cp-internal-ldap-bind-password\n  ldap_group_filter: '(&amp;(cn=%v)(objectclass=groupOfUniqueNames))'\n  ldap_group_id_map: '*:cn'\n  ldap_group_member_id_map: 'groupOfUniqueNames:uniqueMember'\n  ldap_user_filter: '(&amp;(uid=%v)(objectclass=inetOrgPerson))'\n  ldap_user_id_map: '*:uid'\n  ldap_case_insensitive: False\n</code></pre></p> <p>Sample configuration for LDAPS <pre><code>ldap:\n- name: cp4d-ldap\n  ldap_url: ldaps://openldap.cp4d-openldap.svc:646\n  ldap_tls_verify_client: demand\n  ldap_tls_client_cert_vault_secret: cp4d-openldap-cert\n  ldap_tls_client_key_vault_secret: cp4d-openldap-key\n  ldap_base_dn: dc=cp,dc=internal\n  ldap_bind_dn: cn=admin,dc=cp,dc=internal\n  ldap_bind_password_vault_secret: cp-internal-ldap-bind-password\n  ldap_group_filter: '(&amp;(cn=%v)(objectclass=groupOfUniqueNames))'\n  ldap_group_id_map: '*:cn'\n  ldap_group_member_id_map: 'groupOfUniqueNames:uniqueMember'\n  ldap_user_filter: '(&amp;(uid=%v)(objectclass=inetOrgPerson))'\n  ldap_user_id_map: '*:uid'\n  ldap_case_insensitive: False\n</code></pre></p> <p>The LDAP name is referenced in the Cloud Pak for Data Access Control resource and this is also where the mapping from LDAP groups to Cloud Pak for Data groups takes place. </p>"},{"location":"30-reference/configuration/ldap/#property-explanation","title":"Property explanation","text":"Property Description Mandatory Allowed values name Name of the LDAP server, for reference by <code>zen_access_control</code> Yes ldap_url The URL of the LDAP server, including protocol and port Yes ldap_tls_verify_client Option that matches the TLSVerifyClient setting of the LDAP server No try, demand ldap_tls_client_cert_vault_secret Certificate of the LDAP server No ldap_tls_client_key_vault_secret Key of the LDAP server No ldap_url The URL of the LDAP server, including protocol and port Yes ldap_url The URL of the LDAP server, including protocol and port Yes ldap_base_dn Base domain name, specify through <code>dc</code> components Yes ldap_bind_dn The bind user of the LDAP server No ldap_bind_password_vault_secret The deployer vault secret that holds the password of the bind user No ldap_group_filter The filter clause for searching groups Yes ldap_group_id_map The filter to map a group name to an LDAP entry Yes ldap_group_member_id_map The filter to map a user to a group Yes ldap_user_filter The filter clause for searching users Yes ldap_user_id_map The filter to map a username to an LDAP entry Yes ldap_case_insensitive Indicates whether user names are case insensitive No False (default), True <p>Info</p> <p>Please make sure that the bind password is stored in the vault secret specified in the <code>ldap_bind_password_vault_secret</code> property. Use <code>cp-deploy.sh vault set -vs=your_secret_name=your_bind_password</code> to set the vault secret before running the deployer. </p> <p>If your LDAP server supports anonymous binding and you do not want to authenticate, do not specify the <code>ldap_bind_dn</code> and <code>ldap_bind_password_vault_secret</code>.</p>"},{"location":"30-reference/configuration/logging-auditing/","title":"Logging and auditing for Cloud Paks","text":"<p>For logging and auditing of Cloud Pak for Data we make use of the OpenShift logging framework, which delivers a lot of flexibility in capturing logs from applications, storing them in an ElasticSearch datastore in the cluster (currently not supported by the deployer), or forwarding the log entries to external log collectors such as an ElasticSearch, Fluentd, Loki and others.</p> <p></p> <p>OpenShift logging captures 3 types of logging entries from workload that is running on the cluster:</p> <ul> <li>infrastructure - logs generated by OpenShift processes</li> <li>audit - audit logs generated by applications as well as OpenShift</li> <li>application - all other applications on the cluster</li> </ul>"},{"location":"30-reference/configuration/logging-auditing/#logging-configuration---openshift_logging","title":"Logging configuration - <code>openshift_logging</code>","text":"<p>Defines how OpenShift forwards the logs to external log collectors. Currently, the following log collector types are supported:</p> <ul> <li>loki</li> </ul> <p>When OpenShift logging is activated via the <code>openshift_logging</code> object, all 3 logging types are activated automatically. You can specify <code>logging_output</code> items to forward log records to the log collector of your choice. In the below example, the application logs are forwarded to a loki server <code>https://loki-application.sample.com</code> and audit logs to <code>https://loki-audit.sample.com</code>, both have the same certificate to connect with:</p> <pre><code>openshift_logging:\n- openshift_cluster_name: pluto-01\n  configure_es_log_store: False\n  cluster_wide_logging:\n  - input: application\n    logging_name: loki-application\n  - input: infrastructure\n    logging_name: loki-application\n  - input: audit\n    logging_name: loki-audit\n  logging_output:\n  - name: loki-application\n    type: loki\n    url: https://loki-application.sample.com\n    certificates:\n      cert: pluto-01-loki-cert\n      key: pluto-01-loki-key\n      ca: pluto-01-loki-ca\n  - name: loki-audit\n    type: loki\n    url: https://loki-audit.sample.com\n    certificates:\n      cert: pluto-01-loki-cert\n      key: pluto-01-loki-key\n      ca: pluto-01-loki-ca\n</code></pre> <p>Cloud Pak for Data and Foundational Services application logs are automatically picked up and forwarded to the <code>loki-application</code> logging destination and no additional configuration is needed.</p>"},{"location":"30-reference/configuration/logging-auditing/#property-explanation","title":"Property explanation","text":"Property Description Mandatory Allowed values openshift_cluster_name Name of the OpenShift cluster to configure the logging for Yes configure_es_log_store Must internal ElasticSearch log store and Kibana be provisioned? (default False) No True, False (default) cluster_wide_logging Defines which classes of log records will be sent to the log collectors No cluster_wide_logging.input Specifies OpenShift log records class to forwawrd Yes application, infrastructure, audit cluster_wide_logging.logging_name Specifies the <code>logging_output</code> to send the records to . If not specified, records will be sent to the internal log only No cluster_wide_logging.labels Specify your own labels to be added to the log records. Every logging input/output combination can have its own labes No logging_output Defines the log collectors. If <code>configure_es_log_store</code> is True, output will always be sent to the internal ES log store No logging_output.name Log collector name, referenced by <code>cluster_wide_logging</code> or <code>cp4d_audit</code> Yes logging_output.type Type of the log collector, currently only <code>loki</code> is possible Yes loki logging_output.url URL of the log collector; this URL must be reachable from within the cluster Yes logging_output.certificates Defines the vault secrets that hold the certificate elements Yes, if url is https logging_output.certificates.cert Public certificate to connect to the URL Yes logging_output.certificates.key Private key to connect to the URL Yes logging_output.certificates.ca Certificate Authority bundle to connect to the URL Yes <p>If you also want to activate audit logging for Cloud Pak for Data, you can do this by adding a <code>cp4d_audit_config</code> object to your configuration. With the below example, the Cloud Pak for Data audit logger is configured to write log records to the standard output (<code>stdout</code>) of the pods, after which they are forwarded to the <code>loki-audit</code> logging destination by a <code>ClusterLogForwarder</code> custom resource. Optionally <code>labels</code> can be specified which are added to the <code>ClusterLogForwarder</code> custom resource pipeline entry.  </p> <pre><code>cp4d_audit_config:\n- project: cpd\n  audit_replicas: 2\n  audit_output:\n  - type: openshift-logging\n    logging_name: loki-audit\n    labels:\n      cluster_name: \"{{ env_id }}\"    \n</code></pre> <p>Info</p> <p>Because audit log entries are written to the standard output, they will also be picked up by the generic application log forwarder and will therefore also appear in the application logging destination.</p>"},{"location":"30-reference/configuration/logging-auditing/#cloud-pak-for-data-audit-configuration","title":"Cloud Pak for Data audit configuration","text":"<p>IBM Cloud Pak for Data has a centralized auditing component for base platform and services auditable events. Audit events include login and logout to the platform, creation and deletion of connections and many more. Services that support auditing are documented here: https://www.ibm.com/docs/en/cloud-paks/cp-data/4.0?topic=data-services-that-support-audit-logging</p> <p>The Cloud Pak Deployer simplifies the recording of audit log entries by means of the OpenShift logging framework, which can in turn be configured to forward entries to various log collectors such as Fluentd, Loki and ElasticSearch.</p>"},{"location":"30-reference/configuration/logging-auditing/#audit-configuration---cp4d_audit_config","title":"Audit configuration - <code>cp4d_audit_config</code>","text":"<p>A <code>cp4d_audit_config</code> entry defines the audit configuration for a Cloud Pak for Data instance (OpenShift project). The main configuration items are the number of replicas and the output. Currently only one output type is supported: <code>openshift-logging</code>, which allows the OpenShift logging framework to pick up audit entries and forward to the designated collectors.</p> <p>When a <code>cp4d_audit_config</code> entry exists for a certain <code>cp4d</code> project, the <code>zen-audit-config</code> ConfigMap is updated and then the audit logging deployment is restarted. If no configuration changes have been made, no restart is done.</p> <p>Additionally, for the <code>audit_output</code> entries, the OpenShift logging <code>ClusterLogForwarder</code> instance is updated to forward audit entries to the designated logging output. In the example below the auditing is configured with 2 replicas and an <code>input</code> and <code>pipeline</code> is added to the <code>ClusterLogForwarder</code> instance so output to the matching channel defined in <code>openshift_logging.logging_output</code>.</p> <pre><code>cp4d_audit_config:\n- project: cpd\n  audit_replicas: 2\n  audit_output:\n  - type: openshift-logging\n    logging_name: loki-audit\n    labels:\n      cluster_name: \"{{ env_id }}\"\n</code></pre>"},{"location":"30-reference/configuration/logging-auditing/#property-explanation_1","title":"Property explanation","text":"Property Description Mandatory Allowed values project Name of OpenShift project of the matching <code>cp4d</code> entry. The cp4d project must exist. Yes audit_replicas Number of replicas for the Cloud Pak for Data audit logger. No (default 1) audit_output Defines where the audit logs should be written to Yes audit_output.type Type of auditing output, defines where audit logging entries will be written Yes openshift-logging audit_output.logging_name Name of the <code>logging_output</code> entry in the <code>openshift_logging</code> object. This <code>logging_output</code> entry must exist. Yes audit_output.labels Optional list of labels set to the ClusterLogForwarder custom resource pipeline No"},{"location":"30-reference/configuration/monitoring/","title":"Monitoring OpenShift and Cloud Paks","text":"<p>For monitoring of Cloud Pak for Data we make use of the OpenShift Monitoring framework. The observations generated by Cloud Pak for Data are pushed to the OpenShift Monitoring Prometheus endpoint. This will allow (external) monitoring tools to combine the observations from the OpenShift platform and Cloud Pak for Data from a single source. </p> <p></p>"},{"location":"30-reference/configuration/monitoring/#openshift-monitoring","title":"OpenShift monitoring","text":"<p>To deploy Cloud Pak for Data Monitors, its is mandatory to also enable the OpenShift monitoring. OpenShift monitoring is activated via the <code>openshift_monitoring</code> object. </p> <pre><code>openshift_monitoring:\n- openshift_cluster_name: pluto-01\n  user_workload: enabled\n  remote_rewrite_url: http://www.example.com:1234/receive\n  retention_period: 15d\n  pvc_storage_class: ibmc-vpc-block-retain-general-purpose\n  pvc_storage_size_gb: 100\n  grafana_operator: enabled\n  grafana_project: grafana\n  labels:\n    cluster_name: pluto-01\n</code></pre> Property Description Mandatory Allowed values user_worload Allow pushing Prometheus metrics to OpenShift (must be set to True for monitoring to work) Yes True, False pvc_storage_class Storage class to keep persistent monitoring data No Valid storage class pvc_storage_size_gb Size of the PVC holding the monitoring data Yes if pv_storage_class is set remote_rewrite_url Set this value to redirect metrics to remote Prometheus NO retention_period Number of seconds (s), minutes (m), hours(h), days (d), weeks (w), years (y) to retain monitoring data. Default is 15d Yes labels Additional labels to be added to the metrics No grafana_operator Enable Grafana community operator? No False (default), True grafana_project If enabled, project in which to enable the Grafana operator Yes, if grafana_operator enabled <p>Note Labels must be specified as a YAML record where each line is a key-value. The labels will be added to the <code>prometheus</code> key of the <code>user-workload-monitoring-config</code> ConfigMap and to the <code>prometheusK8S</code> key of the <code>cluster-monitoring-config</code> ConfigMap.</p> <p>Note When the Grafana operator is enabled, you can build your own Grafana dashboard based on the metrics collected by Prometheus. When installed, Grafana creates a local admin user with user name <code>root</code> and passwowrd <code>secret</code>. Grafana can be accessed using the OpenShift route that is created in the project specified by <code>grafana_project</code>.</p>"},{"location":"30-reference/configuration/monitoring/#cloud-pak-for-data-monitoring","title":"Cloud Pak for Data monitoring","text":"<p>The observations of Cloud Pak for Data are generated using the zen-watchdog component, which is part of the cpd_platform cartridge and therefore available on each instance of Cloud Pak for Data. Part of the zen-watchdog installation is a set of monitors which focus on the technical deployment of Cloud Pak for Data (e.g. running pods and bound Persistent Volume Claims (pvcs)).</p> <p>Additional monitors which focus more on the operational usage of Cloud Pak for Data can be deployed as well. These monitors are maintained in a seperate Git repository and be accessed at IBM/cp4d-monitors. Using the Cloud Pak Deployer, monitors can be deployed which uses the Cloud Pak for Data zen-watchdog monitor framework. This allows adding custom monitors to the zen-watchdog, making these custom monitors visible in the Cloud Pak for Data metrics.</p> <p></p> <p>Using the Cloud Pak Deployer <code>cp4d_monitors</code> capability implements the following: - Create Cloud Pak for Data ServiceMonitor endpoint to forward zen-watchdog monitor events to OpenShift Cluster monitoring - Create source repository auth secrets (optional, if pulling monitors from secure repo) - Create target container registry auth secrets (optional, if pushing monitor images to secure container registry) - Deploy custom monitors, which will be added to the zen-watchdog monitor framework</p> <p>For custom monitors to be deployed, it is mandatory to enable the OpenShift user-workload monitoring, as specified in OpenShift monitoring.</p> <p>The Cloud Pak for Data monitors are specified in a <code>cp4d_monitors</code> definition. <pre><code>cp4d_monitors:\n- name: cp4d-monitor-set-1\n  cp4d_instance: zen-45\n  openshift_cluster_name: pluto-01\n  default_monitor_source_repo: https://github.com/IBM/cp4d-monitors\n  #default_monitor_source_token_secret: monitors_source_repo_secret\n  #default_monitor_target_cr: de.icr.io/monitorrepo  \n  #default_monitor_target_cr_user_secret: monitors_target_cr_username\n  #default_monitor_target_cr_password_secret: monitors_target_cr_password\n  # List of monitors\n  monitors:\n  - name: cp4dplatformcognosconnectionsinfo\n    context: cp4d-cognos-connections-info\n    label: latest\n    schedule: \"*/15 * * * *\"\n  - name: cp4dplatformcognostaskinfo\n    context: cp4d-cognos-task-info\n    label: latest\n    schedule: \"*/15 * * * *\"\n  - name: cp4dplatformglobalconnections\n    context: cp4d-platform-global-connections\n    label: latest\n    schedule: \"*/15 * * * *\"\n  - name: cp4dplatformwatsonstudiojobinfo\n    context: cp4d-watsonstudio-job-info\n    label: latest\n    schedule: \"*/15 * * * *\"\n  - name: cp4dplatformwatsonstudiojobscheduleinfo\n    context: cp4d-watsonstudio-job-schedule-info\n    label: latest\n    schedule: \"*/15 * * * *\"\n  - name: cp4dplatformwatsonstudioruntimeusage\n    context: cp4d-watsonstudio-runtime-usage\n    label: latest\n    schedule: \"*/15 * * * *\"\n  - name: cp4dplatformwatsonknowledgecataloginfo\n    context: cp4d-wkc-info\n    label: latest\n    schedule: \"*/15 * * * *\"\n  - name: cp4dplatformwmldeploymentspaceinfo\n    context: cp4d-wml-deployment-space-info\n    label: latest  \n    schedule: \"*/15 * * * *\"\n  - name: cp4dplatformwmldeploymentspacejobinfo\n    context: cp4d-wml-deployment-space-job-info\n    label: latest\n    schedule: \"*/15 * * * *\"\n</code></pre></p> <p>Each <code>cp4d_monitors</code> entry contains a set of default settings, which are applicable to the <code>monitors</code> list. These defaults can be overwritten per monitor if needed.</p> Property Description Mandatory Allowed values name The name of the monitor set Yes lowercase RFC 1123 subdomain (1) cp4d_instance The OpenShift project (namespace) on which the Cloud Pak for Data instance resides Yes openshift_cluster_name The Openshift cluster name Yes default_monitor_source_repo The default repository location of all monitors located in the <code>monitors</code> section No default_monitor_source_token_secret The default repo access token secret name, must be available in the vault No default_monitor_target_cr The default target container registry (cr) for the monitor image to be pushed. When omitted, the OpenShift internal registry is used No default_monitor_target_cr_user_secret The default target container registry user name secret name used to push the monitor image. Must be available in the vault No default_monitor_target_cr_password_secret The default target container registry password secret name used to push the monitor image. Must be available in the vault No monitors List of monitors Yes <p>Per <code>monitors</code> entry, the following settings are specified:</p> Property Description Mandatory Allowed values name The name of the monitor entry Yes lowercase RFC 1123 subdomain (1) monitor_source_repo Overrides default_monitor_source_repo for this single monitor No monitor_source_token_secret Overrides default_monitor_source_token_secret for this single monitor No monitor_target_cr Overrides default_monitor_target_cr for this single monitor No monitor_target_cr_user_secret Overrides default_monitor_target_cr_user_secret for this single monitor No monitor_target_cr_user_password Overrides default_monitor_target_cr_user_password for this single monitor No context Sets the context of the monitor the the source repo (sub folder name) Yes label Set the label of the pushed image, default to 'latest' No schedule Sets the schedule of the generated Cloud Pak for Data monitor cronjob Yes <p>Each monitor has a set of <code>event_types</code>, which contain the observations generated by the monitor. These event types are retrieved directly from the github repository, which it is expected that each <code>context</code> contains a file called <code>event_types.yml</code>. During deployment of the monitor this file is retrieved and used to populate the <code>event_types</code> of the monitor. </p> <p>If the Deployer runs and the monitor is already deployed, the following process is used: - The build process is restarted to ensure the latest image of monitor is used - A comparison is made between the monitor's current configuration and the configuration created by the Deployer. If these are identical, the monitor's configuration is left as-is, however if these are different, the monitor's configuration is rebuild and the monitor is re-deployed. </p>"},{"location":"30-reference/configuration/monitoring/#example-monitior---global-platform-connections","title":"Example monitior - global platform connections","text":"<p>This monitor counts the number of Global Platform connections and for each Global Platform Connection a test is executed to test whether the connection can still be established.</p>"},{"location":"30-reference/configuration/monitoring/#generated-metrics","title":"Generated metrics","text":"<p>Once the monitor is deployed, the following metrics are available in IBM Cloud Pak for Data.</p> <p></p> <p>On the Platform Management Events page the following entries are added: - Cloud Pak for Data Global Connections Count - Global Connection - &lt;Global Connection Name&gt; (for each connection)</p>"},{"location":"30-reference/configuration/monitoring/#using-the-ibm-cloud-pak-for-data-prometheus-endpoint","title":"Using the IBM Cloud Pak for Data Prometheus endpoint","text":"<p>https://&lt;CP4D-BASE-URL&gt;/zen/metrics</p> <p>It will generate 2 types of metrics:</p> <ul> <li>global_connections_count   Provides the number of available connections</li> <li>global_connection_valid     For each connection, a test action is performed<ul> <li>1 (Test Connection success)</li> <li>0 (Test connection failed)</li> </ul> </li> </ul> <pre><code># HELP global_connections_count \n# TYPE global_connections_count gauge\nglobal_connections_count{event_type=\"global_connections_count\",monitor_type=\"cp4d_platform_global_connections\",reference=\"Cloud Pak for Data Global Connections Count\"} 2\n\n# HELP global_connection_valid \n# TYPE global_connection_valid gauge\nglobal_connection_valid{event_type=\"global_connection_valid\",monitor_type=\"cp4d_platform_global_connections\",reference=\"Cognos MetaStore Connection\"} 1\nglobal_connection_valid{event_type=\"global_connection_valid\",monitor_type=\"cp4d_platform_global_connections\",reference=\"Cognos non-shared\"} 0\n</code></pre> <p>Zen Watchdog metrics (used in platform management events) - watchdog_cp4d_platform_global_connections_global_connections_count - watchdog_cp4d_platform_global_connections_global_connection_valid (for each connection)</p> <p>Zen Watchdog metrics can have the following values: - 2 (info) - 1 (warning) - 0 (critical)</p> <pre><code># HELP watchdog_cp4d_platform_global_connections_global_connection_valid \n# TYPE watchdog_cp4d_platform_global_connections_global_connection_valid gauge\nwatchdog_cp4d_platform_global_connections_global_connection_valid{event_type=\"global_connection_valid\",monitor_type=\"cp4d_platform_global_connections\",reference=\"Cognos MetaStore Connection\"} 2\nwatchdog_cp4d_platform_global_connections_global_connection_valid{event_type=\"global_connection_valid\",monitor_type=\"cp4d_platform_global_connections\",reference=\"Cognos non-shared\"} 1\n\n# HELP watchdog_cp4d_platform_global_connections_global_connections_count \n# TYPE watchdog_cp4d_platform_global_connections_global_connections_count gauge\nwatchdog_cp4d_platform_global_connections_global_connections_count{event_type=\"global_connections_count\",monitor_type=\"cp4d_platform_global_connections\",reference=\"Cloud Pak for Data Global Connections Count\"} 2\n</code></pre>"},{"location":"30-reference/configuration/openshift/","title":"OpenShift cluster(s)","text":"<p>You can configure one or more OpenShift clusters that will be layed down on the specified infrastructure, or which already exist.</p> <p>Dependent on the cloud platform on which the OpenShift cluster will be provisioned, different installation methods apply. For IBM Cloud, Terraform is used, whereas for vSphere the IPI installer is used. On AWS (ROSA), the <code>rosa</code> CLI is used to create and modify ROSA clusters. Each of the different platforms have slightly different properties for the <code>openshift</code> objects.</p>"},{"location":"30-reference/configuration/openshift/#openshift","title":"<code>openshift</code>","text":"<p>For OpenShift, there are 5 flavours:</p> <ul> <li>Existing OpenShift</li> <li>OpenShift on IBM Cloud</li> <li>OpenShift on AWS - ROSA</li> <li>OpenShift on AWS - self-managed</li> <li>OpenShift on Microsoft Azure - ARO</li> <li>OpenShift on Microsoft Azure - self-managed</li> <li>OpenShift on vSphere</li> </ul> <p>Every OpenShift cluster definition of a few mandatory properties that control which version of OpenShift is installed (except for Existing OpenShift), the number and flavour of control plane and compute nodes and the underlying infrastructure, dependent on the cloud platform on which it is provisioned. Storage is a mandatory element for every <code>openshift</code> definition. For a list of supported storage types per cloud platform, refer to Supported storage types.</p> <p>Additionally, one can configure Upstream DNS Servers and OpenShift logging.</p> <p>The Multicloud Object Gateway (MCG) supports access to s3-compatible object storage via an underpinning block/file storage class, through the Noobaa operator. Some Cloud Pak for Data services such as Watson Assistant need object storage to run. MCG does not need to be installed if OpenShift Data Foundation is also installed as the operator includes Noobaa.</p>"},{"location":"30-reference/configuration/openshift/#existing-openshift","title":"Existing OpenShift","text":"<p>When using the Cloud Pak Deployer on an existing OpenShift cluster, the scripts assume that the cluster is already operational and that any storage classes have been pre-created. The deployer accesses the cluster through a vault secret with the kubeconfig information; the name of the secret is <code>&lt;name&gt;-kubeconfig</code>.</p> <pre><code>openshift:\n- name: sample\n  ocp_version: detect\n  cluster_name: sample\n  domain_name: example.com\n  cloud_native_toolkit: False\n  oadp: False\n  infrastructure:\n    type: standard\n    processor_architecture: amd64\n  upstream_dns:\n  - name: sample-dns\n     zones:\n     - example.com\n     dns_servers:\n     - 172.31.2.73:53\n  gpu:\n    install: auto\n  openshift_ai:\n    install: auto\n    channel: auto\n  mcg:\n    install: True\n    storage_type: storage-class\n    storage_class: managed-nfs-storage\n  openshift_storage:\n  - storage_name: nfs-storage\n    storage_type: nfs\n    # ocp_storage_class_file: managed-nfs-storage\n    # ocp_storage_class_block: managed-nfs-storage\n</code></pre>"},{"location":"30-reference/configuration/openshift/#property-explanation-for-existing-openshift-clusters","title":"Property explanation for existing OpenShift clusters","text":"Property Description Mandatory Allowed values name Name of the OpenShift cluster Yes ocp_version OpenShift version of the cluster. For existing OpenShift you can use <code>detect</code> Yes &gt;= 4.6 cluster_name Name of the cluster (part of the FQDN) Yes domain_name Domain name of the cluster (part of the FQDN) Yes cloud_native_toolkit Must the Cloud Native Toolkit (OpenShift GitOps) be installed? No True, False (default) oadp Must the OpenShift Advanced Data Protection operator be installed No True, False (default) infrastructure.type Infrastructure OpenShift is deployed on. See below for additional explanation detect (default) infrastructure.processor_architecture Architecture of the processor that the OpenShift cluster is deployed on No amd64 (default), ppc64le, s390x openshift_logging[] Logging attributes for OpenShift cluster, see OpenShift logging No upstream_dns[] Upstream DNS servers(s), see Upstream DNS Servers No gpu Control Node Feature Discovery and NVIDIA GPU operators No gpu.install Must Node Feature Discovery and NVIDIA GPU operators be installed (Once installed, False does not uninstall). <code>auto</code> will install the operators if needed by any of the Cloud Pak/watsonx components Yes auto, True, False openshift_ai Control installation of OpenShift AI No openshift_ai.install Must OpenShift AI be installed (Once installed, False does not uninstall). <code>auto</code> will install OpenShift AI if needed by any of the Cloud Pak/watsonx components Yes auto, True, False openshift_ai.channel Which oeprator channel must be installed No auto (default), stable, \u2026 mcg Multicloud Object Gateway properties No mcg.install Must Multicloud Object Gateway be installed (Once installed, False does not uninstall) Yes True, False mcg.storage_type Type of storage supporting the object Noobaa object storage Yes storage-class mcg.storage_class Storage class supporting the Noobaa object storage Yes Existing storage class openshift_storage[] List of storage definitions to be defined on OpenShift, see below for further explanation Yes"},{"location":"30-reference/configuration/openshift/#infastructuretype---type-of-infrastructure","title":"infastructure.type - Type of infrastructure","text":"<p>When deploying on existing OpenShift, the underlying infrastructure can pose some restrictions on capabilities available. For example, Red Hat OpenShift on IBM Cloud (aka ROKS) does not include the Machine Config Operator and ROSA on AWS does not allow to set labels for Machine Config Pools. This means that node settings required for Cloud Pak for Data must be applied in a non-standard manner.</p> <p>The following values are allowed for <code>infrastructure.type</code>:</p> <ul> <li><code>detect</code> (default): The deployer will attempt to detect the underlying cloud infrastructure. This is done by retrieving the existing storage classes and then inferring the cloud type.</li> <li><code>standard</code>: The deployer will assume a standard OpenShift cluster with no further restrictions. This is the fallback value for <code>detect</code> if the underlying infra cannot be detected. </li> <li><code>aws-self-managed</code>: A self-managed OpenShift cluster on AWS. No restrictions.</li> <li><code>aws-rosa</code>: Managed Red Hat OpenShift on AWS. Some restrictions with regards to Machine Config Pools apply.</li> <li><code>azure-aro</code>: Managed Red Hat OpenShift on Azure. No known restrictions.</li> <li><code>vsphere</code>: OpenShift on vSphere. No known restrictions.</li> </ul>"},{"location":"30-reference/configuration/openshift/#openshift_storage---openshift-storage-definitions","title":"openshift_storage[] - OpenShift storage definitions","text":"Property Description Mandatory Allowed values storage_name Name of the storage definition, to be referenced by the Cloud Pak Yes storage_type Type of storage class to use in the OpenShift cluster Yes nfs, odf, aws-elastic, auto, custom ocp_storage_class_file OpenShift storage class to use for file storage if different from default for storage_type Yes if <code>storage_type</code> is <code>custom</code> ocp_storage_class_block OpenShift storage class to use for block storage if different from default for storage_type Yes if <code>storage_type</code> is <code>custom</code> <p>Info</p> <p>The custom storage_type can be used in case you want to use a non-standard storage class(es). In this case the storage class(es) must be already configured on the OCP cluster and set in the respective ocp_storage_class_file and ocp_storage_class_block variables</p> <p>Info</p> <p>The auto storage_type will let the deployer automatically detect the storage type based on the existing storage classes in the OpenShift cluster.</p>"},{"location":"30-reference/configuration/openshift/#supported-storage-types","title":"Supported storage types","text":"<p>An <code>openshift</code> definition always includes the type(s) of storage that it will provide. When the OpenShift cluster is provisioned by the deployer, the necessary infrastructure and storage class(es) are also configured. In case an existing OpenShift cluster is referenced by the configuration, the storage classes are expected to exist already.</p> <p>The table below indicates which storage classes are supported by the Cloud Pak Deployer per cloud infrastructure.</p> <p>Warning</p> <p>The ability to provision or use certain storage types does not imply support by the Cloud Paks or by OpenShift itself. There are several restrictions for production use OpenShift Data Foundation, for example when on ROSA.</p> Cloud Provider NFS Storage ODF Storage Portworx Elastic Custom (2) ibm-cloud Yes Yes Yes No Yes vsphere Yes (1) Yes No No Yes aws No Yes No Yes (3) Yes azure No Yes No No Yes existing-ocp Yes Yes No Yes Yes <ul> <li>(1) An existing NFS server can be specified so that the deployer configures the <code>managed-nfs-storage</code> storage class. The deployer will not provision or change the NFS server itself.</li> <li>(2) If you specify a <code>custom</code> storage type, you must specify the storage class to be used for block (RWO) and file (RWX) storage.</li> <li>(3) Specifying this storage type means that Elastic File Storage (EFS) and Elastic Block Storage (EBS) storage classes will be used. For EFS, an <code>nfs_server</code> object is required to define the \"file server\" storage on AWS.</li> </ul>"},{"location":"30-reference/configuration/openshift/#openshift-on-ibm-cloud-roks","title":"OpenShift on IBM Cloud (ROKS)","text":"<p>VPC-based OpenShift cluster on IBM Cloud, using the Red Hat OpenShift Kubernetes Services (ROKS). <pre><code>openshift:\n- name: sample\n  managed: True\n  ocp_version: 4.8\n  compute_flavour: bx2.16x64\n  secondary_storage: 900gb.10iops-tier\n  compute_nodes: 3\n  cloud_native_toolkit: False\n  oadp: False\n  infrastructure:\n    type: vpc\n    vpc_name: sample\n    subnets:\n    - sample-subnet-zone-1\n    - sample-subnet-zone-2\n    - sample-subnet-zone-3\n    cos_name: sample-cos\n    private_only: False\n    deny_node_ports: False\n  upstream_dns:\n  - name: sample-dns\n     zones:\n     - example.com\n     dns_servers:\n     - 172.31.2.73:53\n  mcg:\n    install: True\n    storage_type: storage-class\n    storage_class: managed-nfs-storage\n  openshift_ai:\n    install: auto\n    channel: auto\n  openshift_storage:\n  - storage_name: nfs-storage\n    storage_type: nfs\n    nfs_server_name: sample-nfs\n  - storage_name: odf-storage\n    storage_type: odf\n    storage_flavour: bx2.16x64\n    secondary_storage: 900gb.10iops-tier\n    odf_storage_label: ocs\n    odf_storage_size_gb: 500\n    odf_version: 4.8.0\n  - storage_name: pwx-storage\n    storage_type: pwx \n    pwx_etcd_location: {{ ibm_cloud_region }}\n    pwx_storage_size_gb: 200 \n    pwx_storage_iops: 10 \n    pwx_storage_profile: \"10iops-tier\"\n    stork_version: 2.6.2\n    portworx_version: 2.7.2\n</code></pre></p>"},{"location":"30-reference/configuration/openshift/#property-explanation-openshift-clusters-on-ibm-cloud-roks","title":"Property explanation OpenShift clusters on IBM Cloud (ROKS)","text":"Property Description Mandatory Allowed values name Name of the OpenShift cluster Yes managed Is the ROKS cluster managed by this deployer? See note below. No True (default), False ocp_version ROKS Kubernetes version. If you want to install <code>4.10</code>, specify <code>\"4.10\"</code> Yes &gt;= 4.6 compute_flavour Type of compute node to be used Yes Node flavours secondary_storage Additional storage to be added to the compute servers No 900gb.10iops-tier, \u2026 compute_nodes Total number of compute nodes. This must be a factor of the number of subnets Yes Integer resource_group IBM Cloud resource group for the ROKS cluster Yes cloud_native_toolkit Must the Cloud Native Toolkit (OpenShift GitOps) be installed? No True, False (default) oadp Must the OpenShift Advanced Data Protection operator be installed No True, False (default) infrastructure.type Type of infrastructure to provision ROKS cluster on No vpc infrastructure.vpc_name Name of the VPC if type is <code>vpc</code> Yes, inferrred from vpc Existing VPC infrastructure.subnets List of subnets within the VPC to use. Either 1 or 3 subnets must be specified Yes Existing subnet infrastructure.cos_name Reference to the <code>cos</code> object created for this cluster Yes Existing cos object infrastructure.private_only If true, it indicates that the ROKS cluster must be provisioned without public endpoints No True, False (default) infrastructure.deny_node_ports If true, the Allow ICMP, TCP and UDP rules for the security group associated with the ROKS cluster are removed if present. If false, the Allow ICMP, TCP and UDP rules are added if not present. No True, False (default) openshift_logging[] Logging attributes for OpenShift cluster, see OpenShift logging No upstream_dns[] Upstream DNS servers(s), see Upstream DNS Servers No gpu Control Node Feature Discovery and NVIDIA GPU operators No gpu.install Must Node Feature Discovery and NVIDIA GPU operators be installed (Once installed, False does not uninstall). <code>auto</code> will install the operators if needed by any of the Cloud Pak/watsonx components Yes auto, True, False openshift_ai Control installation of OpenShift AI No openshift_ai.install Must OpenShift AI be installed (Once installed, False does not uninstall). <code>auto</code> will install OpenShift AI if needed by any of the Cloud Pak/watsonx components Yes auto, True, False openshift_ai.channel Which oeprator channel must be installed No auto (default), stable, \u2026 mcg Multicloud Object Gateway properties No mcg.install Must Multicloud Object Gateway be installed (Once installed, False does not uninstall) Yes True, False mcg.storage_type Type of storage supporting the object Noobaa object storage Yes storage-class mcg.storage_class Storage class supporting the Noobaa object storage Yes Existing storage class openshift_storage[] List of storage definitions to be defined on OpenShift, see below for further explanation Yes <p>The <code>managed</code> attribute indicates whether the ROKS cluster is managed by the Cloud Pak Deployer. If set to <code>False</code>, the deployer will not provision the ROKS cluster but expects it to already be available in the VPC. You can still use the deployer to create the VPC, the subnets, NFS servers and other infrastructure, but first run it without an <code>openshift</code> element. Once the VPC has been created, manually create an OpenShift cluster in the VPC and then add the <code>openshift</code> element with <code>managed</code> set to <code>False</code>. If you intend to use OpenShift Data Foundation, you must also activate the add-on and create the <code>OcsCluster</code> custom resource.</p> <p>Warning</p> <p>If you set <code>infrastructure.private_only</code> to <code>True</code>, the server from which you run the deployer must be able to access the ROKS cluster via its private endpoint, either by establishing a VPN to the cluster's VPC, or by making sure the deployer runs on a server that has a connection with the ROKS VPC via a transit gateway.</p>"},{"location":"30-reference/configuration/openshift/#openshift_storage---openshift-storage-definitions_1","title":"openshift_storage[] - OpenShift storage definitions","text":"Property Description Mandatory Allowed values openshift_storage[] List of storage definitions to be defined on OpenShift Yes storage_name Name of the storage definition, to be referenced by the Cloud Pak Yes storage_type Type of storage class to create in the OpenShift cluster Yes nfs, odf or pwx storage_flavour Type of compute node to be used for the storage nodes Yes Node flavours, default is <code>bx2.16x64</code> secondary_storage Additional storage to be added to the storage server No 900gb.10iops-tier, \u2026 nfs_server_name Name of the NFS server within the VPC Yes if <code>storage_type</code> is <code>nfs</code> Existing <code>nfs_server</code> odf_storage_label Label to be used for the dedicated ODF nodes in the cluster Yes if <code>storage_type</code> is <code>odf</code> odf_storage_size_gb Size of the ODF storage in Gibibytes (Gi) Yes if <code>storage_type</code> is <code>odf</code> odf_version Version of ODF to be deployed. If left empty, the latest version will be deployed No &gt;= 4.6 pwx_etcd_location Location where the etcd service will be deployed, typically the same region as the ROKS cluster Yes if <code>storage_type</code> is <code>pwx</code> pwx_storage_size_gb Size of the Portworx storage that will be provisioned Yes if <code>storage_type</code> is <code>pwx</code> pwx_storage_iops IOPS for the storage volumes that will be provisioned Yes if <code>storage_type</code> is <code>pwx</code> pwx_storage_profile IOPS storage tier the storage volumes that will be provisioned Yes if <code>storage_type</code> is <code>pwx</code> stork_version Version of the Portworx storage orchestration layer for Kubernetes Yes if <code>storage_type</code> is <code>pwx</code> portworx_version Version of the Portworx storage provider Yes if <code>storage_type</code> is <code>pwx</code> <p>Warning</p> <p>When deploying a ROKS cluster with OpenShift Data Foundation (fka OpenShift Container Storage/OCS), the minimum version of OpenShift is 4.7.</p>"},{"location":"30-reference/configuration/openshift/#openshift-on-vsphere","title":"OpenShift on vSphere","text":"<pre><code>openshift:\n- name: sample\n  domain_name: example.com\n  vsphere_name: sample\n  ocp_version: 4.8\n  control_plane_nodes: 3\n  control_plane_vm_definition: control-plane\n  compute_nodes: 3\n  compute_vm_definition: compute\n  api_vip: 10.99.92.51\n  ingress_vip: 10.99.92.52\n  cloud_native_toolkit: False\n  oadp: False\n  infrastructure:\n    openshift_cluster_network_cidr: 10.128.0.0/14\n  upstream_dns:\n  - name: sample-dns\n     zones:\n     - example.com\n     dns_servers:\n     - 172.31.2.73:53\n  gpu:\n    install: auto\n  openshift_ai:\n    install: auto\n    channel: auto\n  mcg:\n    install: True\n    storage_type: storage-class\n    storage_class: thin\n  openshift_storage:\n  - storage_name: nfs-storage\n    storage_type: nfs\n    nfs_server_name: sample-nfs\n  - storage_name: odf-storage\n    storage_type: odf\n    odf_storage_label: ocs\n    odf_storage_size_gb: 512\n    odf_dynamic_storage_class: thin\n</code></pre>"},{"location":"30-reference/configuration/openshift/#property-explanation-openshift-clusters-on-vsphere","title":"Property explanation OpenShift clusters on vSphere","text":"Property Description Mandatory Allowed values name Name of the OpenShift cluster Yes domain_name Domain name of the cluster, this will also depict the route to the API and ingress endpoints Yes ocp_version OpenShift version.  If you want to install <code>4.10</code>, specify <code>\"4.10\"</code> Yes &gt;= 4.6 control_plane_nodes Total number of control plane nodes, typically 3 Yes Integer control_plane_vm_definition <code>vm_definition</code> object that will be used to define number of vCPUs and memory for the control plane nodes Yes Existing <code>vm_definition</code> compute_nodes Total number of compute nodes Yes Integer compute_vm_definition <code>vm_definition</code> object that will be used to define number of vCPUs and memory for the compute nodes Yes Existing <code>vm_definition</code> api_vip Virtual IP address that the installer will provision for the API server Yes ingress_vip Virtual IP address that the installer will provision for the ingress server Yes cloud_native_toolkit Must the Cloud Native Toolkit (OpenShift GitOps) be installed? No True, False (default) oadp Must the OpenShift Advanced Data Protection operator be installed No True, False (default) infrastructure Infrastructure properties No infrastructure.openshift_cluster_network_cidr Network CIDR used by the OpenShift pods. Normally you would not have to change this, unless other systems in the network are in the 10.128.0.0/14 subnet. No CIDR openshift_logging[] Logging attributes for OpenShift cluster, see OpenShift logging No upstream_dns[] Upstream DNS servers(s), see Upstream DNS Servers No gpu Control Node Feature Discovery and NVIDIA GPU operators No gpu.install Must Node Feature Discovery and NVIDIA GPU operators be installed (Once installed, False does not uninstall). <code>auto</code> will install the operators if needed by any of the Cloud Pak/watsonx Yes auto, True, False openshift_ai Control installation of OpenShift AI No openshift_ai.install Must OpenShift AI be installed (Once installed, False does not uninstall). <code>auto</code> will install OpenShift AI if needed by any of the Cloud Pak/watsonx components Yes auto, True, False openshift_ai.channel Which oeprator channel must be installed No auto (default), stable, \u2026 mcg Multicloud Object Gateway properties No mcg.install Must Multicloud Object Gateway be installed (Once installed, False does not uninstall) Yes True, False mcg.storage_type Type of storage supporting the object Noobaa object storage Yes storage-class mcg.storage_class Storage class supporting the Noobaa object storage Yes Existing storage class openshift_storage[] List of storage definitions to be defined on OpenShift, see below for further explanation Yes"},{"location":"30-reference/configuration/openshift/#openshift_storage---openshift-storage-definitions_2","title":"openshift_storage[] - OpenShift storage definitions","text":"Property Description Mandatory Allowed values openshift_storage[] List of storage definitions to be defined on OpenShift Yes storage_name Name of the storage definition, to be referenced by the Cloud Pak Yes storage_type Type of storage class to create in the OpenShift cluster Yes nfs or odf dedicated_nodes Specify if dedicated nodes must be used for ODF No <code>True</code>, <code>False</code> nfs_server_name Name of the NFS server within the VPC Yes if <code>storage_type</code> is <code>nfs</code> Existing <code>nfs_server</code> odf_version Version of the ODF operator. If not specified, this will default to the <code>ocp_version</code> No &gt;= 4.6 odf_storage_label Label to be used for the dedicated ODF nodes in the cluster Yes if <code>storage_type</code> is <code>odf</code> odf_storage_size_gb Size of the ODF storage in Gibibytes (Gi) Yes if <code>storage_type</code> is <code>odf</code> odf_dynamic_storage_class Storage class that will be used for provisioning ODF. On vSphere clusters, <code>thin</code> is usually available after OpenShift installation Yes if <code>storage_type</code> is <code>odf</code> storage_vm_definition VM Definition that defines the virtual machine attributes for the ODF nodes Yes if <code>storage_type</code> is <code>odf</code>"},{"location":"30-reference/configuration/openshift/#openshift-on-aws---self-managed","title":"OpenShift on AWS - self-managed","text":"<pre><code>nfs_server:\n- name: sample-elastic\n  infrastructure:\n    aws_region: eu-west-1\n\nopenshift:\n- name: sample\n  ocp_version: 4.10.34\n  domain_name: cp-deployer.eu\n  compute_flavour: m5.4xlarge\n  compute_nodes: 3\n  cloud_native_toolkit: False\n  oadp: False\n  infrastructure:\n    type: self-managed\n    aws_region: eu-central-1\n    multi_zone: True\n    credentials_mode: Manual\n    private_only: True\n    machine_cidr: 10.2.1.0/24\n    openshift_cluster_network_cidr: 10.128.0.0/14\n    subnet_ids:\n    - subnet-06bbef28f585a0dd3\n    - subnet-0ea5ac344c0fbadf5\n    hosted_zone_id: Z08291873MCIC4TMIK4UP\n    ami_id: ami-09249dd86b1933dd5\n  mcg:\n    install: True\n    storage_type: storage-class\n    storage_class: gp3-csi\n  openshift_storage:\n  - storage_name: odf-storage\n    storage_type: odf\n    odf_storage_label: ocs\n    odf_storage_size_gb: 512\n  - storage_name: sample-elastic\n    storage_type: aws-elastic\n</code></pre>"},{"location":"30-reference/configuration/openshift/#property-explanation-openshift-clusters-on-aws-self-managed","title":"Property explanation OpenShift clusters on AWS (self-managed)","text":"Property Description Mandatory Allowed values name Name of the OpenShift cluster Yes ocp_version OpenShift version version, specified as <code>x.y.z</code> Yes &gt;= 4.6 domain_name Base domain name of the cluster. Together with the <code>name</code>, this will be the domain of the OpenShift cluster. Yes control_plane_flavour Flavour of the AWS servers used for the control plane nodes. <code>m5.xxlarge</code> is the recommended value     4 GB of memory Yes control_plane_nodes Total number of control plane Yes Integer compute_flavour Flavour of the AWS servers used for the compute nodes. <code>m5.4xlarge</code> is a large node with 16 cores and 64 GB of memory Yes compute_nodes Total number of compute nodes Yes Integer cloud_native_toolkit Must the Cloud Native Toolkit (OpenShift GitOps) be installed? No True, False (default) oadp Must the OpenShift Advanced Data Protection operator be installed No True, False (default) infrastructure Infrastructure properties Yes infrastructure.type Type of OpenShift cluster on AWS. Yes <code>rosa</code> or <code>self-managed</code> infrastructure.aws_region Region of AWS where cluster is deployed. Yes infrastructure.multi_zone Determines whether the OpenShift cluster is deployed across multiple availability zones. Default is True. No True (default), False infrastructure.credentials_mode Security requirement of the Cloud Credential Operator (COO) when doing installations with temporary AWS security credentials. Default (omit) is automatically handled by CCO. No Manual, Mint infrastructure.machine_cdr Machine CIDR. This value will be used to create the VPC and its subnets. In case of an existing VPC, specify the CIDR of that VPC. No CIDR infrastructure.openshift_cluster_network_cidr Network CIDR used by the OpenShift pods. Normally you would not have to change this, unless other systems in the network are in the 10.128.0.0/14 subnet. No CIDR infrastructure.subnet_ids Existing public and private subnet IDs in the VPC to be used for the OpenShift cluster.  Must be specified in combination with machine_cidr and hosted_zone_id. No Existing subnet IDs infrastructure.private_only Indicates whether the OpenShift can be accessed from the internet. Default is True No True, False infrastructure.hosted_zone_id ID of the AWS Route 53 hosted zone that controls the DNS entries. If not specified, the OpenShift installer will create a hosted zone for the specified <code>domain_name</code>. This attribute is only needed if you create the OpenShift cluster in an existing VPC No infrastructure.control_plane_iam_role If not standard, specify the IAM role that the OpenShift installer must use for the control plane nodes during cluster creation No infrastructure.compute_iam_role If not standard, specify the IAM role that the OpenShift installer must use for the compute nodes during cluster creation No infrastructure.ami_id ID of the AWS AMI to boot all images No openshift_logging[] Logging attributes for OpenShift cluster, see OpenShift logging No gpu Control Node Feature Discovery and NVIDIA GPU operators No gpu.install Must Node Feature Discovery and NVIDIA GPU operators be installed (Once installed, False does not uninstall). <code>auto</code> will install the operators if needed by any of the Cloud Pak/watsonx Yes auto, True, False openshift_ai Control installation of OpenShift AI No openshift_ai.install Must OpenShift AI be installed (Once installed, False does not uninstall). <code>auto</code> will install OpenShift AI if needed by any of the Cloud Pak/watsonx components Yes auto, True, False openshift_ai.channel Which oeprator channel must be installed No auto (default), stable, \u2026 mcg Multicloud Object Gateway properties No mcg.install Must Multicloud Object Gateway be installed (Once installed, False does not uninstall) Yes True, False mcg.storage_type Type of storage supporting the object Noobaa object storage Yes storage-class mcg.storage_class Storage class supporting the Noobaa object storage Yes Existing storage class openshift_storage[] List of storage definitions to be defined on OpenShift, see below for further explanation Yes <p>When deploying the OpenShift cluster within an existing VPC, you must specify the <code>machine_cidr</code> that covers all subnets and the subnet IDs within the VPC. For example: <pre><code>    machine_cidr: 10.243.0.0/24\n    subnets_ids:\n    - subnet-0e63f662bb1842e8a\n    - subnet-0673351cd49877269\n    - subnet-00b007a7c2677cdbc\n    - subnet-02b676f92c83f4422\n    - subnet-0f1b03a02973508ed\n    - subnet-027ca7cc695ce8515\n</code></pre></p>"},{"location":"30-reference/configuration/openshift/#openshift_storage---openshift-storage-definitions_3","title":"openshift_storage[] - OpenShift storage definitions","text":"Property Description Mandatory Allowed values openshift_storage[] List of storage definitions to be defined on OpenShift Yes storage_name Name of the storage definition, to be referenced by the Cloud Pak Yes storage_type Type of storage class to create in the OpenShift cluster Yes odf, aws-elastic dedicated_nodes Specify if dedicated nodes must be used for ODF No <code>True</code>, <code>False</code> odf_version Version of the ODF operator. If not specified, this will default to the <code>ocp_version</code> No odf_storage_label Label to be used for the dedicated ODF nodes in the cluster Yes if <code>storage_type</code> is <code>odf</code> odf_storage_size_gb Size of the ODF storage in Gibibytes (Gi) Yes if <code>storage_type</code> is <code>odf</code> odf_dynamic_storage_class Storage class that will be used for provisioning ODF. <code>gp3-csi</code> is usually available after OpenShift installation No"},{"location":"30-reference/configuration/openshift/#openshift-on-aws---rosa","title":"OpenShift on AWS - ROSA","text":"<pre><code>nfs_server:\n- name: sample-elastic\n  infrastructure:\n    aws_region: eu-west-1\n\nopenshift:\n- name: sample\n  ocp_version: 4.10.34\n  compute_flavour: m5.4xlarge\n  compute_nodes: 3\n  cloud_native_toolkit: False\n  oadp: False\n  infrastructure:\n    type: rosa\n    aws_region: eu-central-1\n    multi_zone: True\n    use_sts: False\n    credentials_mode: Manual\n  upstream_dns:\n  - name: sample-dns\n     zones:\n     - example.com\n     dns_servers:\n     - 172.31.2.73:53\n  gpu:\n    install: auto\n  openshift_ai:\n    install: auto\n    channel: auto\n  mcg:\n    install: True\n    storage_type: storage-class\n    storage_class: gp3-csi\n  openshift_storage:\n  - storage_name: odf-storage\n    storage_type: odf\n    odf_storage_label: ocs\n    odf_storage_size_gb: 512\n  - storage_name: sample-elastic\n    storage_type: aws-elastic\n</code></pre>"},{"location":"30-reference/configuration/openshift/#property-explanation-openshift-clusters-on-aws-rosa","title":"Property explanation OpenShift clusters on AWS (ROSA)","text":"Property Description Mandatory Allowed values name Name of the OpenShift cluster Yes ocp_version OpenShift version version, specified as <code>x.y.z</code> Yes &gt;= 4.6 compute_flavour Flavour of the AWS servers used for the compute nodes. <code>m5.4xlarge</code> is a large node with 16 cores and 64 GB of memory Yes cloud_native_toolkit Must the Cloud Native Toolkit (OpenShift GitOps) be installed? No True, False (default) oadp Must the OpenShift Advanced Data Protection operator be installed No True, False (default) infrastructure Infrastructure properties Yes infrastructure.type Type of OpenShift cluster on AWS. Yes <code>rosa</code> or <code>self-managed</code> infrastructure.aws_region Region of AWS where cluster is deployed. Yes infrastructure.multi_zone Determines whether the OpenShift cluster is deployed across multiple availability zones. Default is True. No True (default), False infrastructure.use_sts Determines whether AWS Security Token Service must be used by the ROSA installer. Default is False. No True, False (default) infrastructure.credentials_mode Change the security requirement of the Cloud Credential Operator (COO). Default (omit) is automatically handled by CCO. No Manual, Mint infrastructure.machine_cdr Machine CIDR, for example 10.243.0.0/16. No CIDR infrastructure.subnet_ids Existing public and private subnet IDs in the VPC to be used for the OpenShift cluster.  Must be specified in combination with machine_cidr. No Existing subnet IDs compute_nodes Total number of compute nodes Yes Integer upstream_dns[] Upstream DNS servers(s), see Upstream DNS Servers No openshift_logging[] Logging attributes for OpenShift cluster, see OpenShift logging No upstream_dns[] Upstream DNS servers(s), see Upstream DNS Servers No gpu Control Node Feature Discovery and NVIDIA GPU operators No gpu.install Must Node Feature Discovery and NVIDIA GPU operators be installed (Once installed, False does not uninstall). <code>auto</code> will install the operators if needed by any of the Cloud Pak/watsonx Yes auto, True, False openshift_ai Control installation of OpenShift AI No openshift_ai.install Must OpenShift AI be installed (Once installed, False does not uninstall). <code>auto</code> will install OpenShift AI if needed by any of the Cloud Pak/watsonx components Yes auto, True, False openshift_ai.channel Which oeprator channel must be installed No auto (default), stable, \u2026 mcg Multicloud Object Gateway properties No mcg.install Must Multicloud Object Gateway be installed (Once installed, False does not uninstall) Yes True, False mcg.storage_type Type of storage supporting the object Noobaa object storage Yes storage-class mcg.storage_class Storage class supporting the Noobaa object storage Yes Existing storage class openshift_storage[] List of storage definitions to be defined on OpenShift, see below for further explanation Yes <p>When deploying the OpenShift cluster within an existing VPC, you must specify the <code>machine_cidr</code> that covers all subnets and the subnet IDs within the VPC. For example: <pre><code>    machine_cidr: 10.243.0.0/24\n    subnets_ids:\n    - subnet-0e63f662bb1842e8a\n    - subnet-0673351cd49877269\n    - subnet-00b007a7c2677cdbc\n    - subnet-02b676f92c83f4422\n    - subnet-0f1b03a02973508ed\n    - subnet-027ca7cc695ce8515\n</code></pre></p>"},{"location":"30-reference/configuration/openshift/#openshift_storage---openshift-storage-definitions_4","title":"openshift_storage[] - OpenShift storage definitions","text":"Property Description Mandatory Allowed values openshift_storage[] List of storage definitions to be defined on OpenShift Yes storage_name Name of the storage definition, to be referenced by the Cloud Pak Yes storage_type Type of storage class to create in the OpenShift cluster Yes ocs, odf, aws-elastic odf_version Version of the ODF operator. If not specified, this will default to the <code>ocp_version</code> No odf_storage_label Label to be used for the dedicated ODF nodes in the cluster Yes if <code>storage_type</code> is <code>odf</code> odf_storage_size_gb Size of the ODF storage in Gibibytes (Gi) Yes if <code>storage_type</code> is <code>odf</code> odf_dynamic_storage_class Storage class that will be used for provisioning ODF. <code>gp3-csi</code> is usually available after OpenShift installation No"},{"location":"30-reference/configuration/openshift/#openshift-on-microsoft-azure---aro","title":"OpenShift on Microsoft Azure - ARO","text":"<pre><code>openshift:\n- name: sample\n  azure_name: sample\n  domain_name: example.com\n  ocp_version: 4.10.54\n  cloud_native_toolkit: False\n  oadp: False\n  infrastructure:\n    type: aro\n    multi_zone: True\n    private_only: False\n  network:\n    machine_cidr: \"10.0.0.0/16\"\n    pod_cidr: \"10.128.0.0/14\"\n    service_cidr: \"172.30.0.0/16\"\n  gpu:\n    install: auto\n  openshift_ai:\n    install: auto\n    channel: auto\n  openshift_storage:\n  - storage_name: odf-storage\n    storage_type: odf\n    odf_storage_label: ocs\n    odf_storage_size_gb: 512\n    odf_dynamic_storage_class: managed-premium\n</code></pre>"},{"location":"30-reference/configuration/openshift/#property-explanation-for-openshift-cluster-on-microsoft-azure-aro","title":"Property explanation for OpenShift cluster on Microsoft Azure (ARO)","text":"<p>Warning</p> <p>You are not allowed to specify the OCP version of the ARO cluster. The latest current version is provisioned automatically instead no matter what value is specified in the \"ocp_version\" parameter. The \"ocp_version\" parameter is mandatory for compatibility with other layers of the provisioning, such as the OpenShift client. For instance, the value is used by the process which downloads and installs the <code>oc</code> client. Please, specify the value according to what OCP version will be provisioned.</p> Property Description Mandatory Allowed values name Name of the OpenShift cluster Yes azure_name Name of the <code>azure</code> element in the configuration Yes domain_name Domain mame of the cluster, if you want to override the name generated by Azure No ocp_version The OpenShift version. If you want to install <code>4.10</code>, specify <code>\"4.10\"</code> Yes &gt;= 4.6 cloud_native_toolkit Must the Cloud Native Toolkit (OpenShift GitOps) be installed? No True, False (default) oadp Must the OpenShift Advanced Data Protection operator be installed No True, False (default) infrastructure.type Type of OpenShift cluster Yes aro, self-managed infrastructure.multi_zone Specify if the cluster is provisioned in a single zone or 3 zones Yes True, False infrastructure.private_only Specify if the cluster is provisioned in a private virtual network, not allowed for ARO Yes True, False network Cluster network attributes Yes network.machine_cidr CIDR of provisioned machines Yes Must be a minimum of /18 or larger. network.pod_cidr CIDR of pod network Yes Must be a minimum of /18 or larger. network.service_cidr CIDR of service network Yes Must be a minimum of /18 or larger. openshift_logging[] Logging attributes for OpenShift cluster, see OpenShift logging No upstream_dns[] Upstream DNS servers(s), see Upstream DNS Servers No gpu Control Node Feature Discovery and NVIDIA GPU operators No gpu.install Must Node Feature Discovery and NVIDIA GPU operators be installed (Once installed, False does not uninstall). <code>auto</code> will install the operators if needed by any of the Cloud Pak/watsonx Yes auto, True, False openshift_ai Control installation of OpenShift AI No openshift_ai.install Must OpenShift AI be installed (Once installed, False does not uninstall). <code>auto</code> will install OpenShift AI if needed by any of the Cloud Pak/watsonx components Yes auto, True, False openshift_ai.channel Which oeprator channel must be installed No auto (default), stable, \u2026 mcg Multicloud Object Gateway properties No mcg.install Must Multicloud Object Gateway be installed (Once installed, False does not uninstall) Yes True, False mcg.storage_type Type of storage supporting the object Noobaa object storage Yes storage-class mcg.storage_class Storage class supporting the Noobaa object storage Yes Existing storage class openshift_storage[] List of storage definitions to be defined on OpenShift, see below for further explanation Yes"},{"location":"30-reference/configuration/openshift/#openshift_storage---openshift-storage-definitions_5","title":"openshift_storage[] - OpenShift storage definitions","text":"Property Description Mandatory Allowed values openshift_storage[] List of storage definitions to be defined on OpenShift Yes storage_name Name of the storage Yes storage_type Type of storage class to create in the OpenShift cluster Yes <code>odf</code> or <code>nfs</code> dedicated_nodes Specify if dedicated nodes must be used for ODF No <code>True</code>, <code>False</code> odf_version Version of the ODF operator. If not specified, this will default to the <code>ocp_version</code> No odf_storage_label Label (or rather a name) to be used for the dedicated ODF nodes in the cluster - together with the combination of Azure location and zone id Yes if <code>storage_type</code> is <code>odf</code> odf_storage_size_gb Size of the ODF storage in Gibibytes (Gi) Yes if <code>storage_type</code> is <code>odf</code> odf_dynamic_storage_class Storage class that will be used for provisioning ODF. In Azure, you must select <code>managed-premium</code> Yes if <code>storage_type</code> is <code>odf</code> <code>managed-premium</code>"},{"location":"30-reference/configuration/openshift/#openshift-on-microsoft-azure---self-managed","title":"OpenShift on Microsoft Azure - Self-managed","text":"<pre><code>openshift:\n- name: sample\n  azure_name: sample\n  domain_name: example.com\n  ocp_version: 4.10.54\n  cloud_native_toolkit: False\n  oadp: False\n  infrastructure:\n    type: self-managed\n    multi_zone: False\n    private_only: False\n  network:\n    machine_cidr: \"10.0.0.0/16\"\n    pod_cidr: \"10.128.0.0/14\"\n    service_cidr: \"172.30.0.0/16\"\n  gpu:\n    install: auto\n  openshift_ai:\n    install: auto\n    channel: auto\n  openshift_storage:\n  - storage_name: odf-storage\n    storage_type: odf\n    odf_storage_label: ocs\n    odf_storage_size_gb: 512\n    odf_dynamic_storage_class: managed-premium\n</code></pre>"},{"location":"30-reference/configuration/openshift/#property-explanation-for-openshift-cluster-on-microsoft-azure---self-managed","title":"Property explanation for OpenShift cluster on Microsoft Azure - Self-managed","text":"<p>Warning</p> <p>You are not allowed to specify the OCP version of the ARO cluster. The latest current version is provisioned automatically instead no matter what value is specified in the \"ocp_version\" parameter. The \"ocp_version\" parameter is mandatory for compatibility with other layers of the provisioning, such as the OpenShift client. For instance, the value is used by the process which downloads and installs the <code>oc</code> client. Please, specify the value according to what OCP version will be provisioned.</p> Property Description Mandatory Allowed values name Name of the OpenShift cluster Yes azure_name Name of the <code>azure</code> element in the configuration Yes domain_name Domain mame of the cluster, if you want to override the name generated by Azure No ocp_version The OpenShift version. If you want to install <code>4.10</code>, specify <code>\"4.10\"</code> Yes &gt;= 4.6 cloud_native_toolkit Must the Cloud Native Toolkit (OpenShift GitOps) be installed? No True, False (default) oadp Must the OpenShift Advanced Data Protection operator be installed No True, False (default) infrastructure.type Type of OpenShift cluster Yes aro, self-managed infrastructure.multi_zone Specify if the cluster is provisioned in a single zone or 3 zones Yes True, False infrastructure.private_only Specify if the cluster is provisioned in a private virtual network Yes True, False network Cluster network attributes Yes network.machine_cidr CIDR of provisioned machines Yes Must be a minimum of /18 or larger. network.pod_cidr CIDR of pod network Yes Must be a minimum of /18 or larger. network.service_cidr CIDR of service network Yes Must be a minimum of /18 or larger. openshift_logging[] Logging attributes for OpenShift cluster, see OpenShift logging No upstream_dns[] Upstream DNS servers(s), see Upstream DNS Servers No gpu Control Node Feature Discovery and NVIDIA GPU operators No gpu.install Must Node Feature Discovery and NVIDIA GPU operators be installed (Once installed, False does not uninstall). <code>auto</code> will install the operators if needed by any of the Cloud Pak/watsonx Yes auto, True, False openshift_ai Control installation of OpenShift AI No openshift_ai.install Must OpenShift AI be installed (Once installed, False does not uninstall). <code>auto</code> will install OpenShift AI if needed by any of the Cloud Pak/watsonx components Yes auto, True, False openshift_ai.channel Which oeprator channel must be installed No auto (default), stable, \u2026 mcg Multicloud Object Gateway properties No mcg.install Must Multicloud Object Gateway be installed (Once installed, False does not uninstall) Yes True, False mcg.storage_type Type of storage supporting the object Noobaa object storage Yes storage-class mcg.storage_class Storage class supporting the Noobaa object storage Yes Existing storage class openshift_storage[] List of storage definitions to be defined on OpenShift, see below for further explanation Yes"},{"location":"30-reference/configuration/openshift/#openshift_storage---openshift-storage-definitions_6","title":"openshift_storage[] - OpenShift storage definitions","text":"Property Description Mandatory Allowed values openshift_storage[] List of storage definitions to be defined on OpenShift Yes storage_name Name of the storage Yes storage_type Type of storage class to create in the OpenShift cluster Yes <code>odf</code> or <code>nfs</code> dedicated_nodes Specify if dedicated nodes must be used for ODF No <code>True</code>, <code>False</code> odf_version Version of the ODF operator. If not specified, this will default to the <code>ocp_version</code> No odf_storage_label Label (or rather a name) to be used for the dedicated ODF nodes in the cluster - together with the combination of Azure location and zone id Yes if <code>storage_type</code> is <code>odf</code> odf_storage_size_gb Size of the ODF storage in Gibibytes (Gi) Yes if <code>storage_type</code> is <code>odf</code> odf_dynamic_storage_class Storage class that will be used for provisioning ODF. In Azure, you must select the storage class"},{"location":"30-reference/configuration/private-registry/","title":"Private registry","text":"<p>In cases where the OpenShift cluster is in an environment with limited internet connectivity, you may want OpenShift to pull Cloud Pak images from a private image registry (aka container registry). There may also be other reasons for choosing a private registry over the entitled registry.</p>"},{"location":"30-reference/configuration/private-registry/#configuring-a-private-registry","title":"Configuring a private registry","text":"<p>The below steps outline how to configure a private registry for a Cloud Pak deployment. When the <code>image_registry</code> object is referenced by the Cloud Pak object (such as <code>cp4d</code>), the deployer makes the following changes in OpenShift so that images are pulled from the private registry:</p> <ul> <li>Global pull secret: The image registry's credentials are retrieved from the vault (the secret name must be <code>image-registry-&lt;name&gt;</code> and an entry for the registry is added to the global pull secret (secret <code>pull-secret</code> in project <code>openshift-config</code>).</li> <li>ImageContentSourcePolicy: This is a mapping between the original location of the image, for example <code>quay.io/opencloudio/zen-metastoredb@sha256:582cac2366dda8520730184dec2c430e51009a854ed9ccea07db9c3390e13b29</code> is mapped to <code>registry.coc.uk.ibm.com:15000/opencloudio/zen-metastoredb@sha256:582cac2366dda8520730184dec2c430e51009a854ed9ccea07db9c3390e13b29</code>.</li> <li>Image registry settings: OpenShift keeps image registry settings in custom resource <code>image.config.openshift.io/cluster</code>. If a private registry with a self-signed certificate is configured, certificate authority's PEM secret must be created as a configmap in the <code>openshift-config</code> project. The deployer uses the vault secret referenced in <code>registry_trusted_ca_secret</code> property to create or update the configmap so that OpenShift can connect to the registry in a secure manner. Alternatively, you add the <code>registry_insecure: true</code> property to pull images without checking the certificate.</li> </ul>"},{"location":"30-reference/configuration/private-registry/#image_registry","title":"<code>image_registry</code>","text":"<p>Defines a private registry that will be used for pulling the Cloud Pak container images from. Additionally, if the Cloud Pak entitlement key was specified at run time of the deployer, the images defined by the case files will be mirrored to this private registry. <pre><code>image_registry:\n- name: cpd463\n  registry_host_name: registry.example.com\n  registry_port: 5000\n  registry_insecure: false\n  registry_trusted_ca_secret: cpd463-ca-bundle\n</code></pre></p>"},{"location":"30-reference/configuration/private-registry/#properties","title":"Properties","text":"Property Description Mandatory Allowed values name Name by which the image registry is identified. Yes registry_host_name Host name or IP address of the registry server Yes registry_port Port that the image registry listens on. Default is the https port (443) No registry_namespace Namespace (path) within the registry that holds the Cloud Pak images. Mandatory only when using the IBM Cloud Container Registry (ICR) No registry_insecure Defines whether insecure registry access with a self-signed certificate is allowed No True, False (default) registry_trusted_ca_secret Defines the vault secret which holds the certificate authority bundle that must be used when connecting to this private registry. This parameter cannot be specified if <code>registry_insecure</code> is also specified. No <p>Warning</p> <p>The <code>registry_host_name</code> you specify in the <code>image_registry</code> definition must also be available for DNS lookup within OpenShift. If the registry runs on a server that is not registered in the DNS, use its IP address instead of a host name.</p> <p>When mirroring images, the deployer connects to the registry using the host name and port. If the port is omitted, the standard https protocol (443) is used. If a <code>registry_namespace</code> is specified, for example when using the IBM Container Registry on IBM Cloud, it will be appended to the registry URL.</p> <p>The user and password to connect to the registry will be retrieved from the vault, using secret <code>image-registry-&lt;your_image_registry_name&gt;</code> and must be stored in the format <code>registry_user:registry_password</code>. For example, if you want to connect to the image registry <code>cpd404</code> with user <code>admin</code> and password <code>very_s3cret</code>, you would create a secret as follows: <pre><code>cp-deploy.sh vault set \\\n  -vs image-registry-cpd463 \\\n  -vsv \"admin:very_s3cret\"\n</code></pre></p> <p>If you need to connect to a private registry which is not signed by a public certificate authority, you have two choices: * Store the PEM certificate that that holds the CA bundle in a vault secret and specify that secret for the <code>registry_trusted_ca_secret</code> property. This is the recommended method for private registries. * Specify <code>registry_insecure: false</code> (not recommended): This means that the registry (and port) will be marked as insecure and OpenShift will pull images from it, even if its certificate is self-signed.</p> <p>For example, if you have a file <code>/tmp/ca.crt</code> with the PEM certificate for the certificate authority, you can do the following: <pre><code>cp-deploy.sh vault set \\\n  -vs cpd463-ca-bundle \\\n  -vsf /tmp/ca.crt\n</code></pre></p> <p>This will create a vault secret which the deployer will use to populate a <code>configmap</code> in the <code>openshift-config</code> project, which in turn is referenced by the <code>image.config.openshift.io/cluster</code> custom resource. For the above configuration, configmap <code>cpd404-ca-bundle</code> would be created and teh <code>image.config.openshift.io/cluster</code> would look something like this: <pre><code>apiVersion: config.openshift.io/v1\nkind: Image\nmetadata:\n...\n...\n  name: cluster\nspec:\n  additionalTrustedCA:\n    name: cpd463-ca-bundle\n</code></pre></p>"},{"location":"30-reference/configuration/private-registry/#using-the-ibm-container-registry-as-a-private-registry","title":"Using the IBM Container Registry as a private registry","text":"<p>If you want to use a private registry when running the deployer for a ROKS cluster on IBM Cloud, you must use the IBM Container Registry (ICR) service. The deployer will automatically create the specified namespace in the ICR and set up the credentials accordingly. Configure an image_registry object with the host name of the private registry and the namespace that holds the images. An example of using the ICR as a private registry:</p> <pre><code>image_registry:\n- name: cpd463\n  registry_host_name: de.icr.io\n  registry_namespace: cpd463\n</code></pre> <p>The registry host name must end with <code>icr.io</code> and the registry namespace is mandatory. No other properties are needed; the deployer will retrieve them from IBM Cloud.</p> <p>If you have already created the ICR namespace, create a vault secret for the image registry credentials: <pre><code>cp-deploy.sh vault set \\\n  -vs image-registry-cpd463\n  -vsv \"admin:very_s3cret\"\n</code></pre></p> <p>An example of configuring the private registry for a <code>cp4d</code> object is below: <pre><code>cp4d:\n- project: cpd-instance\n  openshift_cluster_name: {{ env_id }}\n  cp4d_version: 4.8.3\n  image_registry_name: cpd463\n</code></pre></p> <p>The Cloud Pak for Data installation refers to the <code>cpd463</code> <code>image_registry</code> object.</p> <p>If the <code>ibm_cp_entitlement_key</code> secret is in the vault at the time of running the deployer, the required images will be mirrored from the entitled registry to the private registry. If all images are already available in the private registry, just specify the <code>--skip-mirror-images</code> flag when you run the deployer.</p>"},{"location":"30-reference/configuration/private-registry/#using-a-private-registry-for-the-cloud-pak-installation-non-ibm-cloud","title":"Using a private registry for the Cloud Pak installation (non-IBM Cloud)","text":"<p>Configure an image_registry object with the host name of the private registry and some optional properties such as port number, CA certificate and whether insecure access to the registry is allowed.</p> <p>Example: <pre><code>image_registry:\n- name: cpd463\n  registry_host_name: registry.example.com\n  registry_port: 5000\n  registry_insecure: false\n  registry_trusted_ca_secret: cpd463-ca-bundle\n</code></pre></p> <p>Warning</p> <p>The <code>registry_host_name</code> you specify in the <code>image_registry</code> definition must also be available for DNS lookup within OpenShift. If the registry runs on a server that is not registered in the DNS, use its IP address instead of a host name.</p> <p>To create the vault secret for the image registry credentials: <pre><code>cp-deploy.sh vault set \\\n  -vs image-registry-cpd463\n  -vsv \"admin:very_s3cret\"\n</code></pre></p> <p>To create the vault secret for the CA bundle: <pre><code>cp-deploy.sh vault set \\\n  -vs cpd463-ca-bundle\n  -vsf /tmp/ca.crt\n</code></pre></p> <p>Where <code>ca.crt</code> looks something like this: <pre><code>-----BEGIN CERTIFICATE-----\nMIIFszCCA5ugAwIBAgIUT02v9OdgdvjgQVslCuL0wwCVaE8wDQYJKoZIhvcNAQEL\nBQAwaTELMAkGA1UEBhMCVVMxETAPBgNVBAgMCE5ldyBZb3JrMQ8wDQYDVQQHDAZB\ncm1vbmsxFjAUBgNVBAoMDUlCTSBDbG91ZCBQYWsxHjAcBgNVBAMMFUlCTSBDbG91\n...\nmcutkgtbkq31XYZj0CiM451Qp8KnTx0=\n-----END CERTIFICATE-\n</code></pre></p> <p>An example of configuring the private registry for a <code>cp4d</code> object is below: <pre><code>cp4d:\n- project: cpd-instance\n  openshift_cluster_name: {{ env_id }}\n  cp4d_version: 4.8.3\n  image_registry_name: cpd463\n</code></pre></p> <p>The Cloud Pak for Data installation refers to the <code>cpd463</code> <code>image_registry</code> object.</p> <p>If the <code>ibm_cp_entitlement_key</code> secret is in the vault at the time of running the deployer, the required images will be mirrored from the entitled registry to the private registry. If all images are already available in the private registry, just specify the <code>--skip-mirror-images</code> flag when you run the deployer.</p>"},{"location":"30-reference/configuration/redhat-sso/","title":"Red Hat Single Sign-on (SSO) configuration","text":"<p>You can configure Red Hat Single Sign-on (SSO) to be installed on the OpenShift cluster as an Identity Provider (IdP). Red Hat SSO implements the open-source Keycloak project which offers a user registry and can also federate other IdPs.</p>"},{"location":"30-reference/configuration/redhat-sso/#red-hat-sso-configuration---openshift_redhat_sso","title":"Red Hat SSO configuration - <code>openshift_redhat_sso</code>","text":"<p>An <code>openshift_redhat_sso</code> resource indicates that the Red Hat Single Sign-on operator must be installed on the referenced OpenShift cluster. A single SSO configuration can have only 1 Keycloak realms defined. The Keycloak realm holds all configuration needed for authentication. If you want to host more than 1 Keycloak server on the cluster, specify multiple <code>openshift_redhat_sso</code> entries, each with its own <code>keycloak_name</code>. The <code>keycloak_name</code> also determines the OpenShift project that will be created.</p> <pre><code>openshift_redhat_sso:\n- openshift_cluster_name: \"{{ env_id }}\"\n  keycloak_name: ibm-keycloak\n  groups:\n  - name: kc-cp4d-admins\n    state: present\n  - name: kc-cp4d-data-engineers\n    state: present\n  - name: kc-cp4d-data-scientists\n    state: present\n  - name: kc-cp4d-monitors\n    state: present\n</code></pre> <p>The above configuration installs the Red Hat SSO operator in OpenShift project <code>ibm-keycloak</code> and creates a Keycloak instance named <code>ibm-keycloak</code>. The instance has a single realm: <code>master</code> which contains the groups, users and clients which are then leveraged by Cloud Pak Foundational Services.</p> <p>Currently you can only define Keycloak groups which are later mapped to Cloud Pak for Data user groups. Creating users and setting up federated identity providers must be done by logging into Keycloak.</p> <p>The Keycloak name is referenced in the Zen Access Control resource and this is also where the mapping from Keycloak groups to Cloud Pak for Data groups takes place. </p>"},{"location":"30-reference/configuration/redhat-sso/#property-explanation","title":"Property explanation","text":"Property Description Mandatory Allowed values openshift_cluster_name Name of OpenShift cluster onto which the Red Hat SSO operator is installed Yes. if more than 1 <code>openshift</code> resource in the configuration keycloak_name Name of the Keycloak server, this also determines the name of the project into which the Keycloak server will be created Yes .groups[] Groups that will be created in the Keycloak realm Yes .name Name of the Keycloak group Yes .state Whether the group is present or absent Yes <code>present</code>, <code>absent</code>"},{"location":"30-reference/configuration/topologies/","title":"Deployment topologies","text":"<p>Configuration of the topology to be deployed typically boils down to choosing the cloud infrastructure you want to deploy, then choosing the type of OpenShift and storage, integrating with infrastructure services and then setting up the Cloud Pak(s). For most initial implementations, a basic deployment will suffice and later this can be extended with additional configuration.</p> <p>Depicted below is the basic deployment topology, followed by a topology with all bells and whistles.</p>"},{"location":"30-reference/configuration/topologies/#basic-deployment","title":"Basic deployment","text":"<p>For more details on each of the configuration elements, refer to:</p> <ul> <li>Infrastructure</li> <li>OpenShift</li> <li>Cloud Pak</li> <li>Cloud Pak Cartridges</li> <li>Cloud Pak Instances</li> <li>Cloud Pak Assets</li> </ul>"},{"location":"30-reference/configuration/topologies/#extended-deployment","title":"Extended deployment","text":"<p>For more details about extended deployment, refer to:</p> <ul> <li>Monitoring</li> <li>Logging and auditing</li> <li>Private registry</li> <li>DNS Servers</li> <li>Cloud Pak for Data access control</li> <li>Cloud Pak for Data SAML</li> </ul>"},{"location":"30-reference/configuration/vault/","title":"Vault configuration","text":""},{"location":"30-reference/configuration/vault/#vault-configuration_1","title":"Vault configuration","text":"<p>Throughout the deployment process, the Cloud Pak Deployer will create secrets in a vault and retrieve them later. Examples of secrets are: ssh keys, Cloud Pak for Data admin password. Additionally, when provisioning infrastructure no the IBM Cloud, the resulting Terraform state file is also stored in the vault so it can be used later if the configuration needs to be changed.</p> <p>Configuration of the vault is done through a <code>vault</code> object in the configuration. If you want to use the file-based vault in the status directory, you do not need to configure anything.</p> <p>The following Vault implementations can be used to store and retrieve secrets: - File Vault (no encryption) - Ansible Vault (encrypted with password) - IBM Cloud Secrets Manager - Hashicorp Vault (token authentication) - Hashicorp Vault (certificate authentication)</p> <p>The File Vault is the default vault and also the simplest. It does not require a password and all secrets are stored in base-64 encoding in a properties file under the <code>&lt;status_directory&gt;/vault</code> directory. The name of the vault file is the <code>environment_name</code> you specified in the global configuration, inventory file or at the command line.</p> <p>The Ansible Vault provides encryption for secrets using Ansible's built-in vault functionality. Secrets are stored in encrypted files under the <code>&lt;status_directory&gt;/vault</code> directory, protected by a password file. This provides a good balance between security and ease of use without requiring external services.</p> <p>All of the other vault options require some secret manager (IBM Cloud service or Hashicorp Vault) to be available and you need to specify a password or provide a certificate.</p> <p>Sample Vault config: <pre><code>vault:\n  vault_type: file-vault\n  vault_authentication_type: none\n</code></pre></p>"},{"location":"30-reference/configuration/vault/#properties-for-all-vault-implementations","title":"Properties for all vault implementations","text":"Property Description Mandatory Allowed values vault_type Chosen implementation of the vault Yes file-vault, ansible-vault, ibmcloud-vault, hashicorp-vault"},{"location":"30-reference/configuration/vault/#properties-for-file-vault","title":"Properties for <code>file-vault</code>","text":"Property Description Mandatory Allowed values vault_authentication_type Authentication method for the file vault No none"},{"location":"30-reference/configuration/vault/#properties-for-ansible-vault","title":"Properties for <code>ansible-vault</code>","text":"Property Description Mandatory Allowed values vault_authentication_type Authentication method for ansible vault Yes password-file vault_password_file Path to the file containing the ansible-vault password Yes <p>Sample Ansible Vault config: <pre><code>vault:\n  vault_type: ansible-vault\n  vault_authentication_type: password-file\n  vault_password_file: /path/to/vault-password-file\n</code></pre></p> <p>Note: The password file should contain only the password (no newline at the end is recommended). You can create it using: <code>echo -n \"your-secure-password\" &gt; /path/to/vault-password-file</code></p>"},{"location":"30-reference/configuration/vault/#properties-for-ibmcloud-vault","title":"Properties for <code>ibmcloud-vault</code>","text":"Property Description Mandatory Allowed values vault_authentication_type Authentication method for the file vault No api-key vault_url URL for the IBM Cloud secrets manager instance Yes"},{"location":"30-reference/configuration/vault/#properties-for-hashicorp-vault","title":"Properties for <code>hashicorp-vault</code>","text":"Property Description Mandatory Allowed values vault_authentication_type Authentication method for the file vault No api-key, certificate vault_url URL for the Hashicorp vault, this is typically https://hostname:8200 Yes vault_api_key When authentication type is api-key, the field to authenticate with Yes vault_secret_path Default secret path to store and retrieve secrets into/from Yes vault_secret_field Default field to store or retrieve secrets Yes vault_secret_path_append_group Determines whether or not the secrete group will be appended to the path Yes True (default), False vault_secret_base64 Depicts if secrets are stored in base64 format for Hashicorp Vault Yes True (default), False"},{"location":"30-reference/process/configure-cloud-pak/","title":"Configure the Cloud Pak(s)","text":"<p>This stage focuses on post-installation configuration of the Cloud Paks and cartridges.</p>"},{"location":"30-reference/process/configure-cloud-pak/#cloud-pak-for-data","title":"Cloud Pak for Data","text":""},{"location":"30-reference/process/configure-cloud-pak/#web-interface-certificate","title":"Web interface certificate","text":"<p>When provisioning on IBM Cloud ROKS, a CA-signed certificate for the ingress subdomain is automatically generated in the IBM Cloud certificate manager. The deployer retrieves the certificate and adds it to the secret that stores the certificate key. This will avoid getting a warning when opening the Cloud Pak for Data home page.</p>"},{"location":"30-reference/process/configure-cloud-pak/#configure-identity-and-access-management","title":"Configure identity and access management","text":"<p>For Cloud Pak for Data you can configure:</p> <ul> <li>SAML for Single Sign-on. When specified in the <code>cp4d_saml_config</code> object, the deployer configures the user management pods to redirect logins to the identity provider (idP) of choice.</li> <li>LDAP configuration. LDAP can be used both for authentication (if no SSO has been configured) and for access management by mapping LDAP groups to Cloud Pak for Data user groups. Specify the LDAP or LDAPS properties in the <code>cp4d_ldap_config</code> object so that the deployer configures it for Cloud Pak for Data. If SAML has been configured for authentication, the configured LDAP server is only used for access management.</li> <li>User group configuration. This creates user-defined user groups in Cloud Pak for Data to match the LDAP configuration. The configuration object used for this is <code>cp4d_user_group_configuration</code>.</li> </ul>"},{"location":"30-reference/process/configure-cloud-pak/#provision-instances","title":"Provision instances","text":"<p>Some cartridges such as Data Virtualization have the ability to create one or more instances to run an isolated installation of the cartridge. If instances have been configured for the cartridge, this steps provisions them. The following Cloud Pak for Data cartridges are currently supported for creating instances:</p> <ul> <li>Analytics engine powered by Apache Spark (<code>analytics-engine</code>)</li> <li>Db2 OLTP (<code>db2</code>)</li> <li>Cognos Analytics (<code>ca</code>)</li> <li>Data Virtualization (<code>dv</code>)</li> </ul>"},{"location":"30-reference/process/configure-cloud-pak/#configure-instance-access","title":"Configure instance access","text":"<p>Cloud Pak for Data does not support group-defined access to cartridge instances. After creation of the instances (and also when the deployer is run with the <code>--cp-config-only</code> flag), the permissions of users accessing the instance is configured.</p> <p>For Cognos Analytics, the Cognos Authorization process is run to apply user group permissions to the Cognos Analytics instance.</p>"},{"location":"30-reference/process/configure-cloud-pak/#create-or-change-platform-connections","title":"Create or change platform connections","text":"<p>Cloud Pak for Data defines data source connections at the platform level and these can be reused in some cartridges like Watson Knowledge Catalog and Watson Studio. The <code>cp4d_connection</code> object defines each of the platform connections that must be managed by the deployer.</p>"},{"location":"30-reference/process/configure-cloud-pak/#backup-and-restore-connections","title":"Backup and restore connections","text":"<p>If you want to back up or restore platform connections, the <code>cp4d_backup_restore_connections</code> object defines the JSON file that will be used for backup and restore.</p>"},{"location":"30-reference/process/configure-infra/","title":"Configure infrastructure","text":"<p>This stage focuses on the configuration of the provisioned infrastructure.</p>"},{"location":"30-reference/process/configure-infra/#configure-infrastructure-for-ibm-cloud","title":"Configure infrastructure for IBM Cloud","text":""},{"location":"30-reference/process/configure-infra/#configure-the-vpc-bastion-servers","title":"Configure the VPC bastion server(s)","text":"<p>In a configuration scenario where NFS is used for OpenShift storage, the NFS server must be provisioned as a VSI within the VPC that contains the OpenShift cluster. It is best practice to shield off the NFS server from the outside world by using a jump host (bastion) to access it.</p> <p>This steps configures the bastion host which has a public IP address to serve as a jump host to access other servers and services within the VPC.</p>"},{"location":"30-reference/process/configure-infra/#configure-the-vpc-nfs-servers","title":"Configure the VPC NFS server(s)","text":"<p>Configures the NFS server using the specs in the <code>nfs_server</code> configuration object(s). It installs the required packages and sets up the NFSv4 service. Additionally, it will format the empty volume as <code>xfs</code> and export it so it can be used by the <code>managed-nfs-storage</code> storage class in the OpenShift cluster.</p>"},{"location":"30-reference/process/configure-infra/#configure-the-openshift-storage-classes","title":"Configure the OpenShift storage classes","text":"<p>This steps takes care of configuring the storage classes in the OpenShift cluster. Storage classes are an abstraction of the underlying physical and virtual storage. When run, it processes the <code>openshift_storage</code> elements within the current <code>openshift</code> configuration object.</p> <p>Two types of storage classes can be automatically created and configured:</p>"},{"location":"30-reference/process/configure-infra/#nfs-storage","title":"NFS Storage","text":"<p>Creates the <code>managed-nfs-storage</code> OpenShift storage class using the specified <code>nfs_server_name</code> which references an <code>nfs_server</code> configuration object.</p>"},{"location":"30-reference/process/configure-infra/#odf-storage","title":"ODF Storage","text":"<p>Activates the ROKS cluster's OpenShift Data Foundation add-on to install the operator into the cluster. Once finished with the preparation, the <code>OcsCluster</code> OpenShift object is created to provision the storage cluster. As the backing storage the <code>ibmc-vpc-block-metro-10iops-tier</code> storage class is used, which has the appropriate IO characteristics for the Cloud Paks.</p> <p>Info</p> <p>Both NFS and ODF storage classes can be created but only 1 storage class of each type can exist in the cluster at the moment. If more than one storage class of the same type is specified, the configuration will fail.</p>"},{"location":"30-reference/process/deploy-assets/","title":"Deploy assets","text":""},{"location":"30-reference/process/deploy-assets/#deployer-hooks","title":"Deployer hooks","text":"<p>There are serveral points where you can make the deployer run Ansible tasks, for example to do additional configuration steps like LDAP provisioning and deploying assets. The names of the <code>.yml</code> that can be used as hooks inside the deployer process are fixed and must be placed inside the <code>$CONFIG_DIR/assets</code> directory.</p> <p></p> <p>Hooks:</p> <ul> <li>Pre-validation (phase 10): <code>deployer-hook-pre-10-validation.yml</code></li> <li>Pre-prepare (phase 20): <code>deployer-hook-pre-20-prepare.yml</code></li> <li>Pre-provision-infra (phase 30): <code>deployer-hook-pre-30-provision-infra.yml</code></li> <li>Pre-configure-infra (phase 40): <code>deployer-hook-pre-40-configure-infra.yml</code></li> <li>Pre-install-cloud-pak (phase 50): <code>deployer-hook-pre-50-install-cloud-pak.yml</code></li> <li>Pre-configure-cloud-pak (phase 60): <code>deployer-hook-pre-60-configure-cloud-pak.yml</code></li> <li>Pre-deploy-assets (phase 70): <code>deployer-hook-pre-70-deploy-assets.yml</code></li> <li>Pre-deploy-assets (phase 80): <code>deployer-hook-pre-80-smoke-tests.yml</code></li> </ul> <p>The hooks are only called once per phase. It is the responsibility of the user to do any validation and error handling inside the Ansible tasks.</p> <p>Sample hooks can be found in the <code>sample-configurations/sample-dynamic/config-samples/assets</code> directory in the deployer GitHub repository.</p>"},{"location":"30-reference/process/deploy-assets/#cloud-pak-for-data","title":"Cloud Pak for Data","text":"<p>For Cloud Pak for Data, this stage does the following:</p> <ul> <li>Deploy Cloud Pak for Data assets which are defined with object <code>cp4d_asset</code></li> <li>Deploy the Cloud Pak for Data monitors identified with <code>cp4d_monitors</code> elements.</li> </ul>"},{"location":"30-reference/process/deploy-assets/#deploy-cloud-pak-for-data-assets","title":"Deploy Cloud Pak for Data assets","text":"<p>See cp4d_asset for more details.</p>"},{"location":"30-reference/process/deploy-assets/#cloud-pak-for-data-monitors","title":"Cloud Pak for Data monitors","text":"<p>See cp4d_monitors for more details.</p>"},{"location":"30-reference/process/install-cloud-pak/","title":"Install the Cloud Pak(s)","text":"<p>This stage focuses on preparing the OpenShift cluster for installing the Cloud Pak(s) and then proceeds with the installation of Cloud Paks and the cartridges. The below documentation will start with a list of steps that will be executed for all Cloud Paks, then proceed with Cloud Pak specific activities. The execution of the steps may slightly differ from the sequence in the documentation.</p> <p>Sections:</p> <ul> <li>Remove obsolete Cloud Pak for Data instances</li> <li>Prepare private image registry</li> <li>Install Cloud Pak for Data and cartridges</li> </ul>"},{"location":"30-reference/process/install-cloud-pak/#remove-cloud-pak-for-data","title":"Remove Cloud Pak for Data","text":"<p>Before going ahead with the mirroring of container images and installation of Cloud Pak for Data, the previous configuration (if any) is retrieved from the vault to determine if a Cloud Pak for Data instance has been removed. If a previously installed <code>cp4d</code> object no longer exists in the current configuration, its associated instance is removed from the OpenShift cluster.</p> <p>First, the custom resources are removed from the OpenShift project. This happens with a grace period of 5 minutes. After the grace period has expired, OpenShift automatically forcefully deletes the custom resource and its associated definitions. Then, the control plane custom resource <code>Ibmcpd</code> is removed and finally the namespace (project). For the namespace deletion, a grace period of 10 minutes is applied.</p>"},{"location":"30-reference/process/install-cloud-pak/#prepare-private-image-registry","title":"Prepare private image registry","text":"<p>When installing the Cloud Paks, images must be pulled from an image registry. All Cloud Paks support pulling images directly from the IBM Entitled Registry using the entitlement key, but there may be situations this is not possible, for example in air-gapped environents, or when images must be scanned for vulnerabilities before they are allowed to be used. In those cases, a private registry will have to be set up.</p> <p>The Cloud Pak Deployer can mirror images to a private registry from the entitled registry. On IBM Cloud, the deployer is also capable of creating a namespace in the IBM Container Registry and mirror the images to that namespace.</p> <p>When a private registry has been specified in the Cloud Pak entry (using the <code>image_registry_name</code> property), the necessary OpenShift configuration changes will also be made.</p>"},{"location":"30-reference/process/install-cloud-pak/#create-ibm-container-registry-namespace-ibm-cloud-only","title":"Create IBM Container Registry namespace (IBM Cloud only)","text":"<p>If OpenShift is deployed on IBM Cloud (ROKS), the IBM Container Registry should be used as the private registry from which the images will be pulled. Images in the ICR are organized by namespace and can be accessed using an API key issued for a service account. If an <code>image_registry</code> object is specified in the configuration, this process will take care of creating the service account, then the API key and it will store the API key in the vault.</p>"},{"location":"30-reference/process/install-cloud-pak/#connect-to-the-specified-private-image-registry","title":"Connect to the specified private image registry","text":"<p>If an image registry has been specified for the Cloud Pak using the <code>image_registry_name</code> property, the referenced <code>image_registry</code> entry is looked up in the configuration and the credentials are retrieved from the vault. Then the connection to the registry is tested by logging on.</p>"},{"location":"30-reference/process/install-cloud-pak/#install-cloud-pak-for-data-and-cartridges","title":"Install Cloud Pak for Data and cartridges","text":""},{"location":"30-reference/process/install-cloud-pak/#prepare-openshift-cluster-for-cloud-pak-installation","title":"Prepare OpenShift cluster for Cloud Pak installation","text":"<p>Cloud Pak for Data requires a number of cluster-wide settings:</p> <ul> <li>Create an <code>ImageContentSourcePolicy</code> if images must be pulled from a private registry</li> <li>Set the global pull secret with the credentials to pull images from the entitled or private image registry</li> <li>Create a <code>Tuned</code> object to set kernel semaphores and other properties of CoreOS containers being spun up</li> <li>Allow unsafe system controls in the Kubelet configuration</li> <li>Set PIDs limit and default ulimit for the CRI-O configuration</li> </ul> <p>For all OpenShift clusters, except ROKS on IBM Cloud, these settings are applied using OpenShift configuration objects and then picked up by the Machine Config Operator. This operator will then apply the settings to the control plane and compute nodes as appropriate and reload them one by one.</p> <p>To avoid having to reload the nodes more than once, the Machine Config Operator is paused before the settings are applied. After all setup, the Machine Config Operator is released and the deployment process will then wait until all nodes are ready with the configuration applied.</p>"},{"location":"30-reference/process/install-cloud-pak/#prepare-openshift-cluster-on-ibm-cloud-and-ibm-cloud-satellite","title":"Prepare OpenShift cluster on IBM Cloud and IBM Cloud Satellite","text":"<p>As mentioned before, ROKS on IBM Cloud does not include the Machine Config Operator and would normally require the compute nodes to be reloaded (classic ROKS) or replaced (ROKS on VPC) to make the changes effective. While implementing this process, we have experienced intermittent reliability issues where replacement of nodes never finished or the cluster ended up in a unusable state. To avoid this, the process is applying the settings in a different manner.</p> <p>On every node, a cron job is created which starts every 5 minutes. It runs a script that checks if any of the cluster-wide settings must be (re-)applied, then updates the local system and restarts the <code>crio</code> and <code>kubelet</code> daemons. If no settings are to be adjusted, the daemons will not be restarted and therefore the cron job has minimal or no effect on the running applications.</p> <p>Compute node changes that are made by the cron job: ImageContentSourcePolicy: File <code>/etc/containers/registries.conf</code> is updated to include registry mirrors for the private registry. Kubelet: File <code>/etc/kubernetes/kubelet.conf</code> is appended with the <code>allowedUnsafeSysctls</code> entries. CRI-O: <code>pids_limit</code> and <code>default_ulimit</code> changes are made to the <code>/etc/crio/crio.conf</code> file. Pull secret: The registry and credentials are appended to the <code>/.docker/config.json</code> configuration.</p> <p>There are scenarios, especially on IBM Cloud Satellite, where custom changes must be applied to the compute nodes. This is possible by adding the <code>apply-custom-node-settings.sh</code> to the <code>assets</code> directory within the <code>CONFIG_DIR</code> directory. Once Kubelet, CRI-O and other changes have been applied, this script (if existing) is run to apply any additional configuration changes to the compute node.</p> <p>By setting the <code>NODE_UPDATED</code> script variable to <code>1</code> you can tell the deployer to restart the <code>crio</code> and <code>kubelet</code> daemons.</p> <p>WARNING: You should never set the <code>NODE_UPDATED</code> script variable to <code>0</code> as this will cause previous changes to the pull secret, ImageContentSourcePolicy and others not to become effective.</p> <p>WARNING: Do not end the script with the <code>exit</code> command; this will stop the calling script from running and therefore not restart the daemons.</p> <p>Sample script: <pre><code>#!/bin/bash\n\n#\n# This is a sample script that will cause the crio and kubelet daemons to be restarted once by checking\n# file /tmp/apply-custom-node-settings-run. If the file doesn't exist, it creates it and sets NODE_UPDATED to 1.\n# The deployer will observe that the node has been updated and restart the daemons.\n#\n\nif [ ! -e /tmp/apply-custom-node-settings-run ];then\n    touch /tmp/apply-custom-node-settings-run\n    NODE_UPDATED=1\nfi\n</code></pre></p>"},{"location":"30-reference/process/install-cloud-pak/#mirror-images-to-the-private-registry","title":"Mirror images to the private registry","text":"<p>If a private image registry is specified, and if the IBM Cloud Pak entitlement key is available in the vault (<code>cp_entitlement_key</code> secret), the Cloud Pak case files for the Foundational Services, the Cloud Pak control plane and cartridges are downloaded to a subdirectory of the status directory that was specified. Then all images defined for the cartridges are mirrored from the entitled registry to the private image registry. Dependent on network speed and how many cartridges have been configured, the mirroring can take a very long time (12+ hours). All images which have already been mirrored to the private registry are skipped by the mirroring process.</p> <p>Even if all images have been mirrored, the act of checking existence and digest can still take a bit of time (10-15 minutes). To avoid this, you can remove the <code>cp_entitlement_key</code> secret from the vault and unset the <code>CP_ENTITLEMENT_KEY</code> environment variable before running the Cloud Pak Deployer.</p>"},{"location":"30-reference/process/install-cloud-pak/#create-catalog-sources","title":"Create catalog sources","text":"<p>The images of the operators which control the Cloud Pak are defined in OpenShift <code>CatalogSource</code> objects which reside in the <code>openshift-marketplace</code> project. Operator subscriptions subsequently reference the catalog source and define the update channel. When images are pulled from the entitled registry, most subscriptions reference the same <code>ibm-operator-catalog</code> catalog source (and also a Db2U catalog source). If images are pulled from a private registry, the control plane and also each cartridge reference their own catalog source in the <code>openshift-marketplace</code> project.</p> <p>This step creates the necessary catalog sources, dependent on whether the entitled registry or a private registry is used. For the entitled registry, it creates the catalog source directly using a YAML template; when using a private registry, the <code>cloudctl case</code> command is used for the control plane and every cartridge to install the catalog sources and their dependencies.</p>"},{"location":"30-reference/process/install-cloud-pak/#get-openshift-storage-classes","title":"Get OpenShift storage classes","text":"<p>Most custom resources defined by the cartridge operators require some back-end storage. To be able to reference the correct OpenShift storage classes, they are retrieved based on the <code>openshift_storage_name</code> property of the Cloud Pak object.</p>"},{"location":"30-reference/process/install-cloud-pak/#prepare-the-cloud-pak-for-data-operator","title":"Prepare the Cloud Pak for Data operator","text":"<p>When using express install, the Cloud Pak for Data operator also installs the Cloud Pak Foundational Services. Consecutively, this part of the deployer:</p> <ul> <li>Creates the operator project if it doesn't exist already</li> <li>Creates an OperatorGroup</li> <li>Installs the license service and certificate manager</li> <li>Creates the platform operator subscription</li> <li>Waits until the ClusterServerVersion objects for the platform operator and Operand Deployment Lifecycle Manager have been created</li> </ul>"},{"location":"30-reference/process/install-cloud-pak/#install-the-cloud-pak-for-data-control-plane","title":"Install the Cloud Pak for Data control plane","text":"<p>When the Cloud Pak for Data operator has been installed, the process continues by creating an OperandRequest object for the platform operator which manages the project in the which Cloud Pak for Data instance is installed. Then it creates an Ibmcpd custom resource in the project which installs the controle plane with nginx the metastore, etc.</p> <p>The Cloud Pak for Data control plane is a pre-requisite for all cartridges so at this stage, the deployer waits until the Ibmcpd status reached the <code>Completed</code> state.</p> <p>Once the control plane has been installed successfully, the deployer generates a new strong 25-character password for the Cloud Pak for Data <code>admin</code> user and stores this into the vault. Additionally, the <code>admin-user-details</code> secret in the OpenShift project is updated with the new password.</p>"},{"location":"30-reference/process/install-cloud-pak/#install-the-specified-cloud-pak-for-data-cartridges","title":"Install the specified Cloud Pak for Data cartridges","text":"<p>Now that the control plane has been installed in the specified OpenShift project, cartridges can be installed. Every cartridge is controlled by its own operator subscription in the operators project and a custom resource. The deployer iterates twice over the specified cartridges, first to create the operator subscriptions, then to create the custom resources.</p>"},{"location":"30-reference/process/install-cloud-pak/#create-cartridge-operator-subscriptions","title":"Create cartridge operator subscriptions","text":"<p>This steps creates <code>subscription</code> objects for each cartridge in the operators project, using a YAML template that is included in the deployer code and the <code>subscription_channel</code> specified in the cartridge definition. Keeping the subscription channel separate delivers flexibility when new subscription channels become available over time.</p> <p>Once the subscription has been created, the deployer waits for the associate CSV(s) to be created and reach the <code>Installed</code> state.</p>"},{"location":"30-reference/process/install-cloud-pak/#delete-obsolete-cartridges","title":"Delete obsolete cartridges","text":"<p>If this is not the first installation, earlier configured cartridges may have been removed. This steps iterates over all supported cartridges and checks if the cartridge has been installed and wheter it exists in the configuration of the current <code>cp4d</code> object. If the cartridge is no longer defined, its custom resource is removed; the operator will then take care of removing all OpenShift configuration.</p>"},{"location":"30-reference/process/install-cloud-pak/#install-the-cartridges","title":"Install the cartridges","text":"<p>This steps creates the Custom Resources for each cartridge. This is the actual installation of the cartridge. Cartridges can be installed in parallel to a certain extent and the operator will wait for the dependencies to be installed first before starting the processes. For example, if Watson Studio and Watson Machine Learning are installed, both have a dependency on the Common Core Services (CCS) and will wait for the CCS object to reach the Completed state before proceeding with the install. Once that is the case, both WS and WML will run the installation process in parallel.</p>"},{"location":"30-reference/process/install-cloud-pak/#wait-until-all-cartridges-are-ready","title":"Wait until all cartridges are ready","text":"<p>Installation of the cartridges can take a very long time; up to 5 hours for Watson Knowledge Catalog. While cartridges are being installed, the deployer checks the states of all cartridges on a regular basis and reports these in a log file. The deployer will retry until all specified cartridges have reached the <code>Completed</code> state.</p>"},{"location":"30-reference/process/install-cloud-pak/#configure-ldap-authentication-for-cloud-pak-for-data","title":"Configure LDAP authentication for Cloud Pak for Data","text":"<p>If LDAP has been configured for the Cloud Pak for Data element, it will be configured after all cartridges have finished installing.</p>"},{"location":"30-reference/process/overview/","title":"Deployment process overview","text":"<p>When running the Cloud Pak Deployer (<code>cp-deploy env apply</code>), a series of pre-defined stages are followed to arrive at the desired end-state.</p>"},{"location":"30-reference/process/overview/#10---validation","title":"10 - Validation","text":"<p>In this stage, the following activities are executed:</p> <ul> <li>Is the specified cloud platform in the inventory file supported?</li> <li>Are the mandatory variables defined?</li> <li>Can the deployer connect to the specified vault?</li> </ul>"},{"location":"30-reference/process/overview/#20---prepare","title":"20 - Prepare","text":"<p>In this stage, the following activities are executed:</p> <ul> <li>Read the configuration files from the <code>config</code> directory</li> <li>Replace variable placeholders in the configuration with the extra parameters passed to the <code>cp-deploy</code> command</li> <li>Expand the configuration with defaults from the <code>defaults</code> directory</li> <li>Run the \"linter\" to check the object attributes in the configuration and their relations</li> <li>Generate the Terraform scripts to provision the infrastructure (IBM Cloud only)</li> <li>Download all CLIs needed for the selected cloud platform and cloud pak(s), if not air-gapped</li> </ul>"},{"location":"30-reference/process/overview/#30---provision-infra","title":"30 - Provision infra","text":"<p>In this stage, the following activities are executed:</p> <ul> <li>Run Terraform to create or change the infrastructure components for IBM cloud</li> <li>Run the OpenShift installer-provisioned infrastructure (IPI) installer for AWS (ROSA), Azure (ARO) or vSphere</li> </ul>"},{"location":"30-reference/process/overview/#40---configure-infra","title":"40 - Configure infra","text":"<p>In this stage, the following activities are executed:</p> <ul> <li>Configure the VPC bastion and NFS server(s) for IBM Cloud</li> <li>Configure the OpenShift storage classes or test validate the existing storege classes if an existing OpenShift cluster is used</li> <li>Configure OpenShift logging</li> </ul>"},{"location":"30-reference/process/overview/#50---install-cloud-pak","title":"50 - Install Cloud Pak","text":"<p>In this stage, the following activities are executed:</p> <ul> <li>Create the IBM Container Registry namespace for IBM Cloud</li> <li>Connect to the specified image registry and create ImageContentSourcePolicy</li> <li>Prepare OpenShift cluster for Cloud Pak for Data installation</li> <li>Mirror images to the private registry</li> <li>Install Cloud Pak for Data control plane</li> <li>Configure Foundational Services license service</li> <li>Install specified Cloud Pak for Data cartridges</li> </ul>"},{"location":"30-reference/process/overview/#60---configure-cloud-pak","title":"60 - Configure Cloud Pak","text":"<p>In this stage, the following activities are executed:</p> <ul> <li>Add OpenShift signed certificate to Cloud Pak for Data web server when on IBM Cloud</li> <li>Configure LDAP for Cloud Pak for Data</li> <li>Configure SAML authentication for Cloud Pak for Data</li> <li>Configure auditing for Cloud Pak for Data</li> <li>Configure instance for the cartridges (Analytics engine, Db2, Cognos Analytics, Data Virtualization, \u2026)</li> <li>Configure instance authorization using the LDAP group mapping</li> </ul>"},{"location":"30-reference/process/overview/#70---deploy-assets","title":"70 - Deploy Assets","text":"<ul> <li>Configure Cloud Pak for Data monitors</li> <li>Install Cloud Pak for Data assets</li> </ul>"},{"location":"30-reference/process/overview/#80---smoke-tests","title":"80 - Smoke Tests","text":"<p>In this stage, the following activities are executed:</p> <ul> <li>Show the Cloud Pak for Data URL and admin password</li> </ul>"},{"location":"30-reference/process/prepare/","title":"Prepare the deployer","text":"<p>This stage mainly takes care of checking the configuration and expanding it where necessary so it can be used by subsequent stages. Additionally, the preparation also calls the roles that will generate Terraform or other configuration files which are needed for provisioning and configuration.</p>"},{"location":"30-reference/process/prepare/#generator","title":"Generator","text":"<p>All <code>yaml</code> files in the <code>config</code> directory of the specified <code>CONFIG_DIR</code> are processed and a composite JSON object, <code>all_config</code> is created, which contains all configuration.</p> <p>While processing the objects defined in the <code>config</code> directory files, the <code>defaults</code> directory is also processed to determine if any supplemental \"default\" variables must be added to the configuration objets. This makes it easy for example to ensure VSIs always use the correct Red Hat Enterprise Linux image available on IBM Cloud.</p> <p>You will find the generator roles under the <code>automation-generators</code> directory. There are cloud-provider dependent roles such as <code>openshift</code> which have a structure dependent on the chosen cloud provider and there are generic roles such as <code>cp4d</code> which are not dependent on the cloud provider.</p> <p>To find the appropriate role for the object, the generator first checks if the role is found under the specified cloud provider directory. If not found, it will call the role under <code>generic</code>.</p>"},{"location":"30-reference/process/prepare/#linting","title":"Linting","text":"<p>Each of the objects have a syntax checking module called <code>preprocessor.py</code>. This Python program checks the attributes of the object in question and can also add defaults for properties which are missing. All errors found are collected and displayed at the end of the generator.</p>"},{"location":"30-reference/process/provision-infra/","title":"Provision infrastructure","text":"<p>This stage will provision the infrastructure that was defined in the input configuration files. Currently, this has only been implemented for IBM Cloud.</p>"},{"location":"30-reference/process/provision-infra/#ibm-cloud","title":"IBM Cloud","text":"<p>The IBM Cloud infrastructure provisioning runs Terraform to initially provision the infrastructure components such as VPC, VSIs, security groups, ROKS cluster and others. Also, if changes have been made in the configuration, Terraform will attempt to make the changes to reach the desired end-state.</p> <p>Based on the chosen action (apply or destroy), Terraform is instructed to provision or change the infrastructure components or to destroy everything.</p> <p>The Terraform state file (tfstate) is maintained in the vault and is critical to enable dynamic updates to the infrastructure. If the state file is lost or corrupted, updates to the infrastructure will have to be done manually. The Ansible tasks have been built in a way that the Terraform state file is always persisted into the vault, even if the apply or destroy process has failed.</p> <p>There are 3 main steps:</p>"},{"location":"30-reference/process/provision-infra/#terraform-init","title":"Terraform init","text":"<p>This step initializes the Terraform provider (ibm) with the correct version. If needed, the Terraform modules for the provider are downloaded or updated.</p>"},{"location":"30-reference/process/provision-infra/#terraform-plan","title":"Terraform plan","text":"<p>Applying changes to the infrastructure using Terraform based on the input configuration files may cause critical components to be replaced (destroyed and recreated). The plan step checks what will be changed. If infrastructure components are destroyed and the <code>--confirm-destroy</code> parameter has not be specified for the deployer, the process is aborted.</p>"},{"location":"30-reference/process/provision-infra/#terraform-apply-or-terraform-destroy","title":"Terraform apply or Terraform destroy","text":"<p>This is the execution of the plan and will provision new infrastructure (apply) or destroy everything (destroy).</p> <p>While the Terraform apply or destroy process is running, a <code>.tfstate</code> file is updated on disk. When the command completes, the deployer writes this as a secret to the vault so it can be used next time to update (or destroy) the infrastructure components.</p>"},{"location":"30-reference/process/smoke-tests/","title":"Smoke tests","text":"<p>This is the final stage before returning control to the process that started the deployer. Here tests to check that the Cloud Pak and its cartridges has been deployed correctly and that everything is running as expected.</p> <p>The method for smoke tests should be dynamic, for example by referencing a Git repository and context (directory within the repository); the code within that directory then deploys the asset(s).</p>"},{"location":"30-reference/process/smoke-tests/#cloud-pak-for-data-smoke-tests","title":"Cloud Pak for Data smoke tests","text":""},{"location":"30-reference/process/smoke-tests/#show-the-cloud-pak-for-data-url-and-admin-password","title":"Show the Cloud Pak for Data URL and admin password","text":"<p>This \"smoke test\" finds the route of the Cloud Pak for Data instance(s) and retrieves the <code>admin</code> password from the vault which is then displayed.</p> <p>Example: <pre><code>['CP4D URL: https://cpd-cpd.fke09-10-a939e0e6a37f1ce85dbfddbb7ab97418-0000.eu-gb.containers.appdomain.cloud', 'CP4D admin password: ITnotgXcMTcGliiPvVLwApmsV']\n</code></pre></p> <p>With this information you can go to the Cloud Pak for Data URL and login using the <code>admin</code> user.</p>"},{"location":"30-reference/process/validate/","title":"10 - Validation - Validate the configuration","text":"<p>In this stage, the following activities are executed:</p> <ul> <li>Is the specified cloud platform in the inventory file supported?</li> <li>Are the mandatory variables defined?</li> <li>Can the deployer connect to the specified vault?</li> </ul>"},{"location":"30-reference/process/cp4d-cartridges/cognos-authorization/","title":"Automated Cognos Authorization using LDAP groups","text":""},{"location":"30-reference/process/cp4d-cartridges/cognos-authorization/#description","title":"Description","text":"<p>The automated cognos authorization capability uses LDAP groups to assign users to a Cognos Analytics Role, which allows these users to login to IBM Cloud Pak for Data and access the Cognos Analytics instance. This capability will perform the following tasks: - Create a User Group and assign the associated LDAP Group(s) and Cloud Pak for Data role(s) - For each member of the LDAP Group(s) part of the User Group, create the user as a Cloud Pak for Data User and assigned the Cloud Pak for Data role(s) - For each member of the LDAP Group(s) part of the User Group, assign membership to the Cognos Analytics instance and authorize for the Cognos Analytics Role</p> <p>If the User Group is already present, validate all LDAP Group(s) are associated with the User Group. Add the LDAP Group(s) not yet assiciated to the User Group. Existing LDAP groups will not be removed from the User Group</p> <p>If a User is already present in Cloud Pak for Data, it will not be updated.</p> <p>If a user is already associated with the Cognos Analytics instance, keep its original membership and do not update the membership</p>"},{"location":"30-reference/process/cp4d-cartridges/cognos-authorization/#pre-requisites","title":"Pre-requisites","text":"<p>Prior to running the script, ensure: - LDAP configuration in IBM Cloud Pak for Data is completed and validated - Cognos Analytics instance is provisioned and running in IBM Cloud Pak for Data - The role(s) that will be associated with the User Group are present in IBM Cloud Pak for Data</p>"},{"location":"30-reference/process/cp4d-cartridges/cognos-authorization/#usage-of-the-script","title":"Usage of the Script","text":"<p>The script is available in automation-roles/50-install-cloud-pak/cp4d-service/files/assign_CA_authorization.sh.</p> <p>Run the script without arguments to show its usage help. <pre><code># ./assign_CA_authorization.sh                                                                               \nUsage:\n\nassign_CA_authorization.sh\n  &lt;CLOUD_PAK_FOR_DATA_URL&gt;\n  &lt;CLOUD_PAK_FOR_DATA_LOGIN_USER&gt;\n  &lt;CLOUD_PAK_FOR_DATA_LOGIN_PASSWORD&gt;\n  &lt;CLOUD_PAK_FOR_DATA_USER_GROUP_NAME&gt;\n  &lt;CLOUD_PAK_FOR_DATA_USER_GROUP_DESCRIPTION&gt;\n  &lt;CLOUD_PAK_FOR_DATA_USER_GROUP_ROLES_ASSIGNMENT&gt;\n  &lt;CLOUD_PAK_FOR_DATA_USER_GROUP_LDAP_GROUPS_MAPPING&gt;\n  &lt;CLOUD_PAK_FOR_DATA_COGNOS_ANALYTICS_ROLE&gt;\n</code></pre></p> <ul> <li>  The URL to the IBM Cloud Pak for Data instance <li> The login user to IBM Cloud Pak for Data, e.g. the admin user <li> The login password to IBM Cloud Pak for Data <li> The Cloud Pak for Data User Group Name <li> The Cloud Pak for Data User Group Description <li> The Cloud Pak for Data roles associated to the User Group. Use a ; seperated list to assign multiple roles <li> The LDAP Groups associated to the User Group. Use a ; seperated list to assign LDAP groups <li> The Cognos Analytics Role each member of the User Group will be associated with, which must be one of: <li>Analytics Administrators</li> <li>Analytics Explorers</li> <li>Analytics Users</li> <li>Analytics Viewer</li>"},{"location":"30-reference/process/cp4d-cartridges/cognos-authorization/#running-the-script","title":"Running the script","text":"<p>Using the command example provided by the <code>./assign_CA_authorization.sh</code> command, run the script with its arguments <pre><code># ./assign_CA_authorization.sh \\\n  https://...... \\\n  admin \\\n  ******** \\\n  \"Cognos User Group\" \\\n  \"Cognos User Group Description\" \\\n  \"wkc_data_scientist_role;zen_administrator_role\" \\\n  \"cn=ca_group,ou=groups,dc=ibm,dc=com\" \\\n  \"Analytics Viewer\"\n</code></pre> The script execution will run through the following tasks:</p> <p>Validation Confirm all required arguments are provided. Confirm at least 1 User Group Role assignment is provided. Confirm at least 1 LDAP Group is provided.</p> <p>Login to Cloud Pak for Data and generate a Bearer token Using the provided IBM Cloud for Data URL, username and password, login to Cloud pak for Data and generate the Bearer token used for subsequent commands. Exit with an error if the login to IBM Cloud Pak for Data fails. </p> <p>Confirm the provided User Group role(s) are present in Cloud Pak for Data Acquire all Cloud Pak for Data roles and confirm the provided User Group role(s) are one of the existing Cloud Pak for Data roles. Exit with an error if a role is provided which is not currently present in IBM Cloud Pak for Data.</p> <p>Confirm the provided Cognos Analytics role is valid Ensure the provided Cognos Analytics role is one of the available Cognos Analytics roles. Exit with an error if a Cognos Analytics role is provided that does not match with the available Cognos Analytics roles.</p> <p>Confirm LDAP is configured in IBM Cloud Pak for Data Ensures the LDAP configuration is completed. Exit with an error if there is no current LDAP configuration.</p> <p>Confirm the provided LDAP groups are present in the LDAP User Registry Using IBM Cloud Pak for Data, query whether the provided LDAP groups are present in the LDAP User registry. Exit with an error if a LDAP Group is not available.</p> <p>Confirm if the IBM Cloud Pak for Data User Group exists Queries the IBM Cloud Pak for Data User Groups. If the provided User Group exists, acquire the Group ID. </p> <p>If the IBM Cloud Pak for Data User Group does not exist, create it If the User Group does not exist, create it, and assign the IBM Cloud Pak for Data Roles and LDAP Groups to the new User Group</p> <p>If the IBM Cloud Pak for Data User Group does exist, validate the associated LDAP Groups If the User Group already exists, confirm all provided LDAP groups are associated with the User Group. Add LDAP groups that are not yet associated.</p> <p>Get the Cognos Analytics instance ID Queries the IBM Cloud Pak for Data service instances and acquires the Cognos Analytics instance ID. Exit with an error if no Cognos Analytics instance is available</p> <p>Ensure each user member of the IBM Cloud Pak for Data User Group is an existing user Each user that is member of the provided LDAP groups, ensure this member is an IBM Cloud Pak for Data User. Create a new user with the provided User Group role(s) if the the user is not yet available. Any existing User(s) will not be updated. If Users are removed from an LDAP Group, these users will not be removed from Cloud Pak for Data. </p> <p>Ensure each user member of the IBM Cloud Pak for Data User Group is associated to the Cognos Analytics instance Each user that is member of the provided LDAP groups, ensure this member is associated to the Cognos Analytics instance with the provided Cognos Analytics role. Any user that is already associated to the Cognos Analytics instance will have its Cognos Analytics role updated to the provided Cognos Analytics Role</p>"},{"location":"40-troubleshooting/cp4d-uninstall/","title":"Uninstall Cloud Pak for Data and Foundational Services","text":"<p>For convenience, the Cloud Pak Deployer includes a script that removes the Cloud Pak for Data instance from the OpenShift cluster, then Cloud Pak Foundational Services and finally the catalog sources and CRDs.</p> <p>Steps:</p> <ul> <li>Make sure you are connected to the OpenShift cluster</li> <li>Run script <code>./scripts/cp4d/cp4d-delete-instance.sh &lt;CP4D_project&gt;</code></li> </ul> <p>You will have to confirm that you want to delete the instance and all other artifacts.</p> <p>Warning</p> <p>Please be very careful with this command. Ensure you are connected to the correct OpenShift cluster and that no other Cloud Paks use operator namespace. The action cannot be undone.</p>"},{"location":"40-troubleshooting/ibm-cloud-access-nfs-server/","title":"Access NFS server provisioned on IBM Cloud","text":"<p>When choosing the \"simple\" sample configuration for ROKS VPC on IBM Cloud, the deployer also provisions a Virtual Server Instance and installs a standard NFS server on it. In some cases you may want to get access to the NFS server for troubleshooting.</p> <p>For security reasons, the NFS server can only be reached via a bastion server that is connected to the internet, i.e. use the bastion server as a jump host, this to avoid exposing NFS volumes to the outside world and provide an extra layer of protection. Additionally, password login is disabled on both the bastion and NFS servers and one must use the private SSH key to connect.</p>"},{"location":"40-troubleshooting/ibm-cloud-access-nfs-server/#start-the-command-line-within-the-container","title":"Start the command line within the container","text":"<p>Getting SSH access to the NFS server is easiest from within the deployer container as it has all tools installed to extract the IP addresses from the Terraform state file.</p> <p>Optional: Ensure that the environment variables for the configuration and status directories are set. If not specified, the directories are assumed to be <code>$HOME/cpd-config</code> and <code>$HOME/cpd-status</code>. <pre><code>export STATUS_DIR=$HOME/cpd-status\nexport CONFIG_DIR=$HOME/cpd-config\n</code></pre></p>"},{"location":"40-troubleshooting/ibm-cloud-access-nfs-server/#set-path-and-alias-for-the-deployer","title":"Set path and alias for the deployer","text":"<pre><code>source ./set-env.sh\n</code></pre>"},{"location":"40-troubleshooting/ibm-cloud-access-nfs-server/#start-the-deployer-command-line","title":"Start the deployer command line","text":"<pre><code>cp-deploy.sh env command\n</code></pre> <pre><code>-------------------------------------------------------------------------------\nEntering Cloud Pak Deployer command line in a container.\nUse the \"exit\" command to leave the container and return to the hosting server.\n-------------------------------------------------------------------------------\nInstalling OpenShift client\nCurrent OpenShift context: pluto-01\n</code></pre>"},{"location":"40-troubleshooting/ibm-cloud-access-nfs-server/#obtain-private-ssh-key","title":"Obtain private SSH key","text":"<p>Access to both the bastion and NFS servers are typically protected by the same SSH key, which is stored in the vault. To list all vault secrets, run the command below.</p> <pre><code>cd /cloud-pak-deployer\ncp-deploy.sh vault list\n</code></pre> <pre><code>cp-deploy.sh vault list\n\nStarting Automation script...\n\nPLAY [Secrets] *****************************************************************\nSecret list for group sample:\n- ibm_cp_entitlement_key\n- sample-terraform-tfstate\n- cp4d_admin_zen_40_fke34d\n- sample-all-config\n- pluto-01-provision-ssh-key\n- pluto-01-provision-ssh-pub-key\n\nPLAY RECAP *********************************************************************\nlocalhost                  : ok=11   changed=0    unreachable=0    failed=0    skipped=21   rescued=0    ignored=0\n</code></pre> <p>Then, retrieve the private key (in the above example <code>pluto-01-provision-ssh-key</code>) to an output file in your <code>~/.ssh</code> directory, make sure it has the correct private key format (new line at the end) and permissions (600). <pre><code>SSH_FILE=~/.ssh/pluto-01-rsa\nmkdir -p ~/.ssh\nchmod 600 ~/.ssh\ncp-deploy.sh vault get -vs pluto-01-provision-ssh-key \\\n    -vsf $SSH_FILE\necho -e \"\\n\" &gt;&gt; $SSH_FILE\nchmod 600 $SSH_FILE\n</code></pre></p>"},{"location":"40-troubleshooting/ibm-cloud-access-nfs-server/#find-the-ip-addresses","title":"Find the IP addresses","text":"<p>To connect to the NFS server, you need the public IP address of the bastion server and the private IP address of the NFS server. Obviously these can be retrieved from the IBM Cloud resource list (https://cloud.ibm.com/resources), but they are also kept in the Terraform \"tfstate\" file</p> <pre><code>cp-deploy.sh vault get -vs sample-terraform-tfstate \\\n    -vsf /tmp/sample-terraform-tfstate\n</code></pre> <p>The below commands do not provide the prettiest output but you should be able to extract the IP addresses from them.</p> <p>For the bastion node public (floating) IP address: <pre><code>cat /tmp/sample-terraform-tfstate | jq -r '.resources[]' | grep -A 10 -E \"ibm_is_float\"\n</code></pre></p> <pre><code>  \"type\": \"ibm_is_floating_ip\",\n  \"name\": \"pluto_01_bastion\",\n  \"provider\": \"provider[\\\"registry.terraform.io/ibm-cloud/ibm\\\"]\",\n  \"instances\": [\n    {\n      \"schema_version\": 0,\n      \"attributes\": {\n        \"address\": \"149.81.215.172\",\n...\n        \"name\": \"pluto-01-bastion\",\n</code></pre> <p>For the NFS server: <pre><code>cat /tmp/sample-terraform-tfstate | jq -r '.resources[]' | grep -A 10 -E \"ibm_is_instance|primary_network_interface\"\n</code></pre></p> <pre><code>...\n--\n  \"type\": \"ibm_is_instance\",\n  \"name\": \"pluto_01_nfs\",\n  \"provider\": \"provider[\\\"registry.terraform.io/ibm-cloud/ibm\\\"]\",\n  \"instances\": [\n...\n--\n        \"primary_network_interface\": [\n...\n            \"name\": \"pluto-01-nfs-nic\",\n            \"port_speed\": 0,\n            \"primary_ipv4_address\": \"10.227.0.138\",\n</code></pre> <p>In the above examples, the IP addresses are:</p> <ul> <li>Bastion public IP address: <code>149.81.215.172</code></li> <li>NFS server private IP address: <code>10.227.0.138</code></li> </ul>"},{"location":"40-troubleshooting/ibm-cloud-access-nfs-server/#ssh-to-the-nfs-server","title":"SSH to the NFS server","text":"<p>Finally, to get command line access to the NFS server: <pre><code>BASTION_IP=149.81.215.172\nNFS_IP=10.227.0.138\nssh -i $SSH_FILE \\\n  -o ProxyCommand=\"ssh -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null \\\n  -i $SSH_FILE -W %h:%p -q $BASTION_IP\" \\\n  root@$NFS_IP\n</code></pre></p>"},{"location":"40-troubleshooting/ibm-cloud-access-nfs-server/#stopping-the-session","title":"Stopping the session","text":"<p>Once you've finished exploring the NFS server, you can exit from it: <pre><code>exit\n</code></pre></p> <p>Finally, exit from the deployer container which is then terminated. <pre><code>exit\n</code></pre></p>"},{"location":"50-advanced/advanced-configuration/","title":"Cloud Pak Deployer Advanced Configuration","text":"<p>The Cloud Pak Deployer includes several samples which you can use to build your own configuration. You can find sample configuration <code>yaml</code> files in the sub-directories of the <code>sample-configurations</code> directory of the repository. Descriptions and topologies are also included in the sub-directories.</p> <p>Warning</p> <p>Do not make changes to the sample configurations in the <code>cloud-pak-deployer</code> directory, but rather copy it to your own home directory or somewhere else and then make changes. If you store your own configuration under the repository's clone, you may not be able to update (pull) the repository with changes applied on GitHub, or accidentally overwrite it.</p> <p>Warning</p> <p>The deployer expects to manage all objects referenced in the configuration files, including the referenced OpenShift cluster and Cloud Pak installation. If you have already pre-provisioned the OpenShift cluster, choose a configuration with <code>existing-ocp</code> cloud platform. If the Cloud Pak has already been installed, unexpected and undesired activities may happen. The deployer has not been designed to alter a pre-provisioned OpenShift cluster or existing Cloud Pak installation.</p>"},{"location":"50-advanced/advanced-configuration/#configuration-steps---static-sample-configuration","title":"Configuration steps - static sample configuration","text":"<ol> <li>Copy the static sample configuration directory to your own directory: <pre><code>mkdir -p $HOME/cpd-config/config\ncp -r ./sample-configurations/roks-odf-cp4d/config/* $HOME/cpd-config/config/\ncd $HOME/cpd-config/config\n</code></pre></li> <li>Edit the \"cp4d-....yaml\" file and select the cartridges to be installed by changing the state to <code>installed</code>. Additionally you can accept the Cloud Pak license in the config file by specifying <code>accept_licenses: True</code>. <pre><code>nano ./config/cp4d-450.yaml\n</code></pre></li> </ol> <p>The configuration typically works without any configuration changes and will create all referenced objects, including the Virtual Private Cloud, subnets, SSH keys, ROKS cluster and ODF storage nodes. There is typically no need to change address prefixes and subnets. The IP addresses used by the provisioned components are private to the VPC and are not externally exposed.</p>"},{"location":"50-advanced/advanced-configuration/#configuration-steps---dynamically-choose-openshift-and-cloud-pak","title":"Configuration steps - dynamically choose OpenShift and Cloud Pak","text":"<ol> <li>Copy the sample configuration directory to your own directory: <pre><code>mkdir -p $HOME/cpd-config/config\n</code></pre></li> <li>Copy the relevant OpenShift configuration file from the <code>samples-configuration</code> directory to the <code>config</code> directory, for example: <pre><code>cp ./sample-configurations/sample-dynamic/config-samples/ocp-ibm-cloud-roks-odf.yaml $HOME/cpd-config/config/\n</code></pre></li> <li> <p>Copy the relevant \"cp4d-\u2026\" file from the <code>samples-configuration</code> directory to the <code>config</code> directory, for example: <pre><code>cp ./sample-configurations/sample-dynamic/config-samples/cp4d-462.yaml $HOME/cpd-config/config/\n</code></pre></p> </li> <li> <p>Edit the \"$HOME/cpd-config/config/cp4d-....yaml\" file and select the cartridges to be installed by changing the state to <code>installed</code>. Additionally you can accept the Cloud Pak license in the config file by specifying <code>accept_licenses: True</code>. <pre><code>nano $HOME/cpd-config/config/cp4d-463.yaml\n</code></pre></p> </li> </ol> <p>For more advanced configuration topics such as using a private registry, setting up transit gateways between VPCs, etc, go to the Advanced configuration section</p>"},{"location":"50-advanced/advanced-configuration/#directory-structure","title":"Directory structure","text":"<p>Every configuration has a fixed directory structure, consisting of mandatory and optional subdirectories. </p> <p>Mandatory subdirectories:</p> <ul> <li><code>config</code>: Keeps one or more <code>yaml</code> files with your OpenShift and Cloud Pak configuration</li> </ul> <p>Additionally, there are 3 optional subdirectories:</p> <ul> <li><code>defaults</code>: Directory that keeps the defaults which will be merged with your configuration</li> <li><code>inventory</code>: Keep global settings for the configuration such as environment name or other variables used in the configs</li> <li><code>assets</code>: Keeps directories of assets which must be deployed onto the Cloud Pak</li> </ul>"},{"location":"50-advanced/advanced-configuration/#config-directory","title":"<code>config</code> directory","text":"<p>You can choose to keep only a single file per subdirectory or, for more complex configurations, you can create multiple yaml files. You can find a full list of all supported object types here: Configuration objects. The generator automatically merges all <code>.yaml</code> files in the config and defaults directory. Files with different extensions are ignored. In the sample configurations we split configuration of the OpenShift <code>ocp-...</code> and Cloud Pak <code>cp4.-...</code> objects.</p> <p>For example, your <code>config</code> directory could hold the following files: <pre><code>cp4d-463.yaml\nocp-ibm-cloud-roks-odf.yaml\n</code></pre></p> <p>This will provision a ROKS cluster on IBM Cloud with OpenShift Data Foundation (fka OCS) and Cloud Pak for Data 4.0.8.</p>"},{"location":"50-advanced/advanced-configuration/#defaults-directory-optional","title":"<code>defaults</code> directory (optional)","text":"<p>Holds the defaults for all object types. If a certain object property has not been specified in the <code>config</code> directory, it will be retrieved from the <code>defaults</code> directory using the flavour specified in the configured object. If no flavour has been selected, the <code>default</code> flavour will be chosen.</p> <p>You should not need this subdirectory in most circumstances.</p>"},{"location":"50-advanced/advanced-configuration/#assets-directory-optional","title":"<code>assets</code> directory (optional)","text":"<p>Optional directory holding the assets you wish to deploy for the Cloud Pak. More information about Cloud Pak for Data assets which can be deployed can be found in object definition cp4d_asset. The directory can be named differently as well, for example <code>cp4d-assets</code> or <code>customer-churn-demo</code>.</p>"},{"location":"50-advanced/advanced-configuration/#inventory-directory-optional","title":"<code>inventory</code> directory (optional)","text":"<p>The Cloud Pak Deployer pipeline has been built using Ansible and it can be configured using \"inventory\" files. Inventory files allow you to specify global variables used throughout Ansible playbooks. In the current version of the Cloud Pak Deployer, the inventory directory has become fully optional as the <code>global_config</code> and <code>vault</code> objects have taken over its role. However, if there are certain global variables such as <code>env_id</code> you want to pass via an inventory file, you can also do this.</p>"},{"location":"50-advanced/advanced-configuration/#vault-secrets","title":"Vault secrets","text":"<p>User passwords, certificates and other \"secret\" information is kept in the vault, which can be either a flat file (not encrypted), HashiCorp Vault or the IBM Cloud Secrets Manager service. Some of the deployment configurations require that the vault is pre-populated with secrets which as needed during the deployment. For example, a vSphere deployment needs the vSphere user and password to authenticate to vSphere and Cloud Pak for Data SAML configuration requires the idP certificate</p> <p>All samples default to the File Vault, meaning that the vault will be kept in the <code>vault</code> directory under the status directory you specify when you run the deployer. Detailed descriptions of the vault settings can be found in the sample inventory file and also here: vault settings.</p> <p>Optional: Ensure that the environment variables for the configuration and status directories are set. If not specified, the directories are assumed to be <code>$HOME/cpd-config</code> and <code>$HOME/cpd-status</code>. <pre><code>export STATUS_DIR=$HOME/cpd-status\nexport CONFIG_DIR=$HOME/cpd-config\n</code></pre></p> <p>Set vSphere user secret: <pre><code>cp-deploy.sh vault set \\\n    --vault-secret vsphere-user \\\n    --vault-secret-value super_user@vsphere.local\n</code></pre></p> <p>Or, if you want to create the secret from an input file: <pre><code>cp-deploy.sh vault set \\\n    --vault-secret kubeconfig \\\n    --vault-secret-file ~/.kube/config\n</code></pre></p>"},{"location":"50-advanced/advanced-configuration/#using-a-github-repository-for-the-configuration","title":"Using a GitHub repository for the configuration","text":"<p>If the configuration is kept in a GitHub repository, you can set environment variables to have the deployer pull the GitHub repository to the current server before starting the process.</p> <p>Set environment variables. <pre><code>export CPD_CONFIG_GIT_REPO=\"https://github.com/IBM/cloud-pak-deployer-config.git\"\nexport CPD_CONFIG_GIT_REF=\"main\"\nexport CPD_CONFIG_GIT_CONTEXT=\"\"\n</code></pre></p> <ul> <li><code>CPD_CONFIG_GIT_REPO</code>: The clone URL of the GitHub repository that holds the configuration.</li> <li><code>CPD_CONFIG_GIT_REF</code>: The branch, tag or commit ID to be cloned. If not specified, the repository's default branch will be cloned.</li> <li><code>CPD_CONFIG_GIT_CONTEXT</code>: The directory within the GitHub repository that holds the configuration. This directory must contain the <code>config</code> directory under which the YAML files are kept.</li> </ul> <p>Info</p> <p>When specifying a GitHub repository, the contents will be copied under <code>$STATUS_DIR/cpd-config</code> and this directory is then set as the configuration directory.  </p>"},{"location":"50-advanced/advanced-configuration/#using-dynamic-variables-extra-variables","title":"Using dynamic variables (extra variables)","text":"<p>In some situations you may want to use a single configuration for deployment in different environments, such as development, acceptance test and production. The Cloud Pak Deployer uses the Jinja2 templating engine which is included in Ansible to pre-process the configuration. This allows you to dynamically adjust the configuration based on extra variables you specify at the command line.</p> <p>Example: <pre><code>cp-deploy.sh env apply \\\n  -e ibm_cloud_region=eu_gb \\\n  -e env_id=jupiter-03 [--accept-all-liceneses]\n</code></pre></p> <p>This passes the <code>env_id</code> and <code>ibm_cloud_region</code> variables to the Cloud Pak Deployer, which can then populate variables in the configuration. In the sample configurations, the <code>env_id</code> is used to specify the name of the VPC, ROKS cluster and others and overrides the value specified in the <code>global_config</code> definition. The <code>ibm_cloud_region</code> overrides region specified in the inventory file.</p> <pre><code>...\nvpc:\n- name: \"{{ env_id }}\"\n  allow_inbound: ['ssh']\n\naddress_prefix:\n### Prefixes for the client environment\n- name: \"{{ env_id }}-zone-1\"\n  vpc: \"{{ env_id }}\"\n  zone: {{ ibm_cloud_region }}-1\n  cidr: 10.231.0.0/26\n...\n</code></pre> <p>When running with the above <code>cp-deploy.sh</code> command, the snippet would be generated as: <pre><code>...\nvpc:\n- name: \"jupiter-03\"\n  allow_inbound: ['ssh']\n\naddress_prefix:\n### Prefixes for the client environment\n- name: \"jupiter-03-zone-1\"\n  vpc: \"jupiter-03\"\n  zone: eu-de-1\n  cidr: 10.231.0.0/26\n...\n</code></pre></p> <p>The <code>ibm_cloud_region</code> variable is specified in the inventory file. This is another method of specifying variables for dynamic configuration.</p> <p>You can even include more complex constructs for dynamic configuration, with <code>if</code> statements, <code>for</code> loops and others.</p> <p>An example where the OpenShift ODF storage classes would only be generated for a specific environment (pluto-prod) would be: <pre><code>  openshift_storage:\n  - storage_name: nfs-storage\n    storage_type: nfs\n    nfs_server_name: \"{{ env_id }}-nfs\"\n{% if env_id == 'jupiter-prod' %}\n  - storage_name: odf-storage\n    storage_type: odf\n    odf_storage_label: ocs\n    odf_storage_size_gb: 500\n{% endif %}\n</code></pre></p> <p>For a more comprehensive overview of Jinja2 templating, see https://docs.ansible.com/ansible/latest/user_guide/playbooks_templating.html</p>"},{"location":"50-advanced/alternative-repo-reg/","title":"Using alternative repositories and registries","text":"<p>Warning</p> <p>In most scenarios you will not need this type of configuration. </p> <p>Alternative repositories and registries are mainly geared towards pre-GA use of the Cloud Paks where CASE files are downloaded from internal repositories and staging container image registries need to be used as images have not been released yet.</p>"},{"location":"50-advanced/alternative-repo-reg/#set-path-and-alias-for-the-deployer","title":"Set path and alias for the deployer","text":"<pre><code>source ./set-env.sh\n</code></pre>"},{"location":"50-advanced/alternative-repo-reg/#building-the-cloud-pak-deployer-image","title":"Building the Cloud Pak Deployer image","text":"<p>By default the Cloud Pak Deployer image is built on top of the <code>olm-utils</code> images in <code>icr.io</code>. If you're working with a pre-release of the Cloud Pak OLM utils image, you can override the setting as follows:</p> <pre><code>export CPD_OLM_UTILS_V3_IMAGE=cp.staging.acme.com:5.2.0\n</code></pre> <p>Or, for Cloud Pak for Data 5.3: <pre><code>export CPD_OLM_UTILS_V4_IMAGE=cp.staging.acme.com:5.3.0\n</code></pre></p> <p>Subsequently, run the install commmand: <pre><code>cp-deploy.sh build\n</code></pre></p>"},{"location":"50-advanced/alternative-repo-reg/#configuring-the-alternative-repositories-and-registries","title":"Configuring the alternative repositories and registries","text":"<p>When specifying a <code>cp_alt_repo</code> object in a YAML file, this is used for all Cloud Paks. The object triggers the following steps: * The following files are created in the <code>/tmp/work</code> directory in the container: <code>play_env.sh</code>, <code>resolvers.yaml</code> and <code>resolvers_auth</code>. * When downloading CASE files using the <code>ibm-pak</code> plug-in, the <code>play_env</code> sets the locations of the resolvers and authorization files. * Also, the locations of the case files for the Cloud Pak, Foundational Servides and Open Content are set in an enviroment variable. * Registry mirrors are configured using an <code>ImageContentSourcePolicy</code> resource in the OpenShift cluster. * Registry credentials are added to the OpenShift cluster's global pull secret.</p> <p>The <code>cp_alt_repo</code> is configured like this: <pre><code>cp_alt_repo:\n  repo:\n    token_secret: github-internal-repo\n    helm_path: https://raw.github.ibm.com/IBMSoftwareHub/charts/5.2.0/local\n    cp_path: https://raw.github.ibm.com/PrivateCloud-analytics/cpd-case-repo/5.2.0/promoted/case-repo-promoted\n    fs_path: https://raw.github.ibm.com/IBMPrivateCloud/cloud-pak/master/repo/case\n    opencontent_path: https://raw.github.ibm.com/IBMPrivateCloud/cloud-pak/master/repo/case\n  registry_pull_secrets:\n  - registry: cp.staging.acme.com\n    pull_secret: cp-staging\n  - registry: fs.staging.acme.com\n    pull_secret: cp-fs-staging\n  registry_mirrors:\n  - source: cp.icr.com/cp\n    mirrors:\n    - cp.staging.acme.com/cp\n  - source: cp.icr.io/cp/cpd\n    mirrors:\n    - cp.staging.acme.com/cp/cpd\n  - source: icr.io/cpopen\n    mirrors:\n    - fs.staging.acme.com/cp\n  - source: icr.io/cpopen/cpfs\n    mirrors:\n    - fs.staging.acme.com/cp\n</code></pre></p>"},{"location":"50-advanced/alternative-repo-reg/#property-explanation","title":"Property explanation","text":"Property Description Mandatory Allowed values repo Repositories to be accessed and the Git token Yes repo.token_secret Secret in the vault that holds the Git login token Yes repo.helm_path Repository path where to find Software Hub helm files (required as from Software Hub 5.2.0) No repo.cp_path Repository path where to find Cloud Pak CASE files Yes repo.fs)path Repository path where to find the Foundational Services CASE files Yes repo.opencontent_path Repository path where to find the Open Content CASE files Yes registry_pull_secrets List of registries and their pull secrets, will be used to configure global pull secret Yes .registry Registry host name Yes .pull_secret Vault secret that holds the pull secret (user:password) for the registry Yes registry_mirrors List of registries and their mirrors, will be used to configure the ImageContentSourcePolicy Yes .source Registry and path referenced by the Cloud Pak/FS pod Yes .mirrors: List of alternate registry locations for this source Yes"},{"location":"50-advanced/alternative-repo-reg/#configuring-the-secrets","title":"Configuring the secrets","text":"<p>Before running the deployer with a <code>cp_alt_repo</code> object, you need to ensure the referenced secrets are present in the vault.</p> <p>For the GitHub token, you need to set the token (typically a deploy key) to login to GitHub or GitHub Enterprise. <pre><code>cp-deploy.sh vault set -vs github-internal-repo=abc123def456\n</code></pre></p> <p>For the registry credentials, specify the user and password separated by a colon (<code>:</code>): <pre><code>cp-deploy.sh vault set -vs cp-staging=\"cp-staging-user:cp-staging-password\"\n</code></pre></p> <p>You can also set these tokens on the <code>cp-deploy.sh env apply</code> command line. <pre><code>cp-deploy.sh env apply -f -vs github-internal-repo=abc123def456 -vs cp-staging=\"cp-staging-user:cp-staging-password\n</code></pre></p>"},{"location":"50-advanced/alternative-repo-reg/#running-the-deployer","title":"Running the deployer","text":"<p>To run the deployer you can now use the standard process: <pre><code>cp-deploy.sh env apply -v\n</code></pre></p>"},{"location":"50-advanced/apply-node-settings-non-mco/","title":"Apply OpenShift node settings when machine config operator does not exist","text":"<p>Cloud Pak Deployer automatically applies cluster and node settings before installing the Cloud Pak(s). Sometimes you may also want to automate applying these node settings without installing the Cloud Pak. For convenience, the repository includes a script that makes the same changes normally done through automation: <code>scripts/cp4d/cp4d-apply-non-mco-cluster-settings.sh</code>.</p> <p>To apply the node settings, do the following:</p> <ul> <li>If images are pulled from the entitled registry, set the <code>CP_ENTITLEMENT_KEY</code> environment variable</li> <li>If images are to be pulled from a private registry, set both the <code>CPD_PRIVATE_REGISTRY</code> and <code>CPD_PRIVATE_REGISTRY_CREDS</code> environment variables</li> <li>Log in to the OpenShift cluster with cluster-admin permissions</li> <li>Run the <code>scripts/cp4d/cp4d-apply-non-mco-cluster-settings.sh</code> script.</li> </ul> <p>The <code>CPD_PRIVATE_REGISTRY</code> value must reference the registry host name and optionally the port and namespace that must prefix the images. For example, if the images are kept in https://de.icr.io/cp4d-470, you must specify <code>de.icr.io/cp4d-470</code> for the <code>CPD_PRIVATE_REGISTRY</code> environment variable. If images are kept in https://cust-reg:5000, you must specify <code>cust-reg:5000</code> for the <code>CPD_PRIVATE_REGISTRY</code> environment variable.</p> <p>For the <code>CPD_PRIVATE_REGISTRY_CREDS</code> value, specify both the user and password in a single string, separated by a colon (<code>:</code>). For example: <code>admin:secret_passw0rd</code>.</p> <p>Warning</p> <p>When setting the private registry and its credentials, the script automatically creates the configuration that will set up ImageContentSourcePolicy and global pull secret alternatives. This change cannot be undone using the script. It is not possible to set the private registry and later change to entitled registry. Changing the private registry's credentials can be done by re-running the script with the new credentials.</p>"},{"location":"50-advanced/apply-node-settings-non-mco/#example","title":"Example","text":"<pre><code>export CPD_PRIVATE_REGISTRY=de.icr.io/cp4d-470\nexport CPD_PRIVATE_REGISTRY_CREDS=\"iamapikey:U97KLPYF663AE4XAQL0\"\n./scripts/cp4d/cp4d-apply-non-mco-cluster-settings.sh\n</code></pre> <pre><code>Creating ConfigMaps and secret\nconfigmap \"cloud-pak-node-fix-scripts\" deleted\nconfigmap/cloud-pak-node-fix-scripts created\nconfigmap \"cloud-pak-node-fix-config\" deleted\nconfigmap/cloud-pak-node-fix-config created\nsecret \"cloud-pak-node-fix-secrets\" deleted\nsecret/cloud-pak-node-fix-secrets created\nSetting global pull secret\n/tmp/.dockerconfigjson\ninfo: pull-secret was not changed\nsecret/cloud-pak-node-fix-secrets data updated\nPrivate registry specified, creating ImageContentSourcePolicy for registry de.icr.io/cp4d-470\nGenerating Tuned config\ntuned.tuned.openshift.io/cp4d-ipc unchanged\nWriting fix scripts to config map\nconfigmap/cloud-pak-node-fix-scripts data updated\nconfigmap/cloud-pak-node-fix-scripts data updated\nconfigmap/cloud-pak-node-fix-scripts data updated\nconfigmap/cloud-pak-node-fix-scripts data updated\nCreating service account for DaemonSet\nserviceaccount/cloud-pak-crontab-sa unchanged\nclusterrole.rbac.authorization.k8s.io/system:openshift:scc:privileged added: \"cloud-pak-crontab-sa\"\nRecreate DaemonSet\ndaemonset.apps \"cloud-pak-crontab-ds\" deleted\ndaemonset.apps/cloud-pak-crontab-ds created\nShowing running DaemonSet pods\nNAME                         READY   STATUS              RESTARTS   AGE\ncloud-pak-crontab-ds-b92f9   0/1     Terminating         0          12m\ncloud-pak-crontab-ds-f85lf   0/1     ContainerCreating   0          0s\ncloud-pak-crontab-ds-jlbvm   0/1     ContainerCreating   0          0s\ncloud-pak-crontab-ds-rbj65   1/1     Terminating         0          12m\ncloud-pak-crontab-ds-vckrs   0/1     ContainerCreating   0          0s\ncloud-pak-crontab-ds-x288p   1/1     Terminating         0          12m\nWaiting for 5 seconds for pods to start\n\nShowing running DaemonSet pods\nNAME                         READY   STATUS    RESTARTS   AGE\ncloud-pak-crontab-ds-f85lf   1/1     Running   0          5s\ncloud-pak-crontab-ds-jlbvm   1/1     Running   0          5s\ncloud-pak-crontab-ds-vckrs   1/1     Running   0          5s\n</code></pre>"},{"location":"50-advanced/build-image-and-run-deployer-on-openshift/","title":"Build image and run deployer on OpenShift","text":""},{"location":"50-advanced/build-image-and-run-deployer-on-openshift/#create-configuration","title":"Create configuration","text":"<pre><code>export CONFIG_DIR=$HOME/cpd-config &amp;&amp; mkdir -p $CONFIG_DIR/config\n\ncat &lt;&lt; EOF &gt; $CONFIG_DIR/config/cpd-config.yaml\n---\nglobal_config:\n  environment_name: demo\n  cloud_platform: existing-ocp\n  confirm_destroy: False\n  optimize_deploy: True\n\nopenshift:\n- name: cpd-demo\n  ocp_version: detect\n  cluster_name: cpd-demo\n  domain_name: example.com\n  openshift_storage:\n  - storage_name: nfs-storage\n    storage_type: nfs\n\ncp4d:\n- project: cpd-instance\n  openshift_cluster_name: cpd-demo\n  cp4d_version: 4.8.3\n  accept_licenses: True\n  cartridges:\n  - name: cp-foundation\n    license_service:\n      state: disabled\n      threads_per_core: 2\n  - name: lite\n\n#\n# All tested cartridges. To install, change the \"state\" property to \"installed\". To uninstall, change the state\n# to \"removed\" or comment out the entire cartridge. Make sure that the \"-\" and properties are aligned with the lite\n# cartridge; the \"-\" is at position 3 and the property starts at position 5.\n#\n\n  - name: analyticsengine \n    size: small \n    state: removed\n\n  - name: bigsql\n    state: removed\n\n  - name: ca\n    size: small\n    instances:\n    - name: ca-instance\n      metastore_ref: ca-metastore\n    state: removed\n\n  - name: cde\n    state: removed\n\n  - name: datagate\n    state: removed\n\n  - name: datastage-ent-plus\n    state: removed\n\n    # The default instance is created automatically with the DataStage installation. If you want to create additional instances\n    # uncomment the section below and specify the various scaling options.\n\n    # instances:\n    #   - name: ds-instance\n    #     # Optional settings\n    #     description: \"datastage ds-instance\"\n    #     size: medium\n    #     storage_class: efs-nfs-client\n    #     storage_size_gb: 60\n    #     # Custom Scale options\n    #     scale_px_runtime:\n    #       replicas: 2\n    #       cpu_request: 500m\n    #       cpu_limit: 2\n    #       memory_request: 2Gi\n    #       memory_limit: 4Gi\n    #     scale_px_compute:\n    #       replicas: 2\n    #       cpu_request: 1\n    #       cpu_limit: 3\n    #       memory_request: 4Gi\n    #       memory_limit: 12Gi    \n\n  - name: db2\n    size: small\n    instances:\n    - name: ca-metastore\n      metadata_size_gb: 20\n      data_size_gb: 20\n      backup_size_gb: 20  \n      transactionlog_size_gb: 20\n    state: removed\n\n  - name: db2wh\n    state: removed\n\n  - name: dmc\n    state: removed\n\n  - name: dods\n    size: small\n    state: removed\n\n  - name: dp\n    size: small\n    state: removed\n\n  - name: dv\n    size: small \n    instances:\n    - name: data-virtualization\n    state: removed\n\n  - name: hadoop\n    size: small\n    state: removed\n\n  - name: mdm\n    size: small\n    wkc_enabled: true\n    state: removed\n\n  - name: openpages\n    state: removed\n\n  - name: planning-analytics\n    state: removed\n\n  - name: rstudio\n    size: small\n    state: removed\n\n  - name: spss\n    state: removed\n\n  - name: voice-gateway\n    replicas: 1\n    state: removed\n\n  - name: watson-assistant\n    size: small\n    state: removed\n\n  - name: watson-discovery\n    state: removed\n\n  - name: watson-ks\n    size: small\n    state: removed\n\n  - name: watson-openscale\n    size: small\n    state: removed\n\n  - name: watson-speech\n    stt_size: xsmall\n    tts_size: xsmall\n    state: removed\n\n  - name: wkc\n    size: small\n    state: removed\n\n  - name: wml\n    size: small\n    state: installed\n\n  - name: wml-accelerator\n    replicas: 1\n    size: small\n    state: removed\n\n  - name: wsl\n    state: installed\n\nEOF\n</code></pre>"},{"location":"50-advanced/build-image-and-run-deployer-on-openshift/#log-in-to-the-openshift-cluster","title":"Log in to the OpenShift cluster","text":"<p>Log is as a cluster administrator to be able to run the deployer with the correct permissions.</p>"},{"location":"50-advanced/build-image-and-run-deployer-on-openshift/#prepare-the-deployer-project","title":"Prepare the deployer project","text":"<pre><code>oc new-project cloud-pak-deployer \n\noc project cloud-pak-deployer\noc create serviceaccount cloud-pak-deployer-sa\noc adm policy add-scc-to-user privileged -z cloud-pak-deployer-sa\noc adm policy add-cluster-role-to-user cluster-admin -z cloud-pak-deployer-sa\n</code></pre>"},{"location":"50-advanced/build-image-and-run-deployer-on-openshift/#build-deployer-image-and-push-to-the-internal-registry","title":"Build deployer image and push to the internal registry","text":"<p>Building the deployer image typically takes ~5 minutes. Only do this if the image has not been built yet.</p> <pre><code>cat &lt;&lt; EOF | oc apply -f -\napiVersion: image.openshift.io/v1\nkind: ImageStream\nmetadata:\n  name: cloud-pak-deployer\nspec:\n  lookupPolicy:\n    local: true\nEOF\n\ncat &lt;&lt; EOF | oc create -f -\nkind: Build\napiVersion: build.openshift.io/v1\nmetadata:\n  generateName: cloud-pak-deployer-bc-\n  namespace: cloud-pak-deployer\nspec:\n  serviceAccount: builder\n  source:\n    type: Git\n    git:\n      uri: 'https://github.com/IBM/cloud-pak-deployer'\n      ref: wizard\n  strategy:\n    type: Docker\n    dockerStrategy:\n      buildArgs:\n      - name: CPD_OLM_UTILS_V3_IMAGE\n        value: icr.io/cpopen/cpd/olm-utils-v3:latest\n      - name: CPD_OLM_UTILS_V4_IMAGE\n        value: icr.io/cpopen/cpd/olm-utils-v4:latest\n  output:\n    to:\n      kind: ImageStreamTag\n      name: 'cloud-pak-deployer:latest'\n  triggeredBy:\n    - message: Manually triggered\nEOF\n</code></pre> <p>Now, wait until the deployer image has been built. <pre><code>oc get build -n cloud-pak-deployer -w\n</code></pre></p>"},{"location":"50-advanced/build-image-and-run-deployer-on-openshift/#set-configuration","title":"Set configuration","text":"<pre><code>oc create cm -n cloud-pak-deployer cloud-pak-deployer-config\noc set data -n cloud-pak-deployer cm/cloud-pak-deployer-config \\\n  --from-file=$CONFIG_DIR/config\n</code></pre>"},{"location":"50-advanced/build-image-and-run-deployer-on-openshift/#start-the-deployer-job","title":"Start the deployer job","text":"<pre><code>export CP_ENTITLEMENT_KEY=your_entitlement_key\n\ncat &lt;&lt; EOF | oc apply -f -\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: cloud-pak-deployer-status\n  namespace: cloud-pak-deployer\nspec:\n  accessModes:\n  - ReadWriteMany\n  resources:\n    requests:\n      storage: 10Gi\nEOF\n\ncat &lt;&lt; EOF | oc apply -f -\napiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: cloud-pak-deployer\n  name: cloud-pak-deployer\n  namespace: cloud-pak-deployer\nspec:\n  parallelism: 1\n  completions: 1\n  backoffLimit: 0\n  template:\n    metadata:\n      name: cloud-pak-deployer\n      labels:\n        app: cloud-pak-deployer\n    spec:\n      containers:\n      - name: cloud-pak-deployer\n        image: cloud-pak-deployer:latest\n        imagePullPolicy: Always\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        env:\n        - name: CONFIG_DIR\n          value: /Data/cpd-config\n        - name: STATUS_DIR\n          value: /Data/cpd-status\n        - name: CP_ENTITLEMENT_KEY\n          value: ${CP_ENTITLEMENT_KEY}\n        volumeMounts:\n        - name: config-volume\n          mountPath: /Data/cpd-config/config\n        - name: status-volume\n          mountPath: /Data/cpd-status\n        command: [\"/bin/sh\",\"-xc\"]\n        args: \n          - /cloud-pak-deployer/cp-deploy.sh env apply -v\n      restartPolicy: Never\n      securityContext:\n        runAsUser: 0\n      serviceAccountName: cloud-pak-deployer-sa\n      volumes:\n      - name: config-volume\n        configMap:\n          name: cloud-pak-deployer-config\n      - name: status-volume\n        persistentVolumeClaim:\n          claimName: cloud-pak-deployer-status        \nEOF\n</code></pre>"},{"location":"50-advanced/build-image-and-run-deployer-on-openshift/#optional-start-debug-job","title":"Optional: start debug job","text":"<p>The debug job can be useful if you want to access the status directory of the deployer if the deployer job has failed. <pre><code>cat &lt;&lt; EOF | oc apply -f -\napiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: cloud-pak-deployer-debug\n  name: cloud-pak-deployer-debug\n  namespace: cloud-pak-deployer\nspec:\n  parallelism: 1\n  completions: 1\n  backoffLimit: 0\n  template:\n    metadata:\n      name: cloud-pak-deployer-debug\n      labels:\n        app: cloud-pak-deployer-debug\n    spec:\n      containers:\n      - name: cloud-pak-deployer-debug\n        image: cloud-pak-deployer:latest\n        imagePullPolicy: Always\n        terminationMessagePath: /dev/termination-log\n        terminationMessagePolicy: File\n        env:\n        - name: CONFIG_DIR\n          value: /Data/cpd-config\n        - name: STATUS_DIR\n          value: /Data/cpd-status\n        volumeMounts:\n        - name: config-volume\n          mountPath: /Data/cpd-config/config\n        - name: status-volume\n          mountPath: /Data/cpd-status\n        command: [\"/bin/sh\",\"-xc\"]\n        args: \n          - sleep infinity\n      restartPolicy: Never\n      securityContext:\n        runAsUser: 0\n      serviceAccountName: cloud-pak-deployer-sa\n      volumes:\n      - name: config-volume\n        configMap:\n          name: cloud-pak-deployer-config\n      - name: status-volume\n        persistentVolumeClaim:\n          claimName: cloud-pak-deployer-status        \nEOF\n</code></pre></p>"},{"location":"50-advanced/build-image-and-run-deployer-on-openshift/#follow-the-logs-of-the-deployment","title":"Follow the logs of the deployment","text":"<pre><code>oc logs -f -n cloud-pak-deployer job/cloud-pak-deployer\n</code></pre> <p>In some cases, especially if the OpenShift cluster is remote from where the <code>oc</code> command is running, the <code>oc logs -f</code> command may terminate abruptly. </p>"},{"location":"50-advanced/cp4d-pre-release/","title":"Cloud Pak for Data pre-release installations","text":"<p>For IBM Employees only</p> <p>If you want to install a Cloud Pak for Data release that has not GA'ed yet, you can request access to the internal CASE repository and container image registries.</p> <p>Refer to: https://github.ibm.com/CloudPakDeployer/cloud-pak-deployer-internal/blob/main/README.md, for configuration instructions.</p>"},{"location":"50-advanced/gitops/","title":"Continuous Adoption using GitOps","text":"<p>The process of supporting multiple products, releases and patch levels within a release has great similarity to the git-flow model, which has been really well-described by Vincent Driessen in his blog post: https://nvie.com/posts/a-successful-git-branching-model/. This model has been and is still very popular with many software-development teams.</p> <p>Below is a description of how a git-flow could be implemented with the Cloud Pak Deployer. The following steps are covered:</p> <ul> <li>Setting up the company's Git and image registry for the Cloud Paks</li> <li>The git-flow change process</li> <li>Feeding Cloud Pak changes into the process</li> <li>Deploying the Cloud Pak changes</li> </ul>"},{"location":"50-advanced/gitops/#environments-git-and-registry","title":"Environments, Git and registry","text":"<p>.</p> <p>There are 4 Cloud Pak environments within the company's domain: Dev, UAT, Pre-prod and Prod. Each of these environments have a namespace in the company's registry (or an isolated registry could be created per environment) and the Cloud Pak release installed is represented by manifests in a branch of the Git repository, respectively dev, uat, pp and prod.</p> <p>Organizing registries by namespace has the advantage that duplication of images can be avoided. Each of the namespaces can have their own set of images that have been approved for running in the associated environment. The image itself is referenced by digest (i.e., checksum) and organized on disk as such. If one tries to copy an image to a different namespace within the same registry, only a new entry is created, the image itself is not duplicated because it already exists.</p> <p>The manifests (CASE files) representing the Cloud Pak components are present in each of the branches of the Git repository, or there is a configuration file that references the location of the case file, including the exact version number.</p> <p>In the Cloud Pak Deployer, we have chosen to reference the CASE versions in the configuration, for example:</p> <pre><code>cp4d:\n- project: cpd-instance\n  openshift_cluster_name: {{ env_id }}\n  cp4d_version: 4.8.3\n  openshift_storage_name: odf-storage\n  cartridges:\n  - name: cpfs\n  - name: cpd_platform\n  - name: ws\n    state: installed\n  - name: wml\n    size: small\n    state: installed\n</code></pre> <p>If Cloud Pak for Data has been configured with a private registry in the deployer config, the deployer will mirror images from the IBM entitled registry to the private registry. In the above configuration, no private registry has been specified. The deployer will automatically download and use the CASE files to create the catalog sources.</p>"},{"location":"50-advanced/gitops/#change-process-using-git-flow","title":"Change process using git-flow","text":"<p>With the initial status in place, the continuous adoption process may commence, using the principles of git-flow.</p> <p>Git-flow addresses a couple of needs for continuous adoption:</p> <ul> <li>Control and visibility over what software (version) runs in which environment; there is a central truth which describes the state of every environment managed</li> <li>New features (in case of the deployer: new operator versions and custom resources) can be tested without affecting the pending releases or production implementation</li> <li>While preparing for a new release, hot fixes can still be applied to the production environments</li> </ul> <p></p> <p>The Git repository consists of 4 branches: dev, uat, pp and prd. At the start, release 4.0.0 is being implemented and it will go through the stages from dev to prd. When the installation has been tested in development, a pull request (PR) is done to promote to the uat branch. The PR is reviewed, and changes are then merged into the uat branch. After testing in the uat branch, the steps are repeated until the 4.0.0 release is eventually in production.</p> <p>With each of the implementation and promotion steps, the registry namespaces and associated with the particular branch are updated with the images described in the manifests kept in the Git repository. Additionally, the changes are installed in the respective environments. The details of these processes will be outlined later.</p> <p>New patches are received, committed and installed on the dev branch on a regular basis and when no issues are found, the changes are gathered into a PR for uat. When no issues are found for 2 weeks, another PR is done for the pp branch and eventually for prd. During this promotion flow, new patches are still being received in dev.</p> <p>While version 4.0.2 is running in production, a critical defect is found for which a hot fix is developed. The hot fix is first committed to the pp branch and tested and then a PR is made to promote it to the prd branch. In the meantime, the dev and uat branches continue with their own release schedule. The hot fix is included in 4.0.4 which will be promoted as part of the 4.0.5 release.</p> <p>The uat, pp and prd branches can be protected by a branch protection rule so that changes from dev can only be promoted (via a pull request) after an approving review or, when the intention is to promote changes in a fully automated manner, after passing status checks and testing. Read Managing a branch protection rule for putting in these controls in GitHub or Protected branches for GitLab.</p> <p>With this flow, there is control over patches, promotion approvals and releases installed in each of the environments. Additional branches could be introduced if additional environments are in play or if different releases are being managed using the git-flow.</p>"},{"location":"50-advanced/gitops/#feeding-patches-and-releases-into-the-flow","title":"Feeding patches and releases into the flow","text":"<p>As discussed above, patches are first \"developed\" in the dev branch, i.e., changes are fed into the Git repository, images are loaded into the company's registry (dev namespace) and the installed into the Dev environment.</p> <p>The process of receiving and installing the patches is common for all Cloud Paks: the <code>cloudctl case</code> tool downloads the CASE file associated with the operator version and the same CASE file can be used to upload images into the company's registry. Then a Catalog Source is created which makes the images available to the operator subscriptions, which in turn manage the various custom resources in the Cloud Pak instance. For example, the <code>ws</code> operator manages the <code>Ws</code> custom resource and this CR ensures that OpenShift deployments, secrets, Config Maps, Stateful Sets, and so forth are managed within the Cloud Pak for Data instance project.</p> <p>In the git-flow example, Watson Studio release <code>4.0.2</code> is installed by updating the Catalog Source. Detailed installation steps for Cloud Pak for Data can be found in the IBM documentation.</p>"},{"location":"50-advanced/gitops/#deploying-the-cloud-pak-changes","title":"Deploying the Cloud Pak changes","text":"<p>Now that the hard work of managing changes to the Git repository branches and image registry namespaces has been done, we can look at the (automatic) deployment of the changes.</p> <p>In a continuous adoption workflow, the implementation of new releases and patches is automated by means of a pipeline, which allows for deployment and testing in a predictable and controlled manner. A pipeline executes a series of steps to inspect the change and then run the command to install it in the respective environment. Moreover, after installation tests can be automatically executed. The most-popular tools for pipelines are ArgoCD, GitLab pipelines and Tekton (serverless).</p> <p>To link the execution of a pipeline with the git-flow pull request, one can use ArcoCD or a GitHub/GitLab webhook. As soon as a PR is accepted and changes are applied to the Git branch, the pipeline is triggered and will run the Cloud Pak Deployer to automatically apply the changes according to the latest version.</p>"},{"location":"50-advanced/locations-to-whitelist/","title":"Locations to whitelist on bastion","text":"<p>When building or running the deployer in an environment with strict policies for internet access, you may have to specify the list of URLs that need to be accessed by the deployer.</p>"},{"location":"50-advanced/locations-to-whitelist/#locations-to-whitelist-when-building-the-deployer-image","title":"Locations to whitelist when building the deployer image.","text":"Location Used for registry.access.redhat.com Base image icr.io olm-utils base image cdn.redhat.com Installing operating system packages cdn-ubi.redhat.com Installing operating system packages rpm.releases.hashicorp.com Hashicorp Vault integration dl.fedoraproject.org Extra Packages for Enterprise Linux (EPEL) mirrors.fedoraproject.org EPEL mirror site fedora.mirrorservice.org EPEL mirror site pypi.org Python packages for deployer galaxy.ansible.com Ansible Galaxy packages"},{"location":"50-advanced/locations-to-whitelist/#locations-to-whitelist-when-running-the-deployer-for-existing-openshift","title":"Locations to whitelist when running the deployer for existing OpenShift.","text":"Location Used for github.com Case files, Cloud Pak clients: cloudctl, cpd-cli. gcr.io Google Container Registry (GCR) objects.githubusercontent.com Binary content for github.com raw.githubusercontent.com Binary content for github.com mirror.openshift.com OpenShift client ocsp.digicert.com Certificate checking subscription.rhsm.redhat.com OpenShift subscriptions"},{"location":"50-advanced/private-registry-and-air-gapped/","title":"Using a private registry","text":"<p>Some environments, especially in situations where the OpenShift cannot directly connect to the internet, require a private registry for OpenShift to pull the Cloud Pak images from. The Cloud Pak Deployer can mirror images from the entitled registry to a private registry that you want to use for the Cloud Pak(s). Also, if infrastructure which holds the OpenShift cluster is fully disconnected from the internet, the Cloud Pak Deployer can build a registry which can be stored on a portable hard disk or pen drive and then shipped to the site.</p> <p>Info</p> <p>Note: In all cases, the deployer can work behind a proxy to access the internet. Go to Running behind proxy for more information.</p> <p>The below instructions are not limited to disconnected (air-gapped) OpenShift clusters, but are more generic for deployment using a private registry.</p> <p>There are three use cases for mirroring images to a private registry and using this to install the Cloud Pak(s):</p> <ul> <li>Use case 1 - Mirror images and install using a bastion server. The bastion server can connect to the internet (directly or via a proxy), to OpenShift and to the private registry used by the OpenShift cluster.</li> <li>Use case 2 - Mirror images with a connected server, install using a bastion. The connected server can connect to the internet and to the private registry used by the OpenShift cluster. The server cannot connect to the OpenShift cluster. The bastion server can connect to the private registry and to the OpenShift cluster.</li> <li>Use case 3 - Mirror images using a portable image registry. The private registry used by the OpenShift cluster cannot be reached from the server that is connected to the internet. You need a portable registry to download images and which you then ship to a server that can connect to the existing OpenShift cluster and its private registry.</li> </ul> <p>Use cases 1 and 3 are also outlined in the Cloud Pak for Data installation documentation: https://www.ibm.com/docs/en/cloud-paks/cp-data/4.5.x?topic=tasks-mirroring-images-your-private-container-registry</p> <p>For specifying a private registry in the Cloud Pak Deployer configuration, please see Private registry. Example of specifying a private registry with a self-signed certificate in the configuration: <pre><code>image_registry:\n- name: cpd453\n  registry_host_name: registry.coc.ibm.com\n  registry_port: 5000\n  registry_insecure: True\n</code></pre></p> <p>The <code>cp4d</code> instance must reference the <code>image_registry</code> object using the <code>image_registry_name</code>: <pre><code>cp4d:\n- project: zen-45\n  openshift_cluster_name: {{ env_id }}\n  cp4d_version: 4.5.3\n  openshift_storage_name: odf-storage\n  image_registry_name: cpd453\n</code></pre></p> <p>Info</p> <p>The deployer only supports using a private registry for the Cloud Pak images, not for OpenShift itself. Air-gapped installation of OpenShift is currently not in scope for the deployer.</p> <p>Warning</p> <p>The <code>registry_host_name</code> you specify in the <code>image_registry</code> definition must also be available for DNS lookup within OpenShift. If the registry runs on a server that is not registered in the DNS, use its IP address instead of a host name.</p> <p>The main 3 directories that are needed for both types of air-gapped installations are:</p> <ul> <li>Cloud Pak Deployer directory: <code>cloud-pak-deployer</code></li> <li>Configuration directory: The directory that holds a all the Cloud Pak Deployer configuration</li> <li>Status directory: The directory that will hold all downloads, vault secrets and the portable registry when applicable (use case 3)</li> </ul> <p>Fpr use cases 2 and 3, where the directories must be shipped to the air-gapped cluster, the Cloud Pak Deployer and Configuration directories will be stored in the Status directory for simplicity.</p>"},{"location":"50-advanced/private-registry-and-air-gapped/#use-case-1---mirror-images-and-install-using-a-bastion-server","title":"Use case 1 - Mirror images and install using a bastion server","text":"<p>This is effectively \"not-air-gapped\" scenario, where the following conditions apply:</p> <ul> <li>The private registry is hosted inside the private dloud</li> <li>The bastion server can connect to the internet and mirror images to the private image registry</li> <li>The bastion server is optionally connected to the internet via a proxy server. See Running behind a proxy for more details</li> <li>The bastion server can connect to OpenShift</li> </ul> <p></p>"},{"location":"50-advanced/private-registry-and-air-gapped/#on-the-bastion-server","title":"On the bastion server","text":"<p>The bastion server is connected to the internet and OpenShift cluster.</p> <ul> <li>If there are restrictions regarding the internet sites that can be reached, ensure that the website domains the deployer needs are whitelisted. For a list of domains, check locations to whitelist</li> <li>If a proxy server is configured for the bastion node, check the settings (<code>http_proxy</code>, <code>https_proxy</code>, <code>no_proxy</code> environment variables)</li> <li>Build the Cloud Pak Deployer image using <code>cp-deploy.sh build</code></li> <li>Create or update the directory with the configuration; make sure all your Cloud Paks and cartridges are specified as well as an <code>image_registry</code> entry to identify the private registry</li> <li>Export the CONFIG_DIR and STATUS_DIR environment variables to respectively point to the configuration directory and the status directory</li> <li>Export the CP_ENTITLEMENT_KEY environment variable with your Cloud Pak entitlement key</li> <li> <p>Create a vault secret <code>image-registry-&lt;name&gt;</code> holding the connection credentials for the private registry specified in the configuration (<code>image_registry</code>). For example for a registry definition with name <code>cpd453</code>, create secret <code>image-registry-cpd453</code>. <pre><code>cp-deploy.sh vault set \\\n    -vs image-registry-cpd453 \\\n    -vsv \"admin:very_s3cret\"\n</code></pre></p> </li> <li> <p>Set the environment variable for the <code>oc login</code> command. For example: <pre><code>export CPD_OC_LOGIN=\"oc login api.pluto-01.coc.ibm.com:6443 -u kubeadmin -p BmxQ5-KjBFx-FgztG-gpTF3 --insecure-skip-tls-verify\"\n</code></pre></p> </li> <li> <p>Run the <code>cp-deploy.sh env apply</code> command to start deployment of the Cloud Pak to the OpenShift cluster. For example: <pre><code>cp-deploy.sh env apply\n</code></pre> The existence of the <code>image_registry</code> definition and its reference in the <code>cp4d</code> definition instruct the deployer to mirror images to the private registry and to configure the OpenShift cluster to pull images from the private registry. If you have already mirrored the Cloud Pak images, you can add the <code>--skip-mirror-images</code> parameter to speed up the deployment process.</p> </li> </ul>"},{"location":"50-advanced/private-registry-and-air-gapped/#use-case-2---mirror-images-with-an-internet-connected-server-install-using-a-bastion","title":"Use case 2 - Mirror images with an internet-connected server, install using a bastion","text":"<p>This use case is also sometimes referred to as \"semi-air-gapped\", where the following conditions apply:</p> <ul> <li>The private registry is hosted outside of the private cloud that hosts the bastion server and OpenShift</li> <li>An internet-connected server external to the private cloud can reach the entitled registry and the private registry</li> <li>The internet-connected server is optionally connected to the internet via a proxy server. See Running behind a proxy for more details</li> <li>The bastion server cannot connect to the internet</li> <li>The bastion server can connect to OpenShift</li> </ul> <p></p> <p>Warning</p> <p>Please note that in this case the Cloud Pak Deployer expects an OpenShift cluster to be available already and will only work with an <code>existing-ocp</code> configuration. The bastion server does not have access to the internet and can therefore not instantiate an OpenShift cluster.</p>"},{"location":"50-advanced/private-registry-and-air-gapped/#on-the-internet-connected-server","title":"On the internet-connected server","text":"<ul> <li>If there are restrictions regarding the internet sites that can be reached, ensure that the website domains the deployer needs are whitelisted. For a list of domains, check locations to whitelist</li> <li>If a proxy server is configured for the internet-connected server, check the settings (<code>http_proxy</code>, <code>https_proxy</code>, <code>no_proxy</code> environment variables)</li> <li>Build the Cloud Pak Deployer image using <code>cp-deploy.sh build</code></li> <li>Create or update the directory with the configuration; make sure all your Cloud Paks and cartridges are specified as well as an <code>image_registry</code> entry to identify the private registry</li> <li>Export the CONFIG_DIR and STATUS_DIR environment variables to respectively point to the configuration directory and the status directory</li> <li>Export the CP_ENTITLEMENT_KEY environment variable with your Cloud Pak entitlement key</li> <li>Create a vault secret <code>image-registry-&lt;name&gt;</code> holding the connection credentials for the private registry specified in the configuration (<code>image_registry</code>). For example for a registry definition with name <code>cpd453</code>, create secret <code>image-registry-cpd453</code>. <pre><code>cp-deploy.sh vault set \\\n    -vs image-registry-cpd453 \\\n    -vsv \"admin:very_s3cret\"\n</code></pre> If the status directory does not exist it is created at this point.</li> </ul>"},{"location":"50-advanced/private-registry-and-air-gapped/#diagram-step-1","title":"Diagram step 1","text":"<ul> <li> <p>Run the deployer using the <code>cp-deploy.sh env download --skip-portable-registry</code> command. For example: <pre><code>cp-deploy.sh env download \\\n    --skip-portable-registry\n</code></pre> This will download all clients to the status directory and then mirror images from the entitled registry to the private registry. If mirroring fails, fix the issue and just run the <code>env download</code> again.</p> </li> <li> <p>Before saving the status directory, you can optionally remove the entitlement key from the vault: <pre><code>cp-deploy.sh vault delete \\\n    -vs ibm_cp_entitlement_key\n</code></pre></p> </li> </ul>"},{"location":"50-advanced/private-registry-and-air-gapped/#diagram-step-2","title":"Diagram step 2","text":"<p>When the download finished successfully, the status directory holds the deployer scripts, the configuration directory and the deployer container image.</p>"},{"location":"50-advanced/private-registry-and-air-gapped/#diagram-step-3","title":"Diagram step 3","text":"<p>Ship the status directory from the internet-connected server to the bastion server.</p> <p>You can use tar with gzip mode or any other compression technique. The total size of the directories should be relatively small, typically &lt; 5 GB</p>"},{"location":"50-advanced/private-registry-and-air-gapped/#on-the-bastion-server_1","title":"On the bastion server","text":"<p>The bastion server is not connected to the internet but is connected to the private registry and the OpenShift cluster.</p>"},{"location":"50-advanced/private-registry-and-air-gapped/#diagram-step-4","title":"Diagram step 4","text":"<p>We're using the instructions in Run on existing OpenShift, adding the <code>--air-gapped</code> and <code>--skip-mirror-images</code> flags, to start the deployer:</p> <ul> <li>Restore the status directory onto the bastion server</li> <li>Export the STATUS_DIR environment variable to point to the status directory</li> <li> <p>Untar the <code>cloud-pak-deployer</code> scripts, for example: <pre><code>tar xvzf $STATUS_DIR/cloud-pak-deployer.tar.gz\n</code></pre></p> </li> <li> <p>Set the CPD_AIRGAP environment variable to <code>true</code> <pre><code>export CPD_AIRGAP=true\n</code></pre></p> </li> <li> <p>Set the environment variable for the <code>oc login</code> command. For example: <pre><code>export CPD_OC_LOGIN=\"oc login api.pluto-01.coc.ibm.com:6443 -u kubeadmin -p BmxQ5-KjBFx-FgztG-gpTF3 --insecure-skip-tls-verify\"\n</code></pre></p> </li> <li> <p>Run the <code>cp-deploy.sh env apply --skip-mirror-images</code> command to start deployment of the Cloud Pak to the OpenShift cluster. For example: <pre><code>cd cloud-pak-deployer\ncp-deploy.sh env apply \\\n    --skip-mirror-images\n</code></pre></p> </li> </ul> <p>The <code>CPD_AIRGGAP</code> environment variable tells the deployer it will not download anything from the internet; <code>--skip-mirror-images</code> indicates that images are already available in the private registry that is included in the configuration (<code>image_registry</code>)</p>"},{"location":"50-advanced/private-registry-and-air-gapped/#use-case-3---mirror-images-using-a-portable-image-registry","title":"Use case 3 - Mirror images using a portable image registry","text":"<p>This use case is also usually referred to as \"air-gapped\", where the following conditions apply:</p> <ul> <li>The private registry is hosted in the private cloud that hosts the bastion server and OpenShift</li> <li>The bastion server cannot connect to the internet</li> <li>The bastion server can connect to the private registry and the OpenShift cluster</li> <li>The internet-connected server cannot connect to the private cloud</li> <li>The internet-connected server is optionally connected to the internet via a proxy server. See Running behind a proxy for more details</li> <li>You need a portable registry to fill the private registry with the Cloud Pak images</li> </ul> <p></p> <p>Warning</p> <p>Please note that in this case the Cloud Pak Deployer expects an OpenShift cluster to be available already and will only work with an <code>existing-ocp</code> configuration. The bastion server does not have access to the internet and can therefore not instantiate an OpenShift cluster.</p>"},{"location":"50-advanced/private-registry-and-air-gapped/#on-the-internet-connected-server_1","title":"On the internet-connected server","text":"<ul> <li>If there are restrictions regarding the internet sites that can be reached, ensure that the website domains the deployer needs are whitelisted. For a list of domains, check locations to whitelist</li> <li>If a proxy server is configured for the bastion node, check the settings (<code>http_proxy</code>, <code>https_proxy</code>, <code>no_proxy</code> environment variables)</li> <li>Build the Cloud Pak Deployer image using <code>cp-deploy.sh build</code></li> <li>Create or update the directory with the configuration, making sure all your Cloud Paks and cartridges are specified</li> <li>Export the CONFIG_DIR and STATUS_DIR environment variables to respectively point to the configuration directory and the status directory</li> <li>Export the CP_ENTITLEMENT_KEY environment variable with your Cloud Pak entitlement key</li> </ul>"},{"location":"50-advanced/private-registry-and-air-gapped/#diagram-step-1_1","title":"Diagram step 1","text":"<ul> <li> <p>Run the deployer using the <code>cp-deploy.sh env download</code> command. For example: <pre><code>cp-deploy.sh env download\n</code></pre> This will download all clients, start the portable registry and then mirror images from the entitled registry to the portable registry. The portable registry data is kept in the status directory. If mirroring fails, fix the issue and just run the <code>env download</code> again.</p> </li> <li> <p>Before saving the status directory, you can optionally remove the entitlement key from the vault: <pre><code>cp-deploy.sh vault delete \\\n    -vs ibm_cp_entitlement_key\n</code></pre></p> </li> </ul> <p>See the download of watsonx.ai in action:</p>"},{"location":"50-advanced/private-registry-and-air-gapped/#diagram-step-2_1","title":"Diagram step 2","text":"<p>When the download finished successfully, the status directory holds the deployer scripts, the configuration directory, the deployer container image and the portable registry.</p>"},{"location":"50-advanced/private-registry-and-air-gapped/#diagram-step-3_1","title":"Diagram step 3","text":"<p>Ship the status directory from the internet-connected server to the bastion server.</p> <p>You can use tar with gzip mode or any other compression technique. The status directory now holds all assets required for the air-gapped installation and its size can be substantial (100+ GB). You may want to use multi-volume tar files if you are using network transfer. </p>"},{"location":"50-advanced/private-registry-and-air-gapped/#on-the-bastion-server_2","title":"On the bastion server","text":"<p>The bastion server is not connected to the internet but is connected to the private registry and OpenShift cluster.</p>"},{"location":"50-advanced/private-registry-and-air-gapped/#diagram-step-4_1","title":"Diagram step 4","text":"<p>See the air-gapped installation of Cloud Pak for Data in action:</p> <p>For the demonstration video, the download of the previous step has first been re-run to only download the Cloud Pak for Data control plane to avoid having to ship and upload ~700 GB.</p> <p>We're using the instructions in Run on existing OpenShift, adding the CPD_AIRGAP environment variable.</p> <ul> <li>Restore the status directory onto the bastion server. Make sure the volume to which you restore has enough space to hold the entire status directory, which includes the portable registry.</li> <li>Export the STATUS_DIR environment variable to point to the status directory</li> <li> <p>Untar the <code>cloud-pak-deployer</code> scripts, for example: <pre><code>tar xvzf $STATUS_DIR/cloud-pak-deployer.tar.gz\ncd cloud-pak-deployer\n</code></pre></p> </li> <li> <p>Set the CPD_AIRGAP environment variable to <code>true</code> <pre><code>export CPD_AIRGAP=true\n</code></pre></p> </li> <li> <p>Set the environment variable for the <code>oc login</code> command. For example: <pre><code>export CPD_OC_LOGIN=\"oc login api.pluto-01.coc.ibm.com:6443 -u kubeadmin -p BmxQ5-KjBFx-FgztG-gpTF3 --insecure-skip-tls-verify\"\n</code></pre></p> </li> <li> <p>Create a vault secret <code>image-registry-&lt;name&gt;</code> holding the connection credentials for the private registry specified in the configuration (<code>image_registry</code>). For example for a registry definition with name <code>cpd453</code>, create secret <code>image-registry-cpd453</code>. <pre><code>cp-deploy.sh vault set \\\n    -vs image-registry-cpd453 \\\n    -vsv \"admin:very_s3cret\"\n</code></pre></p> </li> <li> <p>Run the <code>cp-deploy.sh env apply</code> command to start deployment of the Cloud Pak to the OpenShift cluster. For example: <pre><code>cp-deploy.sh env apply\n</code></pre> The <code>CPD_AIRGGAP</code> environment variable tells the deployer it will not download anything from the internet. As a first action, the deployer mirrors images from the portable registry to the private registry included in the configuration (<code>image_registry</code>)</p> </li> </ul>"},{"location":"50-advanced/private-registry-and-air-gapped/#running-behind-a-proxy","title":"Running behind a proxy","text":"<p>If the Cloud Pak Deployer is run from a server that has the HTTP proxy environment variables set up, i.e. \"proxy\" environment variables are configured on the server and in the terminal session, it will also apply these settings in the deployer container. </p> <p>The following environment variables are automatically applied to the deployer container if set up in the session running the <code>cp-deploy.sh</code> command:</p> <ul> <li><code>http_proxy</code></li> <li><code>https_proxy</code></li> <li><code>no_proxy</code></li> </ul> <p>If you do not want the deployer to use the proxy environment variables, you must remove them before running the <code>cp-deploy.sh</code> command: <pre><code>unset http_proxy\nunset https_proxy\nunset no_proxy\n</code></pre></p>"},{"location":"50-advanced/private-registry-and-air-gapped/#special-settings-for-debug-and-daemonset-images-in-air-gapped-mode","title":"Special settings for debug and DaemonSet images in air-gapped mode","text":"<p>Specifically when running the deployer on IBM Cloud ROKS, certain OpenShift settings must be applied using DaemonSets in the <code>kube-system</code> namespace. Additionally, the deployer uses the <code>oc debug node</code> commands to retrieve <code>kubelet</code> and <code>crio</code> configuration files from the compute nodes.</p> <p>The default container images used by the DaemonSets and <code>oc debug node</code> commands are based on Red Hat's Universal Base Image and will be pulled from Red Hat registries. This is typically not possible in air-gapped installations, hence different images must be used. It is your responsibility to copy suitable (preferably UBI) images to an image registry that is connected to the OpenShift cluster. Also, if a pull secret is needed to pull the image(s) from the registry, you must create the associated secret in the <code>kube-system</code> OpenShift project.</p> <p>To configure alternative container images for the deployer to use, set the following properties in the <code>.inv</code> file kept in your configuration's <code>inventory</code> directory, or specify them as additional command line parameters for the <code>cp-deploy.sh</code> command. </p> <p>If you do not set these values, the deployer assumes that the default images are used for DaemonSet and <code>oc debug node</code>.</p> Property Description Example cpd_oc_debug_image Container image to be used for the <code>oc debug</code> command. <code>registry.redhat.io/rhel8/support-tools:latest</code> cpd_ds_image Container image to be used for the DaemonSets that configure Kubelet, etc. <code>icr.io/cpopen/cpd/olm-utils-v3:latest</code>"},{"location":"80-development/deployer-development-setup/","title":"Deployer Development Setup","text":"<p>Setting up a virtual machine or server to develop the Cloud Pak Deployer code. Focuses on initial setup of a server to run the deployer container, setting up Visual Studio Code, issuing GPG keys and running the deployer in development mode.</p>"},{"location":"80-development/deployer-development-setup/#set-up-a-server-for-development","title":"Set up a server for development","text":"<p>We recommend to use a Red Hat Linux server for development of the Cloud Pak Deployer, either using a virtual server in the cloud or a virtual machine on your workstation. Ideally you run Visual Studio Code on your workstation and connect it to the remote Red Hat Linux server, updating the code and running it immediately from that server.</p>"},{"location":"80-development/deployer-development-setup/#install-required-packages","title":"Install required packages","text":"<p>To allow for remote development, a number of packages need to be installed on the Linux server. Not having these will cause VSCode not to work and the error messages are difficult to debug. To install these packages, run the following as the <code>root</code> user: <pre><code>yum install -y git podman wget unzip tar gpg pinentry\n</code></pre></p> <p>Additionally, you can also install EPEL and <code>screen</code> to make it easier to keep your session if it gets disconnected. <pre><code>yum install -y https://dl.fedoraproject.org/pub/epel/epel-release-latest-8.noarch.rpm\nyum install -y screen\n</code></pre></p>"},{"location":"80-development/deployer-development-setup/#set-up-development-user","title":"Set up development user","text":"<p>It is recommended to use a special development user (your user name) on the Linux server, rather than using <code>root</code>. Not only will this be more secure; it also prevent destructive mistakes. In the below steps, we create a user <code>fk-dev</code> and give it <code>sudo</code> permissions.</p> <pre><code>useradd -G wheel fk-dev\n</code></pre> <p>To give the <code>fk-dev</code> permissions to run commands as <code>root</code>, change the <code>sudo</code> settings. <pre><code>visudo\n</code></pre></p> <p>Scroll down until you see the following line: <pre><code># %wheel        ALL=(ALL)       NOPASSWD: ALL\n</code></pre></p> <p>Change the line to look like this: <pre><code>%wheel        ALL=(ALL)       NOPASSWD: ALL\n</code></pre></p> <p>Now, save the file by pressing Esc, followed by <code>:</code> and <code>x</code>.</p>"},{"location":"80-development/deployer-development-setup/#configure-password-less-ssh-for-development-user","title":"Configure password-less SSH for development user","text":"<p>Especially when running the virtual server in the cloud, users would logon using their SSH key. This requires the public key of the workstation to be added to the development user's SSH configuration.</p> <p>Make sure you run the following commands as the development user (fk-dev): <pre><code>mkdir -p ~/.ssh\nchmod 700 ~/.ssh\ntouch ~/.ssh/authorized_keys\nchmod 600 ~/.ssh/authorized_keys\n</code></pre></p> <p>Then, add the public key of your workstation to the <code>authorized_keys</code> file. <pre><code>vi ~/.ssh/authorized_keys\n</code></pre></p> <p>Press the <code>i</code> to enter insert mode for <code>vi</code>. Then paste the public SSH key, for example: <pre><code>ssh-rsa AAAAB3NzaC1yc2EAAAADAXABAAABAQEGUeXJr0ZHy1SPGOntmr/7ixmK3KV8N3q/+0eSfKVTyGbhUO9lC1+oYcDvwMrizAXBJYWkIIwx4WgC77a78....fP3S5WYgqL fk-dev\n</code></pre></p> <p>Finally save the file by pressing Esc, followed by <code>:</code> and <code>x</code>.</p>"},{"location":"80-development/deployer-development-setup/#configure-git-for-the-development-user","title":"Configure Git for the development user","text":"<p>Run the following commands as the development user (fk-dev): <pre><code>git config --global user.name \"Your full name\"\ngit config --global user.email \"your_email_address\"\ngit config --global credential.helper \"cache --timeout=86400\"\n</code></pre></p>"},{"location":"80-development/deployer-development-setup/#set-up-gpg-for-the-development-user","title":"Set up GPG for the development user","text":"<p>We also want to ensure that commits are verified (trusted) by signing them with a GPG key. This requires set up on the development server and also on your Git account.</p> <p>First, set up a new GPG key: <pre><code>gpg --default-new-key-algo rsa4096 --gen-key\n</code></pre></p> <p>You will be prompted to specify your user information:</p> <ul> <li>Real name: Enter your full name</li> <li>Email address: Your e-mail address that will be used to sign the commits</li> </ul> <p>Press <code>o</code> at the following prompt: <pre><code>Change (N)ame, (E)mail, or (O)kay/(Q)uit?\n</code></pre></p> <p>Then, you will be prompted for a passphrase. You cannot use a passphrase for your GPG key if you want to use it for automatic signing of commits. Just press Enter multiple times until the GPG key has been generated.</p> <p>List the signatures of the known keys. You will use the signature to sign the commits and to retrieve the public key. <pre><code>gpg --list-signatures\n</code></pre></p> <p>Output will look something like this: <pre><code>/home/fk-dev/.gnupg/pubring.kbx\n-----------------------------------\npub   rsa4096 2022-10-30 [SC] [expires: 2024-10-29]\n      BC83E8A97538EDD4E01DC05EA83C67A6D7F71756\nuid           [ultimate] FK Developer &lt;fk-dev@ibm.com&gt;\nsig 3        A83C67A6D7F71756 2022-10-30  FK Developer &lt;fk-dev@ibm.com&gt;\n</code></pre></p> <p>You will use the signature to retrieve the public key: <pre><code>gpg --armor --export A83C67A6D7F71756\n</code></pre></p> <p>The public key will look something like below: <pre><code>-----BEGIN PGP PUBLIC KEY BLOCK-----\n\nmQINBGNeGNQBEAC/y2tovX5s0Z+onUpisnMMleG94nqOtajXG1N0UbHAUQyKfirt\nO8t91ek+e5PEsVkR/RLIM1M1YkiSV4irxW/uFPucXHZDVH8azfnJjf6j6cXWt/ra\n1I2vGV3dIIQ6aJIBEEXC+u+N6rWpCOF5ERVrumGFlDhL/PY8Y9NM0cNQCbOcciTV\n5a5DrqyHC3RD5Bcn5EA0/5ISTCGQyEbJe45G8L+a5yRchn4ACVEztR2B/O5iOZbM\n.\n.\n.\n4ojOJPu0n5QLA5cI3RyZFw==\n=sx91\n-----END PGP PUBLIC KEY BLOCK-----\n</code></pre></p> <p>Now that you have the signature, you can configure Git to sign commits: <pre><code>git config --global user.signingkey A83C67A6D7F71756\ngit config --global commit.gpgsign true\n</code></pre></p> <p>Next, add your GPG key to your Git user.</p> <ul> <li>Go to IBM/cloud-pak-deployer.git</li> <li>Log in using your public GitHub user</li> <li>Click on your user at the top right of the pages</li> <li>Click select</li> <li>In the left menu, select SSH and GPG keys</li> <li>Click New GPG key</li> <li>Enter a meaningful title for your GPG key, for example: FK Development Server</li> <li>Paste the public GPG key</li> <li>Confirm by pushing the Add GPG key button</li> </ul> <p>Commits done on your development server will now be signed with your user name and e-mail address and will show as Verified when listing the commits.</p>"},{"location":"80-development/deployer-development-setup/#clone-the-repository","title":"Clone the repository","text":"<p>Clone the repository using a <code>git</code> command. The command below is the clone of the main Cloud Pak Deployer repository. If you have forked the repository to develop features, you will have to use the URL of your own fork. <pre><code>git clone https://github.com/IBM/cloud-pak-deployer.git\n</code></pre></p>"},{"location":"80-development/deployer-development-setup/#connect-vscode-to-the-development-server","title":"Connect VSCode to the development server","text":"<ul> <li>Install the Remote - SSH extension in VSCode</li> <li>Click on the green icon in the lower left of VSCode</li> <li>Open SSH Config file, choose the one in your home directory</li> <li>Add the following lines: <pre><code>Host nickname_of_your_server\n   HostName ip_address_of_your_server\n   User fk-dev\n</code></pre></li> </ul> <p>Once you have set up this server in the SSH config file, you can connect to it and start remote development. </p> <ul> <li>Open</li> <li>Select the <code>cloud-pak-deployer</code> directory (this is the cloned repository)</li> <li>As the directory is a cloned Git repo, VSCode will automatically open the default branch</li> </ul> <p>From that point forward you can use VSCode as if you were working on your laptop, make changes and use a separate terminal to test your changes.</p>"},{"location":"80-development/deployer-development-setup/#cloud-pak-deployer-developer-command-line-option","title":"Cloud Pak Deployer developer command line option","text":"<p>The Cloud Pak Deployer runs as a container on the server. When you're in the process of developing new features, having to always rebuild the image is a bit of a pain, hence we've introduced a special command line parameter.</p> <pre><code>source ./set-env.sh\n</code></pre> <pre><code>cp-deploy.sh env apply .... --cpd-develop [--accept-all-liceneses]\n</code></pre> <p>When adding the <code>--cpd-develop</code> parameter to the command line, the current directory is mapped as a volume to the <code>/cloud-pak-deployer</code> directory within the container. This means that any latest changes you've done to the Ansible playbooks or other commands will take effect immediately.</p> <p>Warning</p> <p>Even though it is possible to run the deployer multiple times in parallel, for different environments, please be aware that is NOT possible when you use the <code>--cpd-develop</code> parameter. If you run two deploy processes with this parameters, you will see errors with permissions.</p>"},{"location":"80-development/deployer-development-setup/#cloud-pak-deployer-developer-container-image-tag","title":"Cloud Pak Deployer developer container image tag","text":"<p>When working on multiple changes concurrently, you may have to switch between branches or tags. By default, the Cloud Pak Deployer image is built with image <code>latest</code>, but you can override this by setting the <code>CPD_IMAGE_TAG</code> environment variable in your session.</p> <pre><code>source ./set-env.sh\n</code></pre> <pre><code>export CPD_IMAGE_TAG=cp4d-460\ncp-deploy.sh build\n</code></pre> <p>When building the deployer, the image is now tagged: <pre><code>podman image ls\n</code></pre></p> <pre><code>REPOSITORY                           TAG         IMAGE ID      CREATED        SIZE\nlocalhost/cloud-pak-deployer         cp4d-460    8b08cb2f9a2e  8 minutes ago  1.92 GB\n</code></pre> <p>When running the deployer with the same environment variable set, you will see an additional message in the output. <pre><code>cp-deploy.sh env apply\n</code></pre></p> <pre><code>Cloud Pak Deployer image tag cp4d-460 will be used.\n...\n</code></pre>"},{"location":"80-development/deployer-development-setup/#cloud-pak-deployer-podman-or-docker-command","title":"Cloud Pak Deployer podman or docker command","text":"<p>By default, the <code>cp-deploy.sh</code> command detects if <code>podman</code> (preferred) or <code>docker</code> is found on the system. In case both are present, <code>podman</code> is used. You can override this behaviour by setting the <code>CPD_CONTAINER_ENGINE</code> environment variable.</p> <pre><code>source ./set-env.sh\n</code></pre> <pre><code>export CPD_CONTAINER_ENGINE=docker\ncp-deploy.sh build\n</code></pre> <pre><code>Container engine docker will be used.\n</code></pre>"},{"location":"80-development/doc-development-setup/","title":"Documentation Development setup","text":"<p>Mkdocs themes encapsulate all of the configuration and implementation details of static documentation sites. This GitHub repository has been built with a dependency on the Mkdocs tool. This GiHub repository is connected to GitHub Actions; any commit to the <code>main</code> branch will cause a build of the GitHub pages to be triggered. The preferred method of working while developing documentation is to use the tooling from a loacal system</p>"},{"location":"80-development/doc-development-setup/#local-tooling-installation","title":"Local tooling installation","text":"<p>If you want to test the documentation pages you're developing, it is best to run Mkdocs in a container and map your local <code>docs</code> folder to a folder inside the container. This avoids having to install nvm and many modules on your workstation.</p> <p>Do the following:</p> <ul> <li>Make sure you have cloned this repository to your development server</li> <li>Start from the main directory of the cloud-pak-deployer repository <pre><code>cd docs\n./dev-doc-build.sh\n</code></pre></li> </ul> <p>This will build a Red Hat UBI image with all requirements pre-installed. It will take ~2-10 minutes to complete this step, dependent on your network bandwidth.</p>"},{"location":"80-development/doc-development-setup/#running-the-documentation-image","title":"Running the documentation image","text":"<pre><code>./dev-doc-run.sh\n</code></pre> <p>This will start the container as a daemon and tail the logs. Once running, you will see the following message: <pre><code>...\nINFO     -  Documentation built in 3.32 seconds\nINFO     -  [11:55:49] Watching paths for changes: 'src', 'mkdocs.yml'\nINFO     -  [11:55:49] Serving on http://0.0.0.0:8000/cloud-pak-deployer/...\n</code></pre></p>"},{"location":"80-development/doc-development-setup/#starting-the-browser","title":"Starting the browser","text":"<p>Now that the container has fully started, it automatically tracks all changes under the <code>docs</code> folder and updates the pages site automatically. You can view the site by opening a browswer for URL:</p> <p>http://localhost:8000</p>"},{"location":"80-development/doc-development-setup/#stopping-the-documentation-container","title":"Stopping the documentation container","text":"<p>If you don't want to test your changes locally anymore, stop the docker container. <pre><code>podman kill cpd-doc\n</code></pre></p> <p>Next time you want to test your changes, re-run the <code>./dev-doc-run.sh</code>, which will delete the container, delete cache and build the documentation.</p>"},{"location":"80-development/doc-development-setup/#removing-the-docker-container-and-image","title":"Removing the docker container and image","text":"<p>If you want to remove all from your development server, do the following: <pre><code>podman rm -f cpd-doc\npodman rmi -f cpd-doc:latest\n</code></pre></p> <p>Note that after merging your updated documentation with the <code>main</code> branch, the pages site will be rendered by a GitHub action. Go to GitHub Actions if you want to monitor the build process.</p>"},{"location":"80-development/doc-guidelines/","title":"Deployer documentation guidelines","text":""},{"location":"80-development/doc-guidelines/#documentation-guidelines","title":"Documentation guidelines","text":"<p>This document contains a few formatting rules/requirements to maintain uniformity and structure across our documentation. </p>"},{"location":"80-development/doc-guidelines/#formatting","title":"Formatting","text":""},{"location":"80-development/doc-guidelines/#code-block-input","title":"Code block input","text":"<p>Code block inputs should be created by surrounding the code text with three tick marks <code>```</code> key. For example, to create the following code block: <pre><code>oc get nodes\n</code></pre></p> <p>Your markdown input would look like: <pre><code>``` { .bash .copy }\noc get nodes\n```\n</code></pre></p>"},{"location":"80-development/doc-guidelines/#code-block-output","title":"Code block output","text":"<p>Code block outputs should specify the <code>output</code> language. This can be done by putting the language after the opening tick marks. For example, to create the following code block: <pre><code>{\n    \"cloudName\": \"AzureCloud\",\n    \"homeTenantId\": \"fcf67057-50c9-4ad4-98f3-ffca64add9e9\",\n    \"id\": \"d604759d-4ce2-4dbc-b012-b9d7f1d0c185\",\n    \"isDefault\": true,\n    \"managedByTenants\": [],\n    \"name\": \"Microsoft Azure Enterprise\",\n    \"state\": \"Enabled\",\n    \"tenantId\": \"fcf67057-50c9-4ad4-98f3-ffca64add9e9\",\n    \"user\": {\n    \"name\": \"example@example.com\",\n    \"type\": \"user\"\n    }\n}\n</code></pre></p> <p>Your markdown input would look like: <pre><code>```output\n{\n    \"cloudName\": \"AzureCloud\",\n    \"homeTenantId\": \"fcf67057-50c9-4ad4-98f3-ffca64add9e9\",\n    \"id\": \"d604759d-4ce2-4dbc-b012-b9d7f1d0c185\",\n    \"isDefault\": true,\n    \"managedByTenants\": [],\n    \"name\": \"Microsoft Azure Enterprise\",\n    \"state\": \"Enabled\",\n    \"tenantId\": \"fcf67057-50c9-4ad4-98f3-ffca64add9e9\",\n    \"user\": {\n    \"name\": \"example@example.com\",\n    \"type\": \"user\"\n    }\n}\n```\n</code></pre></p>"},{"location":"80-development/doc-guidelines/#information-block-inline-notifications","title":"Information block (inline notifications)","text":"<p>If you want to highlight something to reader, using an information or a warning block, use the following code:</p> <pre><code>!!! warning\n    Warning: please do not shut down the cluster at this stage.\n</code></pre> <p>This will show up as:</p> <p>Warning</p> <p>Warning: please do not shut down the cluster at this stage.</p> <p>You can also <code>info</code> and <code>error</code>.</p>"}]}